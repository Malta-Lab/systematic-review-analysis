{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaravazio/systematic-review-analysis/blob/main/Systematic_Review_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spCPT2q_QnYL"
      },
      "source": [
        "## Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vv6FbQ0qnkm"
      },
      "outputs": [],
      "source": [
        "# Loading Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setting working directory (change as needed)\n",
        "import os\n",
        "\n",
        "# working directory ADD PATH TO FOLDER\n",
        "os.chdir(\"/content/drive/MyDrive/[PATH TO YOUR FOLDER]\")\n",
        "\n",
        "# Installing and downloading useful modules\n",
        "!pip install xlsxwriter # Module to write and organize output tables\n",
        "import xlsxwriter # Module to write and organize output tables\n",
        "import pandas as pd # Data analysis and manipulation tool - https://pandas.pydata.org/\n",
        "import seaborn as sns # Graphical representations - https://seaborn.pydata.org/\n",
        "import matplotlib.pyplot as plt # Graphical representations - https://matplotlib.org/\n",
        "import numpy as np # Scientific computing - https://numpy.org/\n",
        "import scipy.stats as stats # Probability distributions and statistical functions- https://docs.scipy.org/doc/scipy/reference/stats.html\n",
        "import scipy as sp # Probability distributions and statistical functions- https://docs.scipy.org/doc/scipy/reference/stats.html\n",
        "import statsmodels # Statistical models and tests - https://www.statsmodels.org/stable/index.html\n",
        "import statsmodels.stats.api as smsapi # Statistical models and tests - https://www.statsmodels.org/stable/index.html\n",
        "import statsmodels.stats.weightstats as sms # Statistical models and tests - https://www.statsmodels.org/stable/index.html\n",
        "import statsmodels.formula.api as smf # Statistical models and tests - https://www.statsmodels.org/stable/index.html\n",
        "import statsmodels.api as sm # Statistical models and tests - https://www.statsmodels.org/stable/index.html\n",
        "import math # Math!\n",
        "\n",
        "# Some warning are persistent\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "# More data displayed in pandas columns\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 80)\n",
        "pd.set_option('display.max_colwidth', 25) # Maximum width of each column\n",
        "pd.options.display.float_format = \"{:,.3f}\".format # Show to at most 2 decimals values\n",
        "\n",
        "# Print what is in the loaded directory\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcCeGWyUR1iL"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhNFu62vek6l"
      },
      "source": [
        "## General Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR8V0on6R_fW"
      },
      "outputs": [],
      "source": [
        "# Somando colunas e inserindo as novas ao lado da última\n",
        "def somarcolunas(df,somar,excluir=False):\n",
        "  # Criando o nome da nova coluna\n",
        "  for contagem, coluna in enumerate(somar):\n",
        "    name = []\n",
        "    if contagem == 0:\n",
        "      nome = coluna\n",
        "    else:\n",
        "      nome = nome + '_' + coluna\n",
        "\n",
        "  # Calculando e inserindo a nova coluna\n",
        "  for contagem, coluna in enumerate(somar):\n",
        "    if contagem == 0: # Primeiro valor\n",
        "      soma = df[coluna] # Deixando a soma igual ao primeiro elemento\n",
        "    else:\n",
        "      soma = soma + df[coluna] # Somar elementos\n",
        "      if coluna == somar[-1]: # Identificando se estamos no último elemento da lista\n",
        "        if nome not in df.columns: # Vendo se já não existe coluna com esse nome\n",
        "          df.insert(loc = (df.columns.get_loc(somar[-1])+1), column = nome, value = soma)\n",
        "          name.append(nome)\n",
        "          if excluir == True:\n",
        "            df.drop(coluna, axis=1, inplace=True)\n",
        "\n",
        "  truedf = df.copy()\n",
        "  return truedf, name\n",
        "\n",
        "def quantis(df, dividir, divisoes = 3):\n",
        "  name = []\n",
        "  quantilvar = True\n",
        "  for variavel in dividir: # Variáveis que serão divididas\n",
        "    quantis = []\n",
        "    for tamanho in range(divisoes):\n",
        "      quantis.append((tamanho+1)/(divisoes+1))\n",
        "    quartis_bons = []\n",
        "    lista = list(df[variavel].quantile(quantis))\n",
        "    for contagem in range(divisoes):\n",
        "      if (quantilvar == True) and (divisoes == 3):\n",
        "        quantilvar = False\n",
        "        nome = variavel + ' Quartile Division'\n",
        "        temp = ['1' if x <= lista[0] else '2' if x <= lista[1] else '3' if x <= lista[2] else '4' for x in df[variavel]]\n",
        "        if nome not in df.columns: # Vendo se já não existe coluna com esse nome\n",
        "          df.insert(loc = (df.columns.get_loc(variavel)+1), column = nome, value = temp)\n",
        "      nome = variavel + ' More Than ' + str(int((contagem+1)/(divisoes+1)*(100))) + '% (' + str(lista[contagem]) +')'\n",
        "      temp = ['Não' if x <= lista[contagem] else 'Sim' for x in df[variavel]]\n",
        "      if nome not in df.columns: # Vendo se já não existe coluna com esse nome\n",
        "        df.insert(loc = (df.columns.get_loc(variavel)+1), column = nome, value = temp)\n",
        "      quartis_bons.append(nome)\n",
        "      name.append(nome)\n",
        "\n",
        "  return df, name\n",
        "\n",
        "def quantis_single_column(df, dividir, divisoes=3, show_ranges=False):\n",
        "    name = []\n",
        "    for variavel in dividir:  # Variables to be divided\n",
        "        # Calculate the quantile values\n",
        "        quantis = [(i + 1) / (divisoes + 1) for i in range(divisoes)]\n",
        "        lista = list(df[variavel].quantile(quantis))\n",
        "\n",
        "        # Determine the label for the new column\n",
        "        nome = variavel + ' Quantile Division'\n",
        "\n",
        "        # Depending on the show_ranges flag, format the output differently\n",
        "        if show_ranges:\n",
        "            # Create labels that show the range of each quantile\n",
        "            edges = [df[variavel].min()] + lista + [df[variavel].max()]\n",
        "            labels = [f\"{int(edges[i])} - {int(edges[i+1])}\" for i in range(len(edges)-1)]\n",
        "            temp = [labels[sum(x > y for y in lista)] for x in df[variavel]]\n",
        "        else:\n",
        "            # Create a single column showing the quantile division as numbers\n",
        "            temp = [sum(x > y for y in lista) + 1 for x in df[variavel]]\n",
        "\n",
        "        # Check if the column already exists and add it if it doesn't\n",
        "        if nome not in df.columns:\n",
        "            df[nome] = temp\n",
        "\n",
        "        name.append(nome)\n",
        "\n",
        "    return df, name\n",
        "\n",
        "def somarduplas(df,duplas):\n",
        "  names = []\n",
        "  for x in duplas:\n",
        "    for y in duplas:\n",
        "      if x != y:\n",
        "        name1 = x + ' + ' + y\n",
        "        name2 = y + ' + ' + x\n",
        "        print(name1,name2)\n",
        "        if (name1 not in names) and (name2 not in names):\n",
        "          print('both names not in names')\n",
        "          temp = df[x] + df[y]\n",
        "          if name1 not in df.columns: # Vendo se já não existe coluna com esse nome\n",
        "            df.insert(loc = (df.columns.get_loc(x)+1), column = name1, value = temp)\n",
        "            df[name1].replace(1,0,inplace=True)\n",
        "            df[name1].replace(2,1,inplace=True)\n",
        "            names.append(name1)\n",
        "            names.append(name2)\n",
        "\n",
        "  truedf = df.copy()\n",
        "  return truedf\n",
        "\n",
        "def somartrios(df,trios):\n",
        "  names = []\n",
        "  for x in trios:\n",
        "    for y in trios:\n",
        "      for z in trios:\n",
        "        if (x != y) and (x != z) and (y != z):\n",
        "          name1 = x + ' + ' + y + ' + ' + z\n",
        "          name2 = x + ' + ' + z + ' + ' + y\n",
        "          name3 = y + ' + ' + z + ' + ' + x\n",
        "          name4 = y + ' + ' + x + ' + ' + z\n",
        "          name5 = z + ' + ' + x + ' + ' + y\n",
        "          name6 = z + ' + ' + y + ' + ' + x\n",
        "          if (name1 not in names) and (name2 not in names) and (name3 not in names) and (name4 not in names) and (name5 not in names) and (name6 not in names):\n",
        "            temp = df[x] + df[y] + df[z]\n",
        "            if name1 not in df.columns: # Vendo se já não existe coluna com esse nome\n",
        "              df.insert(loc = (df.columns.get_loc(x)+1), column = name1, value = temp)\n",
        "              df[name1].replace(1,0,inplace=True)\n",
        "              df[name1].replace(2,1,inplace=True)\n",
        "              names.append(name1)\n",
        "              names.append(name2)\n",
        "              names.append(name3)\n",
        "              names.append(name4)\n",
        "              names.append(name5)\n",
        "              names.append(name6)\n",
        "\n",
        "  truedf = df.copy()\n",
        "  return truedf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lpw0yQZwSOP"
      },
      "source": [
        "Unicos e Faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLuDtrAkwTao"
      },
      "outputs": [],
      "source": [
        "# Criar tabela que mostra informações importantes de cada variável\n",
        "# Criando a tabela com valores falfantes e quantidade de cada valor\n",
        "\n",
        "def unicosefaltantes(df, organizar = 'Valores Faltantes'):\n",
        "\n",
        "  '''Produza tabelas com tipos de variáveis, quantidade de valores faltantes e únicos para suas tabelas\n",
        "  Diga nos argumentos qual é o banco de dados (primeiro argumento)\n",
        "  Se quiser sortear a tabela por tipo, faltantes ou únicos em ordem descendente,\n",
        "  No segundo argumento, coloque um dos três como string: Tipo de Variável, Valores Faltantes e Valores Únicos\n",
        "  O padrão é sortear por Valores Faltantes'''\n",
        "\n",
        "  variaveis = []\n",
        "  for contagem, col in enumerate(df.columns):\n",
        "    variaveis.append([col,df.dtypes[contagem],df[col].isnull().sum(),len(df[col].value_counts())])\n",
        "\n",
        "  # Organizando a tabela - criar e sortear\n",
        "  variaveis = pd.DataFrame(variaveis,columns=['Variável','Tipo de Variável','Valores Faltantes','Valores Únicos'])\n",
        "  variaveis.sort_values(by=[organizar],ascending=False,inplace=True) # Sorteia por valores faltantes e únicos\n",
        "\n",
        "  return variaveis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQftcwBRwPRQ"
      },
      "source": [
        "## Divisor Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma1J02n_wQPI"
      },
      "outputs": [],
      "source": [
        "'''Criador de novas colunas baseado em resultados de uma única coluna\n",
        "Tens uma coluna onde, dentro de uma pergunta, você quer discriminar se alguém respondeu uma determinada coisa ou não?\n",
        "Exemplo: qual droga você usa? Álcool, Cigarro ou Cocaína? Quer saber se uma pessoa usa álcool sim/não, cigarro sim/não etc?\n",
        "Este código faz isso. Após, será criado um código para \"corrigir\" o que já foi inserido com o código inicial baseado em outras colunas\n",
        "Exemplo: a coluna 1 faz essas perguntas mas também existe as colunas 2 e 3, que também as fazem. É possível alguém marcar umma opção em cada uma delas\n",
        "Agora bastaria só mudar o Sim / Não das colunas já criadas para estar resolvido'''\n",
        "\n",
        "from itertools import chain\n",
        "import pandas as pd\n",
        "\n",
        "def extract_unique_values(dataframe, column_name, separator = ','):\n",
        "    \"\"\"\n",
        "    Extracts unique values from a specified column in a DataFrame where values are comma-separated.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pd.DataFrame): The DataFrame containing the column.\n",
        "        column_name (str): The name of the column to process.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of unique values found in the column.\n",
        "    \"\"\"\n",
        "    # Ensure the column exists in the DataFrame\n",
        "    if column_name not in dataframe.columns:\n",
        "        raise ValueError(f\"The column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "    # Split the values in the column and flatten them\n",
        "    split_values = dataframe[column_name].str.split(separator)\n",
        "    flattened_values = list(chain.from_iterable(split_values))\n",
        "\n",
        "    # Use set to find unique values and convert it back to a list\n",
        "    unique_values = list(set(flattened_values))\n",
        "\n",
        "    return unique_values\n",
        "\n",
        "def divisor(df, colunaprincipal, siglas, significado, colunasextras=None, apagar=False, numerical=True):\n",
        "    \"\"\"\n",
        "    Create new columns in a DataFrame based on the responses in a specific column.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame to process.\n",
        "        colunaprincipal (str): The main column containing comma-separated values.\n",
        "        siglas (list): Abbreviations or keywords to look for within the column.\n",
        "        significado (list): Corresponding meaningful names for new columns to create.\n",
        "        colunasextras (list, optional): Additional columns to consider for adjusting responses. Defaults to None.\n",
        "        apagar (bool, optional): Whether to delete the original columns after processing. Defaults to False.\n",
        "        numerical (bool, optional): Whether to represent responses as numerical ('1', '0') or textual ('Yes', 'No'). Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified DataFrame with new columns added.\n",
        "    \"\"\"\n",
        "    if colunasextras is None:\n",
        "        colunasextras = []\n",
        "\n",
        "    # Part 1: Creating new columns based on the main column\n",
        "    for contagem, valor in enumerate(siglas):\n",
        "        df[significado[contagem]] = df[colunaprincipal].apply(lambda x: 1 if valor in str(x) else 0 if numerical else 'Yes' if valor in str(x) else 'No')\n",
        "\n",
        "    # Part 2: Refining these new columns using additional columns\n",
        "    for coluna in colunasextras:\n",
        "        for idx, val in df[coluna].iteritems():\n",
        "            if val in siglas:\n",
        "                col_idx = siglas.index(val)\n",
        "                df.at[idx, significado[col_idx]] = 1 if numerical else 'Yes'\n",
        "\n",
        "    # Part 3: Optionally removing the original columns\n",
        "    if apagar:\n",
        "        df.drop(columns=[colunaprincipal] + colunasextras, inplace=True)\n",
        "\n",
        "    tempdf = df.copy() # For fragmentation\n",
        "\n",
        "    return tempdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjNY91EfwXo_"
      },
      "source": [
        "Characterize Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoPpXewKwaJK"
      },
      "outputs": [],
      "source": [
        "def calculate_percentage_distribution(df, columns, max_unique_values=10, use_counts=False):\n",
        "    \"\"\"\n",
        "    Calculates and returns a DataFrame containing the distribution (either counts or percentages)\n",
        "    of each value, including NaN, for specified columns in the input DataFrame, excluding columns\n",
        "    with more than 'max_unique_values' unique values.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pandas.DataFrame): The DataFrame containing the data.\n",
        "    - columns (list of str): List of column names to analyze in the DataFrame.\n",
        "    - max_unique_values (int): Maximum number of unique values a column can have to be analyzed.\n",
        "    - use_counts (bool): If True, returns counts instead of percentages. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "    - pandas.DataFrame: A DataFrame where each column corresponds to one of the specified columns\n",
        "      in the input DataFrame and each row corresponds to the distribution of a particular value,\n",
        "      formatted as strings with percentage signs if use_counts is False.\n",
        "    \"\"\"\n",
        "    distribution = {}\n",
        "    skipped_columns = []\n",
        "\n",
        "    for col in columns:\n",
        "        if pd.unique(df[col]).size > max_unique_values:\n",
        "            skipped_columns.append(col)\n",
        "        else:\n",
        "            # Calculate the distribution of each value (including NaN) in the column\n",
        "            if use_counts:\n",
        "                distribution[col] = df[col].value_counts(dropna=False)\n",
        "            else:\n",
        "                distribution[col] = df[col].value_counts(dropna=False, normalize=True) * 100\n",
        "\n",
        "    # Create a DataFrame from the dictionary\n",
        "    distribution_df = pd.DataFrame(distribution)\n",
        "\n",
        "    # Optionally, format the DataFrame for better readability\n",
        "    if not use_counts:\n",
        "        distribution_df = distribution_df.applymap(lambda x: f\"{x:.2f}%\" if pd.notna(x) else \"\")\n",
        "\n",
        "    # Print columns that were skipped\n",
        "    if skipped_columns:\n",
        "        print(\"Skipped columns (more than\", max_unique_values, \"unique values):\", skipped_columns)\n",
        "\n",
        "    return distribution_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTpCFQFyWdjw"
      },
      "source": [
        "Parse description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmR1mbiJWezO"
      },
      "outputs": [],
      "source": [
        "# Function to count if all elements are there\n",
        "def parse_description(desc, all=True):\n",
        "  if all == True:\n",
        "    print('Mode of execution: all need to be present')\n",
        "  else:\n",
        "    print('Mode of execution: at least one needs to be present')\n",
        "  countlist = []\n",
        "  for row in df[colpos]:\n",
        "    count = 0\n",
        "    for element in templist:\n",
        "      if element in row:\n",
        "        count += 1\n",
        "    if all == True:\n",
        "      if count == len(templist):\n",
        "        countlist.append(1)\n",
        "      else:\n",
        "        countlist.append(0)\n",
        "    else:\n",
        "      if count > 0:\n",
        "        countlist.append(1)\n",
        "      else:\n",
        "        countlist.append(0)\n",
        "\n",
        "  return countlist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Cat Comparison"
      ],
      "metadata": {
        "id": "qXh5QMiEX06F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from scipy.stats import chi2_contingency, f_oneway, kruskal, shapiro, ttest_ind, mannwhitneyu\n",
        "\n",
        "def analyze_data_corrected_v8(df, qualitative_variable, numerical_threshold=5,\n",
        "                              p_significance=4, decimal_places=1,\n",
        "                              descending=True, exclude_vars=None, printsteps=False,\n",
        "                              hidetotal=False):\n",
        "    \"\"\"\n",
        "    Adjusted function to include statistical test type and effect size at the end,\n",
        "    format categorical variable output as n (%), and allow configuration for p significance\n",
        "    and decimal places for rounding.\n",
        "\n",
        "    :param df: DataFrame with the data.\n",
        "    :param qualitative_variable: Name of the qualitative variable for analysis.\n",
        "    :param numerical_threshold: Threshold for considering a variable as numerical based on the number of distinct values.\n",
        "    :param p_significance: Number of decimal places for the p-value (default 4).\n",
        "    :param decimal_places: Number of decimal places for rounding results (default 1).\n",
        "    :return: DataFrame formatted with the analysis results.\n",
        "    \"\"\"\n",
        "\n",
        "    if exclude_vars is None:\n",
        "        exclude_vars = []\n",
        "\n",
        "    # Ensure that numerical columns are correctly interpreted\n",
        "    for col in df.columns:\n",
        "        if col not in exclude_vars and col != qualitative_variable:\n",
        "            df.loc[:, col] = pd.to_numeric(df[col].replace('<NA>', np.nan), errors='ignore')\n",
        "\n",
        "    # Get counts of each category and sort them if needed\n",
        "    if descending:\n",
        "        category_counts = df[qualitative_variable].astype(str).value_counts().sort_index()\n",
        "        categories = category_counts.index.tolist()\n",
        "        output_columns = ['Variables'] + [f'{cat} (n = {category_counts[cat]})' for cat in categories]\n",
        "    else:\n",
        "        categories = df[qualitative_variable].unique()\n",
        "        output_columns = ['Variables'] + [f'{cat} (n = {len(df[df[qualitative_variable] == cat])})' for cat in categories]\n",
        "\n",
        "    total_observations = len(df)\n",
        "    results = []\n",
        "\n",
        "    # Statistics for the entire dataset for categorical variables\n",
        "    total_dataset_stats = []\n",
        "    for column in df.columns:\n",
        "        if column in exclude_vars or column == qualitative_variable:\n",
        "            continue\n",
        "        if printsteps:\n",
        "            print('Producing output for variable:', column)\n",
        "        try:\n",
        "            if pd.api.types.is_numeric_dtype(df[column]) and df[column].nunique() >= numerical_threshold:\n",
        "                numeric_data = df[column].dropna().astype(float)  # Ensure conversion to float\n",
        "                mean = round(numeric_data.mean(), decimal_places)\n",
        "                std_dev = round(numeric_data.std(), decimal_places)\n",
        "                median = round(numeric_data.median(), decimal_places)\n",
        "                iqr = round(numeric_data.quantile(0.75) - numeric_data.quantile(0.25), decimal_places)\n",
        "                stats_str = f\"mean: {mean} ± {std_dev}, median: {median} (IQR: {iqr})\"\n",
        "            else:\n",
        "                category_percent = df[column].dropna().value_counts(normalize=True) * 100\n",
        "                percent_str = \"\"\n",
        "                for sub_cat, percent in category_percent.items():\n",
        "                    n_sub_cat = df[column].dropna().value_counts()[sub_cat]\n",
        "                    percent = round(percent, decimal_places)\n",
        "                    percent_str += f\"{sub_cat}: {n_sub_cat} ({percent}%), \"\n",
        "                stats_str = percent_str.strip(', ')\n",
        "            total_dataset_stats.append(stats_str)\n",
        "            if printsteps:\n",
        "                print('Stats:', stats_str)\n",
        "        except Exception as e:\n",
        "            print(e, 'during data organization in:', column)\n",
        "            pass\n",
        "\n",
        "    for column in df.columns:\n",
        "        if column in exclude_vars or column == qualitative_variable:\n",
        "            continue\n",
        "\n",
        "        # Initialize new_row with all possible category keys using the full dataset counts\n",
        "        new_row = {'Variables': column}\n",
        "        for cat in categories:\n",
        "            new_row[f'{cat} (n = {category_counts[cat]})'] = \"No data\"\n",
        "        p_value = None\n",
        "        test_type = ''\n",
        "\n",
        "        # Drop NaNs from the non-missing data for this column\n",
        "        non_missing_data = df[column].dropna()\n",
        "        percent_non_missing = len(non_missing_data) / total_observations * 100\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(df[column]) and df[column].nunique() >= numerical_threshold:\n",
        "            if printsteps:\n",
        "                print('Statistical analysis for numerical variable:', column)\n",
        "            # Numeric variable\n",
        "            try:\n",
        "                # Drop NaNs from each group\n",
        "                groups = [group_data[column].dropna().astype(float) for _, group_data in df.groupby(qualitative_variable)]\n",
        "                if len(categories) == 2:  # Binary outcome\n",
        "                    if all(len(group) >= 3 for group in groups):\n",
        "                        if all(shapiro(group.dropna())[1] > 0.05 for group in groups):\n",
        "                            p_value = ttest_ind(groups[0], groups[1], nan_policy='omit')[1]\n",
        "                            test_type = 't-test'\n",
        "                        else:\n",
        "                            p_value = mannwhitneyu(groups[0], groups[1], nan_policy='omit')[1]\n",
        "                            test_type = 'Mann-Whitney'\n",
        "                else:  # 3 or more outcomes\n",
        "                    if all(len(group) >= 3 for group in groups):\n",
        "                        if all(shapiro(group.dropna())[1] > 0.05 for group in groups):\n",
        "                            p_value = f_oneway(*groups)[1]\n",
        "                            test_type = 'ANOVA'\n",
        "                        else:\n",
        "                            p_value = kruskal(*groups)[1]\n",
        "                            test_type = 'Kruskal-Wallis'\n",
        "\n",
        "                for cat in categories:\n",
        "                    group = df[df[qualitative_variable] == cat][column].dropna().astype(float)\n",
        "                    if not group.empty:  # Check if the group is not empty\n",
        "                        mean = round(group.mean(), decimal_places)\n",
        "                        std_dev = round(group.std(), decimal_places)\n",
        "                        median = round(group.median(), decimal_places)\n",
        "                        iqr = round(group.quantile(0.75) - group.quantile(0.25), decimal_places)\n",
        "                        new_row[f'{cat} (n = {len(df[df[qualitative_variable] == cat])})'] = f\"mean: {mean} ± {std_dev}, median: {median} (IQR: {iqr})\"\n",
        "            except Exception as e:\n",
        "                print(e, 'during numerical statistical analysis in:', column)\n",
        "                pass\n",
        "\n",
        "        else:\n",
        "            if printsteps:\n",
        "                print('Statistical analysis for categorical variable:', column)\n",
        "\n",
        "            # Categorical variable\n",
        "            try:\n",
        "                contingency_table = pd.crosstab(df[column].dropna(), df[qualitative_variable].dropna())\n",
        "                if not contingency_table.empty:\n",
        "                    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
        "                    test_type = 'Chi-square'\n",
        "                    for cat in categories:\n",
        "                        total_group = df[df[qualitative_variable] == cat].shape[0]\n",
        "                        category_percent = contingency_table[cat] / total_group\n",
        "                        percent_str = \"\"\n",
        "                        for sub_cat, val in category_percent.items():\n",
        "                            n_sub_cat = round(val * total_group)\n",
        "                            percent = round(val * 100, decimal_places)\n",
        "                            percent_str += f\"{sub_cat}: {n_sub_cat} ({percent}%), \"\n",
        "                        new_row[f'{cat} (n = {total_group})'] = percent_str.strip(', ')\n",
        "            except Exception as e:\n",
        "                print(e, column)\n",
        "                pass\n",
        "\n",
        "        new_row['p-value'] = round(p_value, p_significance) if p_value is not None else 99\n",
        "        new_row['Test Type'] = test_type\n",
        "        new_row['% Complete Data'] = f\"{round(percent_non_missing, decimal_places)}%\"\n",
        "        results.append(new_row)\n",
        "        if printsteps:\n",
        "            print(new_row)\n",
        "\n",
        "    results_df = pd.DataFrame(results, columns=output_columns + ['p-value', 'Test Type', '% Complete Data'])\n",
        "    if not hidetotal:\n",
        "        results_df.insert(1, f'Total (n = {len(df)})', total_dataset_stats)\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "DpQS9BgtQZAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU50u3TS-pS0"
      },
      "source": [
        "Organizing the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oGwU2tI-pS0"
      },
      "outputs": [],
      "source": [
        "def detailed_output_analysis_condensed(df_output):\n",
        "    \"\"\"\n",
        "    Function to process the output of the analysis function and detail each subcategory of categorical variables\n",
        "    in a single line, condensing the data of each column.\n",
        "\n",
        "    :param df_output: DataFrame resulting from the analysis function.\n",
        "    :return: Detailed DataFrame with subcategories in condensed lines.\n",
        "    \"\"\"\n",
        "    detailed_results = []\n",
        "\n",
        "    for index, row in df_output.iterrows():\n",
        "        variable = row['Variables']\n",
        "        new_base_line = row.to_dict()\n",
        "\n",
        "        # Agora começando da coluna 'Total Dataset'\n",
        "        if any(':' in str(new_base_line[col]) for col in df_output.columns[1:-3]):\n",
        "            subcat_dict = {}\n",
        "            for col in df_output.columns[1:-3]:  # Ajuste aqui também\n",
        "                subcategories = str(new_base_line[col]).split(', ')\n",
        "                for subcat in subcategories:\n",
        "                    if ':' in subcat:\n",
        "                        parts = subcat.split(': ')\n",
        "                        subcat_name = parts[0].strip()\n",
        "                        values = ': '.join(parts[1:])\n",
        "                        subcat_dict.setdefault(subcat_name, new_base_line.copy())\n",
        "                        subcat_dict[subcat_name]['Variables'] = f\"{variable}, {subcat_name}\"\n",
        "                        subcat_dict[subcat_name][col] = values.strip()\n",
        "\n",
        "            detailed_results.extend(subcat_dict.values())\n",
        "        else:\n",
        "            detailed_results.append(new_base_line)\n",
        "\n",
        "    return pd.DataFrame(detailed_results)\n",
        "\n",
        "# This is needed because the above function creates unnecessary lines\n",
        "def condense_rows(df):\n",
        "    \"\"\"\n",
        "    Function to condense information from lines with the same variable name into a single line.\n",
        "\n",
        "    :param df: DataFrame with potentially repeated lines for each variable.\n",
        "    :return: Condensed DataFrame.\n",
        "    \"\"\"\n",
        "    condensed_df = pd.DataFrame()\n",
        "    unique_variables = df['Variables'].unique()\n",
        "\n",
        "    for var in unique_variables:\n",
        "        var_lines = df[df['Variables'] == var]\n",
        "        condensed_line = var_lines.iloc[0].copy()  # Copy the first line as a base\n",
        "\n",
        "        # Iterate through columns to condense information\n",
        "        for col in df.columns[:-3]:  # Include \"Total Dataset\" and other columns, ignore the last 3 columns\n",
        "            column_data = var_lines[col].dropna().unique()\n",
        "            condensed_line[col] = ', '.join(map(str, column_data))\n",
        "\n",
        "        # Append using concat instead of the deprecated append method\n",
        "        condensed_df = pd.concat([condensed_df, pd.DataFrame([condensed_line])], ignore_index=True)\n",
        "\n",
        "    return condensed_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML data organization"
      ],
      "metadata": {
        "id": "K7WUlzg_2cBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "3N_2Vqd12eWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading"
      ],
      "metadata": {
        "id": "AeZNi-jB5CL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add path to ml .xlsx file\n",
        "ml = pd.read_excel('[NAME OF THE EXCEL FILE].xlsx')\n",
        "ml.head()"
      ],
      "metadata": {
        "id": "gEdfsHjr2lzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Variables"
      ],
      "metadata": {
        "id": "2yPK13ln5DMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Year divide"
      ],
      "metadata": {
        "id": "pmL275Qk5EXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = [2020]\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Year of publication'\n",
        "newcolname = 'Before_2020_After' # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos] <= criteria[0], 0, 1)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "uRxbx35z3RjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation"
      ],
      "metadata": {
        "id": "CvgO-K3L5Iyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = ['Not Performed', 'Not Clearly Stated']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Data imputation'\n",
        "newcolname = colpos + \"_Yes_or_No\" # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos] == criteria[0], 0, 1)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "Ph_aKx2g5LkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml.columns"
      ],
      "metadata": {
        "id": "YxiVYIKa6H10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering other\n",
        "\n"
      ],
      "metadata": {
        "id": "dZuLaX5t9Yyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = ['k-Means', 'k-Means']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Clustering Method'\n",
        "newcolname = colpos + \"_K_Means_Yes_or_No\" # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos].isin(criteria), 1, 0)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "QLWY1FB49Yye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = ['Hierarchical Clustering', 'Ward Hierarchical Clustering', 'Hierarchical Clustering ,Two-step clustering (SPSS)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Clustering Method'\n",
        "newcolname = colpos + \"_Hierarchical_Yes_or_No\" # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos].isin(criteria), 1, 0)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "5a8RmU8L9yH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = ['Hierarchical Clustering', 'Ward Hierarchical Clustering', 'Hierarchical Clustering ,Two-step clustering (SPSS)',\n",
        "            'k-Means', 'k-Means']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Clustering Method'\n",
        "newcolname = colpos + \"_Hierarchical_or_K-means_Yes_or_No\" # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos].isin(criteria), 1, 0)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "FF9CGaaS9_Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature reduction"
      ],
      "metadata": {
        "id": "cJV2PX_55_Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = ['Not Performed', 'Not Clearly Stated']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Feature Reduction Strategy'\n",
        "newcolname = colpos + \"_Yes_or_No\" # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos] == criteria[0], 0, 1)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "zQmwcu6T6Asg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "_zEBoCg76fE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temporal stability"
      ],
      "metadata": {
        "id": "qO7HHqK36xDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = ['No temporal stability was for the identified subtypes', 'Not clearly stated']\n",
        "\n",
        "# Preparatory settings Validation method\n",
        "colpos = 'Temporal Stability'\n",
        "newcolname = colpos + \"_Yes_or_No\" # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos] == criteria[0], 0, 1)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "yBlIWtV26yse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code availability"
      ],
      "metadata": {
        "id": "lhzsK1i86-Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = ['No', 'code not available', 'mas o link não funciona', 'Link not working. https://github.com/mbbrendel/MBBS-Subtyping']\n",
        "\n",
        "# Preparatory settings Validation method\n",
        "colpos = 'Code Provided'\n",
        "newcolname = colpos + \"_Yes_or_No\" # Name of the new column\n",
        "insert_position = ml.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(ml[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        ml[colpos] == criteria[0], 0, 1)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  ml.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  ml[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(ml[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "XpkCxqC-6_qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "O0yLX6Bt69Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dependentvar = 'Before_2020_After'\n",
        "\n",
        "# Running the code - first creating the master table\n",
        "comparison_analysis = analyze_data_corrected_v8(ml,\n",
        "                                                qualitative_variable = dependentvar,\n",
        "                                                p_significance=4, decimal_places=1,\n",
        "                                                exclude_vars = ['PMID', 'Article Title', 'Author', 'Year of publication'],\n",
        "                                                numerical_threshold=5, printsteps = True, hidetotal = False)\n",
        "\n",
        "# Running the adjustment function\n",
        "# First one - creates new rows for categories\n",
        "detailed_df = detailed_output_analysis_condensed(comparison_analysis)\n",
        "\n",
        "# Second one - adjusts the last created output\n",
        "condensed_df = condense_rows(detailed_df)\n",
        "print('Lenght of dataset:',len(condensed_df))\n",
        "condensed_df.head(10)"
      ],
      "metadata": {
        "id": "8S3129n73amk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the result\n",
        "sheet_path = 'Tables/'+dependentvar+'_ML_analysis.xlsx'\n",
        "sheet_name = dependentvar[0:31]\n",
        "\n",
        "with pd.ExcelWriter(sheet_path, engine='xlsxwriter') as writer:\n",
        "    condensed_df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width\n",
        "    for column in condensed_df:\n",
        "        column_width = max(condensed_df[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = condensed_df.columns.get_loc(column)\n",
        "        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_width)\n",
        "\n",
        "    # Ensure p-value is treated as numeric\n",
        "    if 'p-value' in condensed_df.columns:\n",
        "        condensed_df['p-value'] = pd.to_numeric(condensed_df['p-value'], errors='coerce')\n",
        "\n",
        "    # Only significant interactions\n",
        "    sigcondensed = condensed_df[condensed_df['p-value'] < 0.05]\n",
        "    sheet_name = 'Significant'\n",
        "\n",
        "    sigcondensed.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width for the significant sheet\n",
        "    for column in sigcondensed:\n",
        "        column_width = max(sigcondensed[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = sigcondensed.columns.get_loc(column)\n",
        "        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_width)"
      ],
      "metadata": {
        "id": "B22A7T9L3is7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68m7i2_fH3Zq"
      },
      "source": [
        "# Reading and exporting\n",
        "\n",
        "Each variable of interest is evaluated, then measures for how to divide and better analyse this variable are studied, if applicable\n",
        "\n",
        "Inspiration for analyses: https://docs.google.com/document/d/12u1vc_-MHrKS5Cd8AIm9HI8wf0qoOtdrMvWz8TttRnw/edit?usp=sharing\n",
        "\n",
        "Dataset: https://docs.google.com/spreadsheets/d/1IOgFD3-STSSr40UDL4XWNSGIvi70rFzT/edit?usp=sharing&ouid=113124543929039104946&rtpof=true&sd=true\n",
        "\n",
        "Forms: https://docs.google.com/forms/u/2/d/1Q9f2LCurKg6TzKlKtFWTNe1MyZbQpFsws6zVB0nyHzM/edit?usp=drive_web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_pKf0R8Ivvs"
      },
      "outputs": [],
      "source": [
        "# Reading - add name of the file\n",
        "df1 = pd.read_excel('[NAME OF THE EXCEL FILE].xlsx')\n",
        "\n",
        "# Checking\n",
        "print(df1.columns)\n",
        "print(df1.shape)\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra data up to May/24"
      ],
      "metadata": {
        "id": "fVnLyIvyS6UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New data - papers that need to be included after the first search (up to May 2024)\n",
        "df2 = pd.read_excel('[NAME OF THE EXCEL FILE].xlsx')\n",
        "\n",
        "# Checking\n",
        "print(df2.columns)\n",
        "print(df2.shape)\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "r1H1v_ehLHWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking what is misssing between datasets"
      ],
      "metadata": {
        "id": "NBDCnTUJTL98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get columns of both DataFrames\n",
        "df_columns = set(df1.columns)\n",
        "df2_columns = set(df2.columns)\n",
        "\n",
        "# Columns in df but not in df2\n",
        "columns_in_df_not_in_df2 = df_columns - df2_columns\n",
        "\n",
        "# Columns in df2 but not in df\n",
        "columns_in_df2_not_in_df = df2_columns - df_columns\n",
        "\n",
        "# Print the results\n",
        "print(\"Columns in df but not in df2:\", columns_in_df_not_in_df2)\n",
        "print(\"Columns in df2 but not in df:\", columns_in_df2_not_in_df)\n",
        "\n",
        "# For a more detailed view, you can create a summary\n",
        "summary = {\n",
        "    'Columns in df but not in df2': list(columns_in_df_not_in_df2),\n",
        "    'Columns in df2 but not in df': list(columns_in_df2_not_in_df)\n",
        "}\n",
        "\n",
        "# Display the summary\n",
        "summary_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in summary.items()]))\n",
        "summary_df"
      ],
      "metadata": {
        "id": "0ma4QVyAP_vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uniting dataset"
      ],
      "metadata": {
        "id": "SCsCyjYxTBjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge DataFrames vertically\n",
        "df = pd.concat([df1, df2], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "oEh7uWMWTCj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting what is of interest"
      ],
      "metadata": {
        "id": "D-b2d-F7TOtI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "331Nj9djJRmH"
      },
      "outputs": [],
      "source": [
        "# Selecting only possible clinical variables of interest (no ML analyses)\n",
        "df = df[['Article Title',\n",
        "       'Author (only the last name)', 'Year of publication', 'Reviewer', 'Inclusion Criteria',\n",
        "       'Exclusion Criteria',\n",
        "       'Which type of PD patients were included in the clustering analyses?',\n",
        "       'Were subtyped patients roughly at the same disease stage or duration?',\n",
        "       'From how many different centers came the patients to be subtyped?',\n",
        "       'How were patients recruited?', 'Which diagnostic criteria was utilized to diagnose PD?',\n",
        "       'Which was the sampling method?', 'Was there a longitudinal follow-up in any way?',\n",
        "       'If longitudinal follow-up was provided, how much years were patients followed for?',\n",
        "       'Completeness of follow-up',\n",
        "       'Which type of descriptive data regarding the PD population was presented?',\n",
        "       'Were the included patients for clustering purposes drug-naive?',\n",
        "       'Number of PD patients utilized for clustering purposes',\n",
        "       'Number of PD patients utilized for validation purposes',\n",
        "       'Did the study include healthy controls in the clustering analyses alongside PD patients?',\n",
        "       'Number of healthy controls utilized for validation or technique elaboration purposes',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [AMP-PD]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [BioFIND]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Fox Insight]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [GP2 Dataset]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [LRRK2 Cohort Consortium]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson Progression Marker Initiative (PPMI)]',\n",
        "       \"Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson's Disease Biomarker Program (PDBP)]\",\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [UK Biobank]',\n",
        "      'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Oxford Parkinson Disease Center Discovery Cohort]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]',\n",
        "       'If a different publicly available dataset than the ones mentioned above was utlized for any means, please specify its full name',\n",
        "       'From which countries or world region came the patients whose data was used in the clustering?',\n",
        "       'Which type data was utilized for the clustering algorithm?',\n",
        "       'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?',\n",
        "       'If the paper contains multiple exploration of clusters (creates different subgroups multiple times) shortly describe the differences in the cluster strategy for each exploration:',\n",
        "       'If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?',\n",
        "       'If neuroimaging data was utilized for clustering, did the article perform any sort of quality control measure?',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI cortical thickness]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI subcortical volume]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI white matter lesions / hyperintensities]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [Functional MRI]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI Others]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [PET]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [SPECT DAT binding ratios]',\n",
        "       'How many subtypes did the evaluated study identified?',\n",
        "       'If they were identified, what are the names (meaning) of the identified clusters?',\n",
        "       'What is the number of patients present in each of the identified clusters?',\n",
        "       'Subtype stability','Did the article provide an individual level prediction algorithm?']].reset_index(drop=True)\n",
        "\n",
        "'''Saving here some columns that were excluded because they are subjective and were not reviewed\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 1) [Subtyping Algorithm]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 1) [Time Required]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 1) [Applicable to Drug-Naive Stage]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 1) [Cost]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 1) [Applicability to general population]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 2) [Prognostic Value]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 2) [Treatment Implication]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 2) [Clinical importance of differences in the variables used to define subtypes]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 2) [Clinical importance of differences in external clinical/demographic features between subtypes]\",\n",
        "       \"Based on all the information you've read, how would you rate the clinical applicability of the studied subtypes? (Part 2) [Clinical importance of differences in pathological/biomarker features between subtypes]\"'''\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gg3_k2xKF4W"
      },
      "outputs": [],
      "source": [
        "# The \"Reviewer\" has the \"Consensus\" value, which is the entry for the aforementioned\n",
        "print(df['Reviewer'].value_counts(dropna=False))\n",
        "print('Before subsetting',df.shape)\n",
        "df = df[df['Reviewer'] == 'Consensus'].reset_index(drop=True)\n",
        "print('After subsetting',df.shape)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df), len(df1), len(df2)"
      ],
      "metadata": {
        "id": "-xl56qXrT1Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the joined dataset"
      ],
      "metadata": {
        "id": "dXGuCpqFfW0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Review_Dataset.csv')"
      ],
      "metadata": {
        "id": "0c-I5VgnfchV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "XQWxw1TzfalF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial variable creation"
      ],
      "metadata": {
        "id": "vw1tGD0Uf97L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Review_Dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "l8vepTR0CJ7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyMXBxFcKwdE"
      },
      "source": [
        "## Good variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l10vFUa0KyFn"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDC3AV7UK0mb"
      },
      "outputs": [],
      "source": [
        "relevantvars = ['Article Title', 'Author (only the last name)', 'Year of publication',\n",
        "                'Number of PD patients utilized for clustering purposes',\n",
        "                'Number of healthy controls utilized for validation or technique elaboration purposes',\n",
        "                'How many subtypes did the evaluated study identified?']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of groups"
      ],
      "metadata": {
        "id": "XL1ZpbrHNfLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting in numeric\n",
        "df['Number of PD patients utilized for clustering purposes for bubble'] = df['Number of PD patients utilized for clustering purposes']\n",
        "df['Number of PD patients utilized for clustering purposes'] = pd.to_numeric(df['Number of PD patients utilized for clustering purposes'], errors='coerce').astype('Int64')\n",
        "df['Number of healthy controls utilized for validation or technique elaboration purposes'] = pd.to_numeric(df['Number of healthy controls utilized for validation or technique elaboration purposes'], errors='coerce').astype('Int64')\n",
        "df['How many subtypes did the evaluated study identified?'] = pd.to_numeric(df['How many subtypes did the evaluated study identified?'], errors='coerce').astype('Int64')"
      ],
      "metadata": {
        "id": "nZJSS2PDOFA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPuCccfyObO4"
      },
      "source": [
        "## Monogenic x sporadic\n",
        "\n",
        "It is important to know exactly the genetic background of the population you are studying. Have the reviewed studies done so? This information is presented in two columns, with the second one needing to be divided for its subvalues\n",
        "\n",
        "First: 'Which type of PD patients were included in the clustering analyses?'\n",
        "\n",
        "Second: 'Which type of descriptive data regarding the PD population was presented?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fp15563PSvf"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = ['None of the mentioned data on the PD population was informed by the study',\n",
        "           'Age', 'Gender', 'Ethnicity', 'Education', 'Income', 'Family History',\n",
        "           'Age at disease onset', 'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)',\n",
        "           'Vital Signs (blood pressure, cardiac frequency)', 'Physical Attributes (weight, height etc)',\n",
        "           'Medication Doses', 'Genetic Status (monogenic PD, polygenic risk score)',\n",
        "           'Summarised data from clinical scales (such as mean MDS-UPDRS scores, number of non-motor symptoms)',\n",
        "           'Summarised data from neuroimaging studies (such as cortical thickness)',\n",
        "           \"Biomarker's profile (results from biomarker tests)\"]\n",
        "\n",
        "# Preparing to run the function\n",
        "main_column = 'Which type of descriptive data regarding the PD population was presented?'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if siglas[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + siglas\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "df['Genetic Status (monogenic PD, polygenic risk score)'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YLU_X1AO9_6"
      },
      "outputs": [],
      "source": [
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, siglas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RC9AKL2dZPt"
      },
      "source": [
        "Creating the definition of not possessing genetic information\n",
        "\n",
        "If study reported either genetic information or makes some distinction between study types in their definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEH9FhG0ejxF"
      },
      "outputs": [],
      "source": [
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    (df['Genetic Status (monogenic PD, polygenic risk score)'] == 1) |\n",
        "    (df['Which type of PD patients were included in the clustering analyses?'] == 'Monogenic and Sporadic / Idiopathic'),\n",
        "    1, 0)\n",
        "\n",
        "colpos = 'Which type of PD patients were included in the clustering analyses?' # Name of the column position to be inserted\n",
        "newcolname = 'Genetic_Status_Informed' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[['Article Title',colpos,newcolname,'Genetic Status (monogenic PD, polygenic risk score)']].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqRgM-vi6eVc"
      },
      "source": [
        "## Diagnostic criteria\n",
        "\n",
        "More on PDBP Dx Criteria (highly variable): https://movementdisorders.onlinelibrary.wiley.com/doi/full/10.1002/mds.26438?casa_token=0Kk48i-Na7sAAAAA%3A80sx8jSKQVLwmT8ixQfiqqxSBT9PeFOedBg-nXQ_v24Bm3DzTtt8BeHgW0fQwk4pNzfnTxCaaCOs6L7f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF3dEQN3_rnQ"
      },
      "outputs": [],
      "source": [
        "# Checking\n",
        "df['Which diagnostic criteria was utilized to diagnose PD?'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mebuloT6hHc"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = ['UK Brain Bank Criteria', 'MDS Clinical Criteria',\n",
        "          'Clinical Diagnosis + Positive Dopamine Transporter (DAT) SPECT (PPMI)',\n",
        "          'Clinical Diagnosis (no specific criteria mentioned - PDBP)',\n",
        "          'Douglas Criteria',\n",
        "          'World Health Organization Criteria',\n",
        "          'ICD-10 Criteria',\n",
        "          'Postmortem pathological diagnosis (part of the sample)',\n",
        "          'Postmortem pathological diagnosis (complete sample)',\n",
        "          'Self Report',\n",
        "          'Other criteria',\n",
        "          'Not sure or clearly stated']\n",
        "\n",
        "# Preparing to run the function\n",
        "main_column = 'Which diagnostic criteria was utilized to diagnose PD?'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if siglas[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + siglas\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, siglas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSY3z1Nz8pK1"
      },
      "source": [
        "Dividing into having a formal criteria or having an unkown or self report criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6okh8xnO8vD1"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Self Report', 'Not sure or clearly stated', 'Clinical Diagnosis (no specific criteria mentioned - PDBP)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which diagnostic criteria was utilized to diagnose PD?' # Analysis column and position\n",
        "newcolname = 'Formal_Criteria_Utilized_or_Informed' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    df[colpos].isin(criteria),\n",
        "    0, 1) # Inverted here\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXOHIHnO_NPM"
      },
      "source": [
        "According to Mestre classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW6Th27__SLw"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Self Report', 'Not sure or clearly stated', 'Clinical Diagnosis (no specific criteria mentioned - PDBP)']\n",
        "criteria2 = ['Postmortem pathological diagnosis (part of the sample)', 'Postmortem pathological diagnosis (complete sample)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which diagnostic criteria was utilized to diagnose PD?' # Analysis column and position\n",
        "newcolname = 'Formal_Criteria_From_Mestre' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    df[colpos].isin(criteria),  # First criterion\n",
        "    0,  # No formal criteria - 0\n",
        "    np.where(\n",
        "        df[colpos].isin(criteria2),  # Second criterion\n",
        "        2, # Highest standadrd - pathological diagnosis\n",
        "        1  # Formal criteria, but not pathological diagnosis\n",
        "    )\n",
        ")\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMp_qFTNoX6E"
      },
      "source": [
        "## Recruitment strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTkJyTgkoh5e"
      },
      "outputs": [],
      "source": [
        "# Checking\n",
        "df['How were patients recruited?'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEdu7Wqnobd1"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Community or population based']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'How were patients recruited?' # Analysis column and position\n",
        "newcolname = 'Recruitment_Community_or_Population' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    df[colpos].isin(criteria),\n",
        "    1, 0)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sxAux11AuYg"
      },
      "source": [
        "## Cohorts used\n",
        "\n",
        "The values here is already in different and individual columns, but it has \"Creation and \"Validation\" options. We should create new columns to update them only if this cohort was utilized or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwrOPtlWmrtD"
      },
      "source": [
        "**Individual datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfYczbq8AuYm"
      },
      "outputs": [],
      "source": [
        "# List of columns\n",
        "templist = ['Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [AMP-PD]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [BioFIND]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Fox Insight]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [GP2 Dataset]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [LRRK2 Cohort Consortium]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson Progression Marker Initiative (PPMI)]',\n",
        "       \"Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson's Disease Biomarker Program (PDBP)]\",\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [UK Biobank]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Oxford Parkinson Disease Center Discovery Cohort]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOW_jrXhAuYm"
      },
      "outputs": [],
      "source": [
        "# Create new columns based on whether the original columns are NaN or not\n",
        "newlist = []\n",
        "for col in templist:\n",
        "    df['Dataset_Used ' + col] = np.where(df[col].isna(), 0, 1)\n",
        "    newlist.append('Dataset_Used ' + col)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newlist[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + newlist\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Viewing\n",
        "df[[newlist[5],'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson Progression Marker Initiative (PPMI)]']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5O7qqhOWTH0"
      },
      "source": [
        "**Sum of elements**\n",
        "\n",
        "Sum of columns that have creation and columns that have validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9DaZqJRWXvh"
      },
      "outputs": [],
      "source": [
        "# Create new columns based on whether the original columns are NaN or not\n",
        "creationlist = []\n",
        "validationlist = []\n",
        "\n",
        "for row in range(len(df)):\n",
        "  tempnumber = 0\n",
        "  tempnumber2 = 0\n",
        "  for col in templist:\n",
        "    if 'Creation' in str(df.at[row, col]):\n",
        "      tempnumber = tempnumber + 1\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    if 'Validation' in str(df.at[row, col]):\n",
        "      tempnumber2 = tempnumber2 + 1\n",
        "    else:\n",
        "      pass\n",
        "  creationlist.append(tempnumber)\n",
        "  validationlist.append(tempnumber2)\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]' # Analysis column and position\n",
        "newcolname = 'Creation_Datasets_Sum' # Name of the new column\n",
        "newcolname2 = 'Validation_Datasets_Sum' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, creationlist) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, validationlist) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = creationlist # Updating columns\n",
        "  df[newcolname2] = validationlist # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newlist[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + newcolname + newcolname2\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Viewing\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))\n",
        "df[[newcolname,newcolname2]+templist].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmqZAocIYxnx"
      },
      "outputs": [],
      "source": [
        "# What are those values 0 in creation? All datasets needs to have creation to be in this analysis\n",
        "pd.set_option('display.max_colwidth', 25) # Maximum width of each column\n",
        "df[df[newcolname] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5SgUo7waB3q"
      },
      "outputs": [],
      "source": [
        "# What are those studies with 0 in validation?\n",
        "pd.set_option('display.max_colwidth', 25) # Maximum width of each column\n",
        "df[df[newcolname2] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLfuaDscdZP9"
      },
      "outputs": [],
      "source": [
        "# Checking if performed validation in a different cohort\n",
        "validation_cols = df[templist].apply(lambda x: x == 'Validation').any(axis=1)\n",
        "validationint = validation_cols.astype(int)\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Datasets_Sum' # Analysis column and position\n",
        "newcolname = 'Validation_in_a_Different_Cohort' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, validationint) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = validationint # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newlist[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + newcolname\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Checking how it goes\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[[newcolname] + templist].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVOoqOKXmtJu"
      },
      "source": [
        "**Open access datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXklRP67nZ9q"
      },
      "outputs": [],
      "source": [
        "# Remove local datasets from the list\n",
        "try:\n",
        "  newlist.remove('Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]')\n",
        "except:\n",
        "  print('Already removed the local or specific dataset column')\n",
        "\n",
        "# Sum all values and, if it 1 or gre# Checking if additional and relevant data was lost\n",
        "df['Open_Datasets_Used_Sum'] = df[newlist].sum(axis=1)\n",
        "df['Open_Datasets_Used_Yes_or_No'] = df['Open_Datasets_Used_Sum'].apply(lambda x: 1 if x >= 1 else 0)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if 'Open_Datasets_Used_Sum' not in relevantvars:\n",
        "  relevantvars.append('Open_Datasets_Used_Sum')\n",
        "  relevantvars.append('Open_Datasets_Used_Yes_or_No')\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Filter rows where 'Cluster_Sum' is 0, indicating no siglas were found in these rows\n",
        "print(df['Open_Datasets_Used_Sum'].value_counts(dropna=False))\n",
        "print(df['Open_Datasets_Used_Yes_or_No'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcXSVP76Hjzo"
      },
      "source": [
        "## Countries represented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBFKgYcEHowx"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = [\"Sub-Saharan Africa\", \"Middle East & North Africa\", \"East Asia & Pacific (China, Japan, Australia)\",\n",
        "          \"South Asia (India, Pakistan, Bangladesh)\", \"Europe & Central Asia\", \"Latin America & the Caribbean\",\n",
        "          \"North America\"]\n",
        "\n",
        "# Preparing to run the function\n",
        "main_column = 'From which countries or world region came the patients whose data was used in the clustering?'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if siglas[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + siglas\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "df[main_column].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kQXwXaVLvWF"
      },
      "source": [
        "## Similar PD staging\n",
        "\n",
        "Ideally, subtyped patents should be in the same disease stage of duration, for factors such as a different progression not to interfere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTKI5dE88ncd"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Were subtyped patients roughly at the same disease stage or duration?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fSC6dw4L3yw"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Yes']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Similar_PD_Staging' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JvHiWegPhTU"
      },
      "source": [
        "## Longitudinal follow-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITcGL3WzgPRo"
      },
      "source": [
        "**Follow-up status**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hv08cV-6YXO"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Was there a longitudinal follow-up in any way?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x2zg0JN6WZY"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['The study utilized longitudinal clinical data for clustering and/or association or interaction analyses']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Longitudinal_Followup_Status' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkoJBxSdgUXB"
      },
      "source": [
        "**Time and completeness**\n",
        "\n",
        "If longitudinal follow-up was provided, how much years were patients followed for?\tCompleteness of follow-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtwtATcxgZEp"
      },
      "outputs": [],
      "source": [
        "print(df['If longitudinal follow-up was provided, how much years were patients followed for?'].value_counts(dropna=False))\n",
        "print(df['Completeness of follow-up'].value_counts(dropna=False))\n",
        "\n",
        "# Formal criteria list\n",
        "criteria = ['The study utilized longitudinal clinical data for clustering and/or association or interaction analyses']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Longitudinal_Followup_Time' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRPORyaNjDWu"
      },
      "outputs": [],
      "source": [
        "# Just creating new column only with numbers and forcing strings to become NaN for both exames\n",
        "\n",
        "# Creating colnames\n",
        "newcolname = 'Longitudinal_Followup_Years'\n",
        "newcolname2 = 'Completeness_of_Followup'\n",
        "colpos = 'Completeness of follow-up'\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Creating new Series with forced conversion and NaN for non-numeric values\n",
        "newcol = pd.to_numeric(df['If longitudinal follow-up was provided, how much years were patients followed for?'], errors='coerce')\n",
        "newcol2 = pd.to_numeric(df['Completeness of follow-up'], errors='coerce')\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))\n",
        "df[[newcolname,newcolname2]].describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_p5GfAKDmuX"
      },
      "source": [
        "**Mestre definitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RquzzbNRDpDU"
      },
      "outputs": [],
      "source": [
        "# Checking columns\n",
        "df[['Longitudinal_Followup_Status','Longitudinal_Followup_Years','Completeness_of_Followup']].head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztl82eUqJNS1"
      },
      "outputs": [],
      "source": [
        "# Longitudinal follow-up definition\n",
        "# Define the conditions\n",
        "condition1 = (df['Longitudinal_Followup_Status'] == 1) & (df['Longitudinal_Followup_Years'] >= 1) & (df['Longitudinal_Followup_Years'] < 3)\n",
        "condition2 = (df['Longitudinal_Followup_Status'] == 1) & (df['Longitudinal_Followup_Years'] >= 3)\n",
        "\n",
        "# Creating new Series with forced conversion and NaN for non-numeric values\n",
        "newcolname = 'Longitudinal_Folloup_Years_Mestre_Definition'\n",
        "newcol = np.where(condition1, 1, np.where(condition2, 2, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "# Checking\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2elXRnYJ667"
      },
      "outputs": [],
      "source": [
        "# Longitudinal follow-up completion\n",
        "# Define the conditions based on corrected criteria\n",
        "condition0 = (df['Longitudinal_Followup_Status'] == 0) | (df['Completeness_of_Followup'].isna()) | (df['Completeness_of_Followup'] <= 0.5)\n",
        "condition1 = (df['Longitudinal_Followup_Status'] == 1) & (df['Completeness_of_Followup'] > 0.5) & (df['Completeness_of_Followup'] <= 0.75)\n",
        "condition2 = (df['Longitudinal_Followup_Status'] == 1) & (df['Completeness_of_Followup'] > 0.75)\n",
        "\n",
        "# Create new column with the corrected conditions\n",
        "newcolname = 'Longitudinal_Folloup_Completion_Mestre_Definition'\n",
        "newcol = np.where(condition0, 0, np.where(condition1, 1, np.where(condition2, 2, 0)))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "# Checking\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXkCwlvibpHs"
      },
      "source": [
        "## Drug-naive status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neldt2Kz5k0j"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Were the included patients for clustering purposes drug-naive?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4Vk8jmS5oVZ"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Yes']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Drug_Naive_Status' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W2MsmrE4IfS"
      },
      "source": [
        "## Number of centers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DxxfODt4Uzp"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'From how many different centers came the patients to be subtyped?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5kRYN3_4KYn"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['More than one center']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Multicentric_X_Others' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh6DbVI6ko9e"
      },
      "source": [
        "## Sampling method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXoaVQf9ko9n"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Which was the sampling method?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHzDWGqpko9n"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Consecutive or random']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Consecutive_or_Random_Sampling' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZqPcaH4tvNL"
      },
      "source": [
        "## Treatment response (cluster or post hoc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYgwrU9stvNN"
      },
      "outputs": [],
      "source": [
        "# Definindo colunas de interesse\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?'  # Análise de coluna e posição\n",
        "colpos2 = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?'  # Análise de coluna e posição\n",
        "\n",
        "# Critério formal\n",
        "criteria = 'Clinical - Medication Response'\n",
        "\n",
        "# Configurações preparatórias\n",
        "newcolname = 'Treatment_Response_Evaluated'  # Nome da nova coluna\n",
        "insert_position = df.columns.get_loc(colpos) + 1  # Calculando posição\n",
        "\n",
        "# Função para verificar se o critério está presente\n",
        "def contains_criteria(value, criteria):\n",
        "    if isinstance(value, str):\n",
        "        return criteria in value.split(', ')\n",
        "    return False\n",
        "\n",
        "# Nova coluna a ser criada\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]) & pd.isna(df[colpos2]), np.nan,  # Mantém NaN como NaN\n",
        "    np.where(df[colpos].apply(lambda x: contains_criteria(x, criteria)) |\n",
        "             df[colpos2].apply(lambda x: contains_criteria(x, criteria)), 1, 0))\n",
        "\n",
        "# Inserindo a coluna\n",
        "try:\n",
        "    df.insert(insert_position, newcolname, newcol)  # Inserindo a coluna\n",
        "except ValueError:\n",
        "    df[newcolname] = newcol  # Atualizando colunas\n",
        "\n",
        "# Adicionando esta lista de variáveis a relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "    relevantvars.append(newcolname)\n",
        "else:\n",
        "    print(newcolname, 'Já adicionada')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQvhb9K6NNTf"
      },
      "source": [
        "## Performed validation\n",
        "\n",
        "**Either cross-validation or in another cohort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbNbRfxnNNTg"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Number of PD patients utilized for validation purposes' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8c1kpEyUIHb"
      },
      "source": [
        "**Validation in the same dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3alZQhDuQsD-"
      },
      "outputs": [],
      "source": [
        "# List of columns\n",
        "templist = ['Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [AMP-PD]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [BioFIND]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Fox Insight]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [GP2 Dataset]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [LRRK2 Cohort Consortium]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson Progression Marker Initiative (PPMI)]',\n",
        "       \"Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson's Disease Biomarker Program (PDBP)]\",\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [UK Biobank]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl_IL49TQXwH"
      },
      "outputs": [],
      "source": [
        "# Checking if performed validation in a different cohort\n",
        "tempcols = df[templist].apply(lambda x: x == 'Creation, Validation').any(axis=1)\n",
        "tempint = tempcols.astype(int)\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]' # Analysis column and position\n",
        "newcolname = 'Validation_in_the_Same_Datasets_Sum' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, tempint) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = tempint # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newlist[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + newcolname\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Checking how it goes\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[[newcolname] + templist].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4BRravfSmfT"
      },
      "outputs": [],
      "source": [
        "# Longitudinal follow-up definition\n",
        "# Define the conditions\n",
        "condition1 = (df['Validation_in_the_Same_Datasets_Sum'] == 1)\n",
        "condition2 = (df['Validation_in_a_Different_Cohort'] == 1)\n",
        "\n",
        "# Creating new Series with forced conversion and NaN for non-numeric values\n",
        "newcolname = 'Validation_not_Perfored_X_same_Cohort_X_Different_Cohort'\n",
        "newcolname2 = 'Performed_Validation' # Either manner of validation\n",
        "newcol = np.where(condition1, 1, np.where(condition2, 2, 0))\n",
        "newcol2 = np.where(condition1 | condition2, 1, 0)  # Use the OR operator to combine conditions\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "# Checking\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQGn-WcdPbAA"
      },
      "outputs": [],
      "source": [
        "df[['Validation_in_the_Same_Datasets_Sum','Validation_in_a_Different_Cohort','Validation_not_Perfored_X_same_Cohort_X_Different_Cohort']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iASaxu4OQETb"
      },
      "source": [
        "## Used controls for comparison\n",
        "\n",
        "Either cross-validation or in another cohort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K41XZp2QETb"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Number of healthy controls utilized for validation or technique elaboration purposes' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFPfx24cQETb"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['No data from controls was utilized for any of these purporses']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Used_Controls_For_Comparisons' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    df[colpos].isin(criteria),\n",
        "    0, 1)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HluusRTgQdxH"
      },
      "source": [
        "## Countries of patients\n",
        "\n",
        "Those 17 from 'Sub-Saharan Africa, Middle East & North Africa, Europe & Central Asia, North America' are PPMI patients\n",
        "\n",
        "https://www.ppmi-info.org/about-ppmi/ppmi-clinical-sites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6TaREITQieu"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'From which countries or world region came the patients whose data was used in the clustering?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_35At8KWRBr6"
      },
      "source": [
        "### Option 1\n",
        "\n",
        "Option 1 - not considering that PPMI recruits from Nigeria and Israel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZoVOtVsQfzP"
      },
      "outputs": [],
      "source": [
        "df['Original country without Sub-Saharan Africa and Middle East & North Africa'] = df[colpos].copy()\n",
        "df['Original country without Sub-Saharan Africa and Middle East & North Africa'] = df['Original country without Sub-Saharan Africa and Middle East & North Africa'].str.replace('Sub-Saharan Africa, Middle East & North Africa, Europe & Central Asia, North America',\n",
        "                                                                                         'Europe & Central Asia, North America', regex=False)\n",
        "\n",
        "df['Original country without Sub-Saharan Africa and Middle East & North Africa'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjQleTL4TRmD"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = ['Sub-Saharan Africa', 'Middle East & North Africa', 'East Asia & Pacific (China, Japan, Australia)',\n",
        "          'South Asia (India, Pakistan, Bangladesh)', 'Europe & Central Asia', 'Latin America & the Caribbean',\n",
        "          'North America']\n",
        "\n",
        "# Preparing to run the function\n",
        "main_column = 'Original country without Sub-Saharan Africa and Middle East & North Africa'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if siglas[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + siglas\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Modifying the names to remember that this was modified\n",
        "tempnames = []\n",
        "\n",
        "for name in siglas:\n",
        "  tempname = name + str('_Adjusted_PPMI')\n",
        "  tempnames.append(tempname)\n",
        "\n",
        "# Create a dictionary mapping old names to new names\n",
        "column_mapping = dict(zip(siglas, tempnames))\n",
        "\n",
        "# Rename columns in the DataFrame using the column_mapping dictionary\n",
        "df.rename(columns=column_mapping, inplace=True)\n",
        "\n",
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, tempnames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQo31hYLT6gs"
      },
      "source": [
        "### Option 2\n",
        "\n",
        "Option 2 - considering that PPMI recruits from Nigeria and Israel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOXGQFOrT6gt"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = ['Sub-Saharan Africa', 'Middle East & North Africa', 'East Asia & Pacific (China, Japan, Australia)',\n",
        "          'South Asia (India, Pakistan, Bangladesh)', 'Europe & Central Asia', 'Latin America & the Caribbean',\n",
        "          'North America']\n",
        "\n",
        "# Preparing to run the function\n",
        "main_column = 'From which countries or world region came the patients whose data was used in the clustering?'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if siglas[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + siglas\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, siglas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVy-K0pRuDGX"
      },
      "source": [
        "## Minimum descriptive data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A16SNzYuZHv"
      },
      "outputs": [],
      "source": [
        "siglas = ['None of the mentioned data on the PD population was informed by the study',\n",
        "           'Age', 'Gender', 'Ethnicity', 'Education', 'Income', 'Family History',\n",
        "           'Age at disease onset', 'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)',\n",
        "           'Vital Signs (blood pressure, cardiac frequency)', 'Physical Attributes (weight, height etc)',\n",
        "           'Medication Doses', 'Genetic Status (monogenic PD, polygenic risk score)',\n",
        "           'Summarised data from clinical scales (such as mean MDS-UPDRS scores, number of non-motor symptoms)',\n",
        "           'Summarised data from neuroimaging studies (such as cortical thickness)',\n",
        "           \"Biomarker's profile (results from biomarker tests)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6ot5MmIur5o"
      },
      "outputs": [],
      "source": [
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, siglas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Fd5LARuSag"
      },
      "source": [
        "### C1 - Age and Sex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPU7UXtEubEL"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Age', 'Gender']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type of descriptive data regarding the PD population was presented?' # Analysis column and position\n",
        "newcolname = 'Minimum_Info_Age_Sex' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Looping through columns\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=True)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[siglas].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDa8RR_43S8L"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Age', 'Gender', 'Age at disease onset']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type of descriptive data regarding the PD population was presented?' # Analysis column and position\n",
        "newcolname = 'Minimum_Info_Age_Sex_OnsetAge' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Looping through columns\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=True)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[siglas].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Ct0oPjvwl3"
      },
      "source": [
        "### C2 - Age, Sex and Staging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DegUlrovuty"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Age', 'Gender', 'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type of descriptive data regarding the PD population was presented?' # Analysis column and position\n",
        "newcolname = 'Minimum_Info_Age_Sex_Staging' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Looping through columns\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=True)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[siglas].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNr5_GWSwPM-"
      },
      "source": [
        "### C3 - Age, Sex, Staging and Scales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YHQTOOBzNzf"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list as a set\n",
        "templist = ['Age', 'Gender',\n",
        "            'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)',\n",
        "            'Summarised data from clinical scales (such as mean MDS-UPDRS scores, number of non-motor symptoms)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type of descriptive data regarding the PD population was presented?' # Analysis column and position\n",
        "newcolname = 'Minimum_Info_Age_Sex_Staging_Scales' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Create new column by applying the parsing function and checking for subset\n",
        "newcol = parse_description(templist, all=True)\n",
        "\n",
        "# Inserting the new column\n",
        "try:\n",
        "    df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "    df[newcolname] = newcol # Updating column if already exists\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Output the new column's value counts and a preview of the DataFrame\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df.head(5)  # Assuming siglas was a placeholder for demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eckqiVJ3v6OP"
      },
      "source": [
        "### C4 - Age, Sex, Staging, Scales and Medication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9piTzGjfv6OY"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Age', 'Gender',\n",
        "            'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)',\n",
        "            'Summarised data from clinical scales (such as mean MDS-UPDRS scores, number of non-motor symptoms)',\n",
        "            'Medication Doses']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type of descriptive data regarding the PD population was presented?' # Analysis column and position\n",
        "newcolname = 'Minimum_Info_Age_Sex_Staging_Scales_Medication' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Looping through columns\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=True)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[siglas].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h4YslcJ1Pu5"
      },
      "source": [
        "### C5 - Any Complex Data\n",
        "\n",
        "Checking if ANY of those is present: Omics, Neuroimaging and Biomarkers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe90fb4D1PvB"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Genetic Status (monogenic PD, polygenic risk score)',\n",
        "           'Summarised data from neuroimaging studies (such as cortical thickness)',\n",
        "           \"Biomarker's profile (results from biomarker tests)\"]\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type of descriptive data regarding the PD population was presented?' # Analysis column and position\n",
        "newcolname = 'Minimum_Info_Complex_Data_Any_Present' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Looping through columns\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[siglas].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MVkun6LVS0S"
      },
      "source": [
        "# Creating clustering variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vYKRGOUY-WH"
      },
      "source": [
        "## General division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhEXIBFUYE5v"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = [\n",
        "    'Clinical - Vital Signs or Neurologic Examination Findings',\n",
        "    'Clinical - Age',\n",
        "    'Clinical - Gender',\n",
        "    'Clinical - Ethnicity',\n",
        "    'Clinical - Age at disease onset',\n",
        "    'Clinical - Disease staging',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)',\n",
        "    'Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "    'Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "    'Clinical - Cognition (memory, attention, executive, visuospatial memory etc)',\n",
        "    'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Medication (types and/or dosages)',\n",
        "    'Clinical - Other (pain, fatigue etc)',\n",
        "    'Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "    'Neuroimaging - Structural or Volumetric MRI',\n",
        "    'Neuroimaging - Diffusion Imaging MRI',\n",
        "    'Neuroimaging - DaTSCAN',\n",
        "    'Neuroimaging - Radiomics',\n",
        "    'Neuroimaging - Functional MRI',\n",
        "    'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)',\n",
        "    'Neurophysiology - Electroencephalography',\n",
        "    'Neurophysiology - Electroneuromiography',\n",
        "    'Neurophysiology - Other',\n",
        "    'Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)',\n",
        "    'Clinical - Phonation',\n",
        "    'Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparing to run the divisor function\n",
        "main_column = 'Which type data was utilized for the clustering algorithm?'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Renaming to identify they are from the clustering group\n",
        "clustering_names = []\n",
        "for value in siglas:\n",
        "  clustering_names.append(str('Clustering_')+value)\n",
        "\n",
        "column_mapping = dict(zip(siglas, clustering_names))\n",
        "\n",
        "# Rename columns in the DataFrame using the column_mapping dictionary\n",
        "if 'Clustering_Clinical - Age' not in df.columns:\n",
        "  df.rename(columns=column_mapping, inplace=True)\n",
        "  relevantvars = relevantvars + clustering_names\n",
        "else:\n",
        "  print('Already added and renamed')\n",
        "\n",
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, clustering_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevantvars"
      ],
      "metadata": {
        "id": "gBbcE8iBB-Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0agiMzxakuW6"
      },
      "outputs": [],
      "source": [
        "# Checking if additional and relevant data was lost\n",
        "df['Cluster_Sum'] = df[clustering_names].sum(axis=1)\n",
        "\n",
        "# Filter rows where 'Cluster_Sum' is 0, indicating no siglas were found in these rows\n",
        "df[df['Cluster_Sum'] == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFXFa2NVaI29"
      },
      "source": [
        "## Any data subtypes\n",
        "\n",
        "Identifying big data subtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZLwO9IYlOMy"
      },
      "source": [
        "### Clinical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WQ4RqEHfaUe9"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = [\n",
        "    'Clinical - Vital Signs or Neurologic Examination Findings',\n",
        "    'Clinical - Age',\n",
        "    'Clinical - Gender',\n",
        "    'Clinical - Ethnicity',\n",
        "    'Clinical - Age at disease onset',\n",
        "    'Clinical - Disease staging',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)',\n",
        "    'Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "    'Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "    'Clinical - Cognition (memory, attention, executive, visuospatial memory etc)',\n",
        "    'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Medication (types and/or dosages)',\n",
        "    'Clinical - Other (pain, fatigue etc)',\n",
        "    'Clinical - Phonation',\n",
        "    'Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Clinical' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJoaMs6SmBuj"
      },
      "source": [
        "### Non Clinical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESWBh6_HlmOl"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "    'Neuroimaging - Structural or Volumetric MRI',\n",
        "    'Neuroimaging - Diffusion Imaging MRI',\n",
        "    'Neuroimaging - DaTSCAN',\n",
        "    'Neuroimaging - Radiomics',\n",
        "    'Neuroimaging - Functional MRI',\n",
        "    'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)',\n",
        "    'Neurophysiology - Electroencephalography',\n",
        "    'Neurophysiology - Electroneuromiography',\n",
        "    'Neurophysiology - Other',\n",
        "    'Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Non_Clinical' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFuIlJWDmFHq"
      },
      "source": [
        "### Biomarkers\n",
        "\n",
        "Check variable 'Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append('Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)')\n",
        "else:\n",
        "  print('Already added')"
      ],
      "metadata": {
        "id": "bp0puiO0CSJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE0zurUfmQMz"
      },
      "source": [
        "### Neuroimaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TFfzJn2mQMz"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Neuroimaging - Structural or Volumetric MRI',\n",
        "    'Neuroimaging - Diffusion Imaging MRI',\n",
        "    'Neuroimaging - DaTSCAN',\n",
        "    'Neuroimaging - Radiomics',\n",
        "    'Neuroimaging - Functional MRI',\n",
        "    'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Neuroimaging' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4FsIQOqmQUN"
      },
      "source": [
        "### Neurophysiology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "progDFBqmQUO"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Neurophysiology - Electroencephalography',\n",
        "    'Neurophysiology - Electroneuromiography',\n",
        "    'Neurophysiology - Other']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Neurophysiology' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnTKK6HSmang"
      },
      "source": [
        "### Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwGyFUJlmanm"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Omics' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWJAOtiRla4V"
      },
      "source": [
        "### Omics or Biomarkers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eWtGJ2Qla4W"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)',\n",
        "            'Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Omics_or_Biomarkers' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggJQad_sayU4"
      },
      "source": [
        "## Specific data subtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59hyFDuayU5"
      },
      "source": [
        "### Demographic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekN5wcADayU5"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Age',\n",
        "    'Clinical - Gender',\n",
        "    'Clinical - Ethnicity']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Demographic' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j9VjKsAmrHD"
      },
      "source": [
        "### Disease staging or age at onset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqz02fdzmrHE"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Age at disease onset','Clinical - Disease staging']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Any_Time_or_Staging_Measurements' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25S_XdaxayU5"
      },
      "source": [
        "### Clinical motor scales\n",
        "\n",
        "Motor clinical data obtained from rating scales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrRbxwl8ayU5"
      },
      "source": [
        "Including complications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St3cFh_OayU5"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)',\n",
        "    'Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Motor_Scales_Plus_Complications' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-omCtg21ayU5"
      },
      "source": [
        "Excluding complications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAZmrGzhayU6"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Motor_Scales_No_Complications' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMyDORevdouJ"
      },
      "source": [
        "Including complications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuUBoYETdqj4"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Including_Motor_Complications' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrFziQhUayU6"
      },
      "source": [
        "### Clinical cognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4_qkBnfayU6"
      },
      "source": [
        "Including cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfZUqCjBayU6"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Cognition (memory, attention, executive, visuospatial memory etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Including_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfZARcyLayU6"
      },
      "source": [
        "Only cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsPAkrGqayU6"
      },
      "outputs": [],
      "source": [
        "# Preparatory settings\n",
        "tempvar = 'Clinical - Cognition (memory, attention, executive, visuospatial memory etc)'\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Only_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Create a mask where the condition is True if the column matches tempvar exactly, False otherwise\n",
        "mask = df[colpos] == tempvar\n",
        "\n",
        "# Convert the Boolean mask to integers (True becomes 1, False becomes 0)\n",
        "newcol = mask.astype(int)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[['Article Title',colpos,newcolname]].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbTgj2XjayU6"
      },
      "source": [
        "### Clinical non motor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq_PI-OzayU6"
      },
      "source": [
        "Including cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIHH548OayU6"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Cognition (memory, attention, executive, visuospatial memory etc)',\n",
        "    'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Other (pain, fatigue etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Non_Motor_Including_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jYAgxEmayU6"
      },
      "source": [
        "Excluding cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVNZYUb9ayU6"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Other (pain, fatigue etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Non_Motor_Excluding_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_52Le11Qf80P"
      },
      "source": [
        "Excluding cognition, other and QoL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNQ3N6t4fw3o"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',]\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjeFCiuSayU6"
      },
      "source": [
        "### Clinical neuropsychiatric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3FDJTdrayU7"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Neuropsychiatric' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T6wshcAayU7"
      },
      "source": [
        "### Clinical sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8oQ0NC9ayU7"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)']\n",
        "\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Sleep' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXUSpm3ayU7"
      },
      "source": [
        "### Clinical olfaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yLW1HBwayU7"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Olfaction' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ygFGU7TayU7"
      },
      "source": [
        "### Clinical autonomic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDvMiIi1ayU7"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Any_Autonomic' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ic0qNjiuq_s"
      },
      "source": [
        "### Clinical other"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quality of life"
      ],
      "metadata": {
        "id": "dAjCtNEnuq_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gxjEn3quq_s"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Quality of Life']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_QoL' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other non-motor"
      ],
      "metadata": {
        "id": "o-SR9DQXuq_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Other (pain, fatigue etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Non_Motor_Other' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "_mh9ssTbuq_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E12ifkbuukj"
      },
      "source": [
        "### Clinical other motor\n",
        "\n",
        "All united"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31ZmnWo1uukj"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Vital Signs or Neurologic Examination Findings',\n",
        "            'Clinical - Phonation',\n",
        "            'Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Other_Motor' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vital signs of examination"
      ],
      "metadata": {
        "id": "OQoM5AgMuukk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Vital Signs or Neurologic Examination Findings']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Vital_or_Neurologic_Examination' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "gzdtWpLVuukk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phonation"
      ],
      "metadata": {
        "id": "GGj0SHvFuukk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Phonation']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Phonation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "-TAtSxXquukk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Motor Tests (example: Timed-Up & Go)"
      ],
      "metadata": {
        "id": "C4Lmywzwuukk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Clinical_Motor_Tests' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "-lPMyw-Vuukk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXh1S53tAJVE"
      },
      "source": [
        "## Counting domains\n",
        "\n",
        "For usage associated with Mestre article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86rBDIP9AapL"
      },
      "outputs": [],
      "source": [
        "clusteringlist = ['Clustering_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                  'Clustering_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                  'Clustering_Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "                  'Clustering_Clinical_Other_Motor',\n",
        "                  'Clustering_Any_Demographic',\n",
        "                  'Clustering_Any_Time_or_Staging_Measurements',\n",
        "                  'Clustering_Clinical_Including_Cognition',\n",
        "                  'Clustering_Clinical_Any_Neuropsychiatric',\n",
        "                  'Clustering_Clinical_Any_Sleep',\n",
        "                  'Clustering_Clinical_Any_Olfaction',\n",
        "                  'Clustering_Clinical_Any_Autonomic',\n",
        "                  'Clustering_Clinical - Medication (types and/or dosages)',\n",
        "                  'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                  'Clustering_Any_Neuroimaging', 'Clustering_Any_Neurophysiology', 'Clustering_Any_Omics']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Number_of_Specific_Domains' # Name of the new column\n",
        "newcolname2 = 'Clustering_More_Than_One_Domain_Mestre_Division' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df[clusteringlist].apply(lambda x: x.sum(), axis=1)\n",
        "\n",
        "# New column to be created - classification according to mestre\n",
        "criteria = [1] # Number of features\n",
        "\n",
        "# Creating mestre division\n",
        "newcol2 = np.where(\n",
        "    newcol.isin(criteria),\n",
        "    0, 1)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusteringlist = ['Clustering_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                  'Clustering_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                  'Clustering_Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "                  'Clustering_Clinical_Other_Motor',\n",
        "                  'Clustering_Any_Demographic',\n",
        "                  'Clustering_Any_Time_or_Staging_Measurements',\n",
        "                  'Clustering_Clinical_Including_Cognition',\n",
        "                  'Clustering_Clinical_Any_Neuropsychiatric',\n",
        "                  'Clustering_Clinical_Any_Sleep',\n",
        "                  'Clustering_Clinical_Any_Olfaction',\n",
        "                  'Clustering_Clinical_Any_Autonomic',\n",
        "                  'Clustering_Clinical - Medication (types and/or dosages)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was utilized for the clustering algorithm?' # Analysis column and position\n",
        "newcolname = 'Clustering_Number_of_Specific_Clinical_Domains' # Name of the new column\n",
        "newcolname2 = 'Clustering_More_Than_One_Clinical_Domain_Mestre_Division' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df[clusteringlist].apply(lambda x: x.sum(), axis=1)\n",
        "\n",
        "# New column to be created - classification according to mestre\n",
        "criteria = [1] # Number of features\n",
        "\n",
        "# Creating mestre division\n",
        "newcol2 = np.where(\n",
        "    newcol.isin(criteria),\n",
        "    0, 1)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "r-34qIHYXKKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTWlk50o6oSI"
      },
      "source": [
        "# Creating validation variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsp8GwcC6oSO"
      },
      "source": [
        "## General division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM6tIG_x6oSO"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = ['The study did not provide association / interpretation analyses',\n",
        "    'Clinical - Vital Signs or Neurologic Examination Findings',\n",
        "    'Clinical - Age',\n",
        "    'Clinical - Gender',\n",
        "    'Clinical - Ethnicity',\n",
        "    'Clinical - Age at disease onset',\n",
        "    'Clinical - Disease staging',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)',\n",
        "    'Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "    'Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "    'Clinical - Cognition (memory, attention, executive, visuospatial memory etc)',\n",
        "    'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Medication (types and/or dosages)',\n",
        "    'Clinical - Other (pain, fatigue etc)',\n",
        "    'Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "    'Neuroimaging - Structural or Volumetric MRI',\n",
        "    'Neuroimaging - Diffusion Imaging MRI',\n",
        "    'Neuroimaging - DaTSCAN',\n",
        "    'Neuroimaging - Radiomics',\n",
        "    'Neuroimaging - Functional MRI',\n",
        "    'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)',\n",
        "    'Neurophysiology - Electroencephalography',\n",
        "    'Neurophysiology - Electroneuromiography',\n",
        "    'Neurophysiology - Other',\n",
        "    'Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)',\n",
        "    'Clinical - Phonation',\n",
        "    'Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparing to run the divisor function\n",
        "main_column = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Renaming to identify they are from the clustering group\n",
        "validation_names = []\n",
        "for value in siglas:\n",
        "  validation_names.append(str('Validation_')+value)\n",
        "\n",
        "column_mapping = dict(zip(siglas, validation_names))\n",
        "\n",
        "# Rename columns in the DataFrame using the column_mapping dictionary\n",
        "if 'Validation_Clinical - Age' not in df.columns:\n",
        "  df.rename(columns=column_mapping, inplace=True)\n",
        "  relevantvars = relevantvars + validation_names\n",
        "else:\n",
        "  print('Already added and renamed')\n",
        "\n",
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, validation_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_OjMsrS6oSO"
      },
      "outputs": [],
      "source": [
        "# Checking if additional and relevant data was lost\n",
        "df['Validation_Sum'] = df[validation_names].sum(axis=1)\n",
        "\n",
        "# Filter rows where 'Cluster_Sum' is 0, indicating no siglas were found in these rows\n",
        "df[df['Validation_Sum'] == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl9hWTDH6oSO"
      },
      "source": [
        "## Any data subtypes\n",
        "\n",
        "Identifying big data subtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwSL4pv16oSO"
      },
      "source": [
        "### Clinical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ok2mmOcW6oSO"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = [\n",
        "    'Clinical - Vital Signs or Neurologic Examination Findings',\n",
        "    'Clinical - Age',\n",
        "    'Clinical - Gender',\n",
        "    'Clinical - Ethnicity',\n",
        "    'Clinical - Age at disease onset',\n",
        "    'Clinical - Disease staging',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)',\n",
        "    'Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "    'Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "    'Clinical - Cognition (memory, attention, executive, visuospatial memory etc)',\n",
        "    'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Medication (types and/or dosages)',\n",
        "    'Clinical - Other (pain, fatigue etc)',\n",
        "    'Clinical - Phonation',\n",
        "    'Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Clinical' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kysTEDCN6oSP"
      },
      "source": [
        "### Non Clinical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6FH88QI6oSP"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "    'Neuroimaging - Structural or Volumetric MRI',\n",
        "    'Neuroimaging - Diffusion Imaging MRI',\n",
        "    'Neuroimaging - DaTSCAN',\n",
        "    'Neuroimaging - Radiomics',\n",
        "    'Neuroimaging - Functional MRI',\n",
        "    'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)',\n",
        "    'Neurophysiology - Electroencephalography',\n",
        "    'Neurophysiology - Electroneuromiography',\n",
        "    'Neurophysiology - Other',\n",
        "    'Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Non_Clinical' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY7MGchW6oSP"
      },
      "source": [
        "### Biomarkers\n",
        "\n",
        "Check variable 'Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7fMNvye6oSP"
      },
      "source": [
        "### Neuroimaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvdJihUF6oSP"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Neuroimaging - Structural or Volumetric MRI',\n",
        "    'Neuroimaging - Diffusion Imaging MRI',\n",
        "    'Neuroimaging - DaTSCAN',\n",
        "    'Neuroimaging - Radiomics',\n",
        "    'Neuroimaging - Functional MRI',\n",
        "    'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Neuroimaging' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CedyvcZ6oSP"
      },
      "source": [
        "### Neurophysiology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgy_K69Z6oSP"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Neurophysiology - Electroencephalography',\n",
        "    'Neurophysiology - Electroneuromiography',\n",
        "    'Neurophysiology - Other']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Neurophysiology' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0im_DLg6oSP"
      },
      "source": [
        "### Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGZJC-PT6oSP"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Omics' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Wee_02lLw5"
      },
      "source": [
        "### Omics or Biomarkers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk_F10vvlOUj"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)',\n",
        "            'Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Omics_or_Biomarkers' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dBYnghD6oSP"
      },
      "source": [
        "## Specific data subtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6jO0LHr6oSP"
      },
      "source": [
        "### Demographic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zctBwxUZ6oSP"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Age',\n",
        "    'Clinical - Gender',\n",
        "    'Clinical - Ethnicity']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Demographic' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQvhbxS2m_JS"
      },
      "source": [
        "### Disease staging or age at onset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QitD2nim_JY"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Age at disease onset','Clinical - Disease staging']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Any_Time_or_Staging_Measurements' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv1weeAA6oSQ"
      },
      "source": [
        "### Clinical motor scales\n",
        "\n",
        "Motor clinical data obtained from rating scales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzdrwc186oSQ"
      },
      "source": [
        "Including complications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX9SSC6U6oSQ"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)',\n",
        "    'Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Motor_Scales_Plus_Complications' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGkzjZ1g6oSQ"
      },
      "source": [
        "Excluding complications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNFjUltK6oSQ"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Features Obtained Through Rating Scales (tremor)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (bradykinesia)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (rigidity)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (gait and/or posture)',\n",
        "    'Clinical - Motor Features Obtained Through Rating Scales (speech)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Motor_Scales_No_Complications' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avlLP8qydU0f"
      },
      "source": [
        "Including only complications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JiSIZkTdaqX"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Including_Motor_Complications' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaISilgk6oSQ"
      },
      "source": [
        "### Clinical cognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBnltSgu6oSQ"
      },
      "source": [
        "Including cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53lPzpc66oSQ"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Cognition (memory, attention, executive, visuospatial memory etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Including_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T-yEo816oSQ"
      },
      "source": [
        "Only cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV_ZwAeY6oSQ"
      },
      "outputs": [],
      "source": [
        "# Preparatory settings\n",
        "tempvar = 'Clinical - Cognition (memory, attention, executive, visuospatial memory etc)'\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Only_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Create a mask where the condition is True if the column matches tempvar exactly, False otherwise\n",
        "mask = df[colpos] == tempvar\n",
        "\n",
        "# Convert the Boolean mask to integers (True becomes 1, False becomes 0)\n",
        "newcol = mask.astype(int)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "df[['Article Title',colpos,newcolname]].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edZ8p2rO6oSQ"
      },
      "source": [
        "### Clinical non motor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180tRX3F6oSQ"
      },
      "source": [
        "Including cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MurC2ZfC6oSQ"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Cognition (memory, attention, executive, visuospatial memory etc)',\n",
        "    'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Other (pain, fatigue etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Non_Motor_Including_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeiyhhKX6oSQ"
      },
      "source": [
        "Excluding cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU-I8ASe6oSQ"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Other (pain, fatigue etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Non_Motor_Excluding_Cognition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsUCXcWngBIY"
      },
      "source": [
        "Excluding cognition, other and QoL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ERWrI2Nfr2c"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',]\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqGjH1uJXb3J"
      },
      "source": [
        "### Clinical neuropsychiatric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSHhqc5DXb3R"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Neuropsychiatric' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUPQRQaNZtt0"
      },
      "source": [
        "### Clinical sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSZnmcqUZtt6"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)']\n",
        "\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Sleep' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slhTPaTxZt3D"
      },
      "source": [
        "### Clinical olfaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmeVJaWmZt3D"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Olfaction' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_b0jIhVZuEv"
      },
      "source": [
        "### Clinical autonomic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd-kkny2ZuEv"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Any_Autonomic' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0cIxW_FaPx6"
      },
      "source": [
        "### Clinical other"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quality of life"
      ],
      "metadata": {
        "id": "EFPi3V67ucNI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLnQZbYiaPyA"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Quality of Life']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_QoL' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other non-motor"
      ],
      "metadata": {
        "id": "x7RmDI_wuLNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Other (pain, fatigue etc)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Non_Motor_Other' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "8zaqcGHpuMOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8vNouof6oSQ"
      },
      "source": [
        "### Clinical other motor\n",
        "\n",
        "All united"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgKl6Au26oSQ"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Vital Signs or Neurologic Examination Findings',\n",
        "            'Clinical - Phonation',\n",
        "            'Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Other_Motor' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vital signs of examination"
      ],
      "metadata": {
        "id": "srr7A2evuQZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Vital Signs or Neurologic Examination Findings']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Vital_or_Neurologic_Examination' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "Z3AAckYBuTaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phonation"
      ],
      "metadata": {
        "id": "v3rniI4VuUas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Phonation']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Phonation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "2HVTw7ruuViY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Motor Tests (example: Timed-Up & Go)"
      ],
      "metadata": {
        "id": "sVjqivweuWG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "templist = ['Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Clinical_Motor_Tests' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = parse_description(templist, all=False)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "nS2kqYdwuYhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ivEfZzGCLSX"
      },
      "source": [
        "## Counting domains\n",
        "\n",
        "Based on Mestre definition of post hoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECqEJgFuCNeb"
      },
      "outputs": [],
      "source": [
        "validationlist = ['Validation_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                  'Validation_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                  'Validation_Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "                  'Validation_Clinical_Other_Motor',\n",
        "                  'Validation_Any_Demographic',\n",
        "                  'Validation_Any_Time_or_Staging_Measurements',\n",
        "                  'Validation_Clinical_Including_Cognition',\n",
        "                  'Validation_Clinical_Any_Neuropsychiatric',\n",
        "                  'Validation_Clinical_Any_Sleep',\n",
        "                  'Validation_Clinical_Any_Olfaction',\n",
        "                  'Validation_Clinical_Any_Autonomic',\n",
        "                  'Validation_Clinical - Medication (types and/or dosages)',\n",
        "                  'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                  'Validation_Any_Neuroimaging', 'Validation_Any_Neurophysiology', 'Validation_Any_Omics']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Number_of_Specific_Domains' # Name of the new column\n",
        "newcolname2 = 'Validation_More_Than_One_Domain_Mestre_Division' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df[validationlist].apply(lambda x: x.sum(), axis=1)\n",
        "\n",
        "# Define the criteria clearly\n",
        "criteria_zero = [0]  # No domains\n",
        "criteria_one = [1]  # Exactly one domain\n",
        "\n",
        "# Calculate the number of validation areas each record pertains to\n",
        "newcol = df[validationlist].apply(lambda x: x.sum(), axis=1)\n",
        "\n",
        "# Assign categories based on the count of domains\n",
        "newcol2 = np.where(\n",
        "    newcol.isin(criteria_zero),\n",
        "    0,  # No domains\n",
        "    np.where(\n",
        "        newcol.isin(criteria_one),\n",
        "        1,  # Exactly one domain\n",
        "        2  # More than one domain\n",
        "    )\n",
        ")\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validationlist = ['Validation_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                  'Validation_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                  'Validation_Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "                  'Validation_Clinical_Other_Motor',\n",
        "                  'Validation_Any_Demographic',\n",
        "                  'Validation_Any_Time_or_Staging_Measurements',\n",
        "                  'Validation_Clinical_Including_Cognition',\n",
        "                  'Validation_Clinical_Any_Neuropsychiatric',\n",
        "                  'Validation_Clinical_Any_Sleep',\n",
        "                  'Validation_Clinical_Any_Olfaction',\n",
        "                  'Validation_Clinical_Any_Autonomic',\n",
        "                  'Validation_Clinical - Medication (types and/or dosages)']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?' # Analysis column and position\n",
        "newcolname = 'Validation_Number_of_Specific_Clinical_Domains' # Name of the new column\n",
        "newcolname2 = 'Validation_More_Than_One_Clinical_Domain_Mestre_Division' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df[validationlist].apply(lambda x: x.sum(), axis=1)\n",
        "\n",
        "# Define the criteria clearly\n",
        "criteria_zero = [0]  # No domains\n",
        "criteria_one = [1]  # Exactly one domain\n",
        "\n",
        "# Calculate the number of validation areas each record pertains to\n",
        "newcol = df[validationlist].apply(lambda x: x.sum(), axis=1)\n",
        "\n",
        "# Assign categories based on the count of domains\n",
        "newcol2 = np.where(\n",
        "    newcol.isin(criteria_zero),\n",
        "    0,  # No domains\n",
        "    np.where(\n",
        "        newcol.isin(criteria_one),\n",
        "        1,  # Exactly one domain\n",
        "        2  # More than one domain\n",
        "    )\n",
        ")\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "fB0zuCI3XTT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC2QtTNvBfko"
      },
      "source": [
        "# Additional Variable Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3xYCYHMglwe"
      },
      "outputs": [],
      "source": [
        "# Reviewing the dataframe\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyqNR44PgyBr"
      },
      "outputs": [],
      "source": [
        "df.columns[75:95]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdoftF1gdkJ"
      },
      "source": [
        "## Clustering and Validation Criteria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n5SgiGgCvz0"
      },
      "source": [
        "### General Group\n",
        "\n",
        "Clinical x Biomarker x Neuroimaging x Neurophisiology x Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xzjUcLMCY9P"
      },
      "outputs": [],
      "source": [
        "clusteringlist = ['Clustering_Any_Clinical', 'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                  'Clustering_Any_Neuroimaging', 'Clustering_Any_Neurophysiology', 'Clustering_Any_Omics']\n",
        "\n",
        "validationlist = ['Validation_Any_Clinical', 'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                  'Validation_Any_Neuroimaging', 'Validation_Any_Neurophysiology', 'Validation_Any_Omics']\n",
        "\n",
        "# Calculate differences\n",
        "newcol = sum((df[validate] == 1) & (df[cluster] == 0) for cluster, validate in zip(clusteringlist, validationlist))\n",
        "newcol2 = newcol.apply(lambda x: 1 if x >= 1 else 0) # Binarize if used another domain or not\n",
        "\n",
        "# Creating colnames\n",
        "newcolname = 'General_Cluster_Validation_Differences_Sum'\n",
        "newcolname2 = 'General_Cluster_Validation_Differences_Yes_or_No'\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?'\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "# Displaying the count of differences and a preview of the DataFrame\n",
        "print(df['General_Cluster_Validation_Differences_Sum'].value_counts(dropna=False))\n",
        "print(df['General_Cluster_Validation_Differences_Yes_or_No'].value_counts(dropna=False))\n",
        "df[['Article Title'] + ['General_Cluster_Validation_Differences_Sum'] + ['General_Cluster_Validation_Differences_Yes_or_No'] + clusteringlist + validationlist].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqR_GrR_HcuA"
      },
      "source": [
        "### Specific Group\n",
        "\n",
        "Clinical Subdivided x Biomarker x Neuroimaging x Neurophisiology x Omics\n",
        "\n",
        "Probably this one will be utilized\n",
        "\n",
        "**Includes wearable sensors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PKARblMHcuG"
      },
      "outputs": [],
      "source": [
        "clusteringlist = ['Clustering_Any_Demographic',\n",
        "                  'Clustering_Any_Time_or_Staging_Measurements','Clustering_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                  'Clustering_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                  'Clustering_Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "                  'Clustering_Clinical_Other_Motor',\n",
        "                  'Clustering_Clinical_Including_Cognition',\n",
        "                  'Clustering_Clinical_Any_Neuropsychiatric',\n",
        "                  'Clustering_Clinical_Any_Sleep',\n",
        "                  'Clustering_Clinical_Any_Olfaction',\n",
        "                  'Clustering_Clinical_Any_Autonomic',\n",
        "                  'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                  'Clustering_Any_Neuroimaging', 'Clustering_Any_Neurophysiology', 'Clustering_Any_Omics']\n",
        "\n",
        "validationlist = ['Validation_Any_Demographic',\n",
        "                  'Validation_Any_Time_or_Staging_Measurements',\n",
        "                  'Validation_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                  'Validation_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                  'Validation_Clinical - Motor Features Obtained Through Wearable Sensors or other Technical Instruments',\n",
        "                  'Validation_Clinical_Other_Motor',\n",
        "                  'Validation_Clinical_Including_Cognition',\n",
        "                  'Validation_Clinical_Any_Neuropsychiatric',\n",
        "                  'Validation_Clinical_Any_Sleep',\n",
        "                  'Validation_Clinical_Any_Olfaction',\n",
        "                  'Validation_Clinical_Any_Autonomic',\n",
        "                  'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                  'Validation_Any_Neuroimaging', 'Validation_Any_Neurophysiology', 'Validation_Any_Omics']\n",
        "\n",
        "# Non-motor - other was removed because I didn't think it was relevant enough\n",
        "\n",
        "# Calculate differences\n",
        "newcol = sum((df[validate] == 1) & (df[cluster] == 0) for cluster, validate in zip(clusteringlist, validationlist))\n",
        "newcol2 = newcol.apply(lambda x: 1 if x >= 1 else 0) # Binarize if used another domain or not\n",
        "newcol3 = newcol.apply(lambda x: 1 if x >= 2 else 0) # Binarize if used 2 or more other domains or not\n",
        "\n",
        "# Creating colnames\n",
        "newcolname = 'Specific_Cluster_Validation_Differences_Sum'\n",
        "newcolname2 = 'Specific_Cluster_Validation_Differences_Yes_or_No'\n",
        "newcolname3 = 'Specific_Cluster_Validation_Differences_2_or_More_Domains_Yes_or_No'\n",
        "colpos = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?'\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "  df.insert(insert_position, newcolname3, newcol3) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns,\n",
        "  df[newcolname3] = newcol3 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "# Displaying the count of differences and a preview of the DataFrame\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))\n",
        "print(df[newcolname3].value_counts(dropna=False))\n",
        "df[['Article Title'] + ['Specific_Cluster_Validation_Differences_Sum'] + ['Specific_Cluster_Validation_Differences_Yes_or_No'] + clusteringlist + validationlist].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8L3NIFxpGxE"
      },
      "source": [
        "### Pairwise category creation\n",
        "\n",
        "Identifies what categories of general group are present either in clustering or validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjtEmjK-cUIc"
      },
      "source": [
        "#### Specific"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4bay-N-sW-f"
      },
      "source": [
        "##### Clinical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKDv2Xg6cgOL"
      },
      "source": [
        "Demographic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biy5f7RdcVcD"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Demographic': lambda x: x == 1,\n",
        "    'Validation_Any_Demographic': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Quality check\n",
        "print(calculate_percentage_distribution(df, list(criteria.keys()), use_counts=True))\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Demographic_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time based features"
      ],
      "metadata": {
        "id": "_tQWp2MSnzqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Time_or_Staging_Measurements': lambda x: x == 1,\n",
        "    'Validation_Any_Time_or_Staging_Measurements': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Quality check\n",
        "print(calculate_percentage_distribution(df, list(criteria.keys()), use_counts=True))\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Time_or_Staging_Measurements_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "AkcPdLTGn1Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk5xNfLvciBq"
      },
      "source": [
        "Motor scales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkFE2ajkc368"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Clinical_Any_Motor_Scales_No_Complications': lambda x: x == 1,\n",
        "    'Validation_Clinical_Any_Motor_Scales_No_Complications': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Quality check\n",
        "print(calculate_percentage_distribution(df, list(criteria.keys()), use_counts=True))\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Motor_Scales_no_Complications_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaSI9WPCcixM"
      },
      "source": [
        "Motor complications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EanEBqmyc4fL"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Clinical_Including_Motor_Complications': lambda x: x == 1,\n",
        "    'Validation_Clinical_Including_Motor_Complications': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Quality check\n",
        "print(calculate_percentage_distribution(df, list(criteria.keys()), use_counts=True))\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Motor_Complications_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9LG3MlXcshr"
      },
      "source": [
        "Cognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6wBcwoWc5C6"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Clinical_Including_Cognition': lambda x: x == 1,\n",
        "    'Validation_Clinical_Including_Cognition': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Quality check\n",
        "print(calculate_percentage_distribution(df, list(criteria.keys()), use_counts=True))\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Cognition_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2wT-_97cziH"
      },
      "source": [
        "Non-motor (non cognition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-5qeCUMc5r1"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other': lambda x: x == 1,\n",
        "    'Validation_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Quality check\n",
        "print(calculate_percentage_distribution(df, list(criteria.keys()), use_counts=True))\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Non_Motor_Excluding_Cognition_QoL_Other_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40FUKnpgzOL4"
      },
      "source": [
        "##### Clinical special"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wCuLbcFzRwr"
      },
      "outputs": [],
      "source": [
        "# Names of the columns to check against the elements in 'templist'\n",
        "col1 = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?'\n",
        "col2 = 'Which type data was utilized for the clustering algorithm?'\n",
        "\n",
        "# List representing values that can be present in two different types of columns\n",
        "templist = ['Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)',\n",
        "    'Clinical - Quality of Life',\n",
        "    'Clinical - Other (pain, fatigue etc)',\n",
        " 'Clinical - Vital Signs or Neurologic Examination Findings',\n",
        " 'Clinical - Phonation',\n",
        " 'Clinical - Motor Tests (example: Timed-Up & Go)']\n",
        "\n",
        "# Function to create columns based on the presence of each item in 'templist'\n",
        "for item in templist:\n",
        "  tempname = item + '_Either_in_Clustering_or_Validation'\n",
        "  # Create a new column for each item checking presence in either of the two columns\n",
        "  df[tempname] = df.apply(lambda row: int(item in row[col1] or item in row[col2]), axis=1)\n",
        "\n",
        "  # Adding this list of variables to the relevantvars\n",
        "  if tempname not in relevantvars:\n",
        "    relevantvars.append(tempname)\n",
        "  else:\n",
        "    print(tempname, 'Already added')\n",
        "\n",
        "# Displaying the counts for one of the new columns to verify (replace the index for other items)\n",
        "print(df['Clinical - Phonation_Either_in_Clustering_or_Validation'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2mBk1IIsat9"
      },
      "source": [
        "##### Neuroimaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc7kV77zscLt"
      },
      "outputs": [],
      "source": [
        "# Names of the columns to check against the elements in 'templist'\n",
        "col1 = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?'\n",
        "col2 = 'Which type data was utilized for the clustering algorithm?'\n",
        "\n",
        "# List representing values that can be present in two different types of columns\n",
        "templist = [\n",
        "    'Neuroimaging - Structural or Volumetric MRI',\n",
        "    'Neuroimaging - Diffusion Imaging MRI',\n",
        "    'Neuroimaging - DaTSCAN',\n",
        "    'Neuroimaging - Radiomics',\n",
        "    'Neuroimaging - Functional MRI',\n",
        "    'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)'\n",
        "]\n",
        "\n",
        "# Function to create columns based on the presence of each item in 'templist'\n",
        "for item in templist:\n",
        "  tempname = item + '_Either_in_Clustering_or_Validation'\n",
        "  # Create a new column for each item checking presence in either of the two columns\n",
        "  df[tempname] = df.apply(lambda row: int(item in row[col1] or item in row[col2]), axis=1)\n",
        "\n",
        "  # Adding this list of variables to the relevantvars\n",
        "  if tempname not in relevantvars:\n",
        "    relevantvars.append(tempname)\n",
        "  else:\n",
        "    print(tempname, 'Already added')\n",
        "\n",
        "# Displaying the counts for one of the new columns to verify (replace the index for other items)\n",
        "print(df['Neuroimaging - Structural or Volumetric MRI_Either_in_Clustering_or_Validation'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeCEIssGtG_J"
      },
      "source": [
        "##### Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vClvRW4ntG_R"
      },
      "outputs": [],
      "source": [
        "# Names of the columns to check against the elements in 'templist'\n",
        "col1 = 'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?'\n",
        "col2 = 'Which type data was utilized for the clustering algorithm?'\n",
        "\n",
        "# List representing values that can be present in two different types of columns\n",
        "templist = ['Omics - Proteomics',\n",
        "    'Omics - Transcriptomics',\n",
        "    'Omics - Genomics',\n",
        "    'Omics - Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)']\n",
        "\n",
        "# Function to create columns based on the presence of each item in 'templist'\n",
        "for item in templist:\n",
        "  tempname = item + '_Either_in_Clustering_or_Validation'\n",
        "  # Create a new column for each item checking presence in either of the two columns\n",
        "  df[tempname] = df.apply(lambda row: int(item in row[col1] or item in row[col2]), axis=1)\n",
        "  # Adding this list of variables to the relevantvars\n",
        "  if tempname not in relevantvars:\n",
        "    relevantvars.append(tempname)\n",
        "  else:\n",
        "    print(tempname, 'Already added')\n",
        "\n",
        "# Displaying the counts for one of the new columns to verify (replace the index for other items)\n",
        "print(df['Omics - Genomics_Either_in_Clustering_or_Validation'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vXl5RrlcPaw"
      },
      "source": [
        "#### General"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRnQklU7pXh0"
      },
      "source": [
        "Clinical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyKowyl5pjv5"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Clinical': lambda x: x == 1,\n",
        "    'Validation_Any_Clinical': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Clinical_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkSHO4clptC4"
      },
      "source": [
        "Non Clinical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5nMvHq_pt_4"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Non_Clinical': lambda x: x == 1,\n",
        "    'Validation_Any_Non_Clinical': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Any_Non_Clinical_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSR5NBb6puhM"
      },
      "source": [
        "Biomarkers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYlr33w1p-Wu"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)': lambda x: x == 1,\n",
        "    'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Any_Biomarkers_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpyIRYT_pvON"
      },
      "source": [
        "Neurophisiology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMHBSj5UqEt0"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Neurophysiology': lambda x: x == 1,\n",
        "    'Validation_Any_Neurophysiology': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Any_Neurophisiology_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrnErZBlpwug"
      },
      "source": [
        "Neuroimaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX-2BBsrqQF1"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Neuroimaging': lambda x: x == 1,\n",
        "    'Validation_Any_Neuroimaging': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Any_Neuroimaging_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVzuS8-Spze9"
      },
      "source": [
        "Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nR2W-q0qfqU"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Omics': lambda x: x == 1,\n",
        "    'Validation_Any_Omics': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Any_Omics_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW3ahmzLdjDC"
      },
      "source": [
        "## Rating scales used\n",
        "\n",
        "I will list here only the most significant and common ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOJkBhh1d5ts"
      },
      "outputs": [],
      "source": [
        "# Checking\n",
        "df['If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?'].value_counts()[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKodgVE5fHSC"
      },
      "outputs": [],
      "source": [
        "# Correcting some data - unifying Hoehn & Yahr\n",
        "df['If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?'] = df['If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?'].str.replace('The Hoehn & Yahr Scale (or the modified Hoehn & Yahr)',\n",
        "                                                                                         'Hoehn and Yahr Scale or Modified Hoehn and Yahr Scale', regex=False)\n",
        "\n",
        "# STAI\n",
        "df['If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?'] = df['If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?'].str.replace('State-Trait Anxiety Inventory for Adults',\n",
        "                                                                                         'State-Trait Anxiety Inventory (STAI)', regex=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scrPKuzkd5ts"
      },
      "outputs": [],
      "source": [
        "# Defining the different options for answers\n",
        "siglas = ['UPDRS motor scores','UPDRS non-motor scores',\n",
        "          'MDS-UPDRS Part I', 'MDS-UPDRS Part II', 'MDS-UPDRS Part III', 'MDS-UPDRS Part IV',\n",
        "          'Hoehn and Yahr Scale or Modified Hoehn and Yahr Scale', 'University of Pennsylvania Smell Identification Test (UPSIT)',\n",
        "          'State-Trait Anxiety Inventory for Adults (STAI)', 'Non-Motor Symptoms Scale (NNMS)', 'Non-Motor Symptoms Questionnaire (NMSQ)',\n",
        "          'Montreal Cognitive Assessment (MoCA)', 'Mini-Mental State Examination (MMSE)', 'Semantic Fluency Test (e.g. fruits, objects, names etc)',\n",
        "          'SCOPA-AUT', 'The Schwab and England ADL (Activities of Daily Living) scale', 'Geriatric Depression Scale (GDS)', 'Beck Depression Inventory (BDI)',\n",
        "          'Hospital Anxiety and Depression Scale (HADS)', 'Questionnaire for Impulsive-Compulsive Disorders in Parkinson’s Disease (QUIP)',\n",
        "          'REM-Sleep Behavior Disorder Screening Questionnaire (RBDSQ)', 'Epworth Sleepiness Scale (ESS)']\n",
        "\n",
        "ratingscales = siglas\n",
        "\n",
        "# Preparing to run the function\n",
        "main_column = 'If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?'\n",
        "significado = siglas\n",
        "\n",
        "# Dividing in new columns with 0 or 1\n",
        "df = divisor(df, colunaprincipal = main_column, siglas = siglas,\n",
        "                            significado = significado, apagar=False, numerical=True)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if siglas[0] not in relevantvars:\n",
        "  relevantvars = relevantvars + siglas\n",
        "else:\n",
        "  print('Already added')\n",
        "\n",
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, siglas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1u3lVEmhOHe"
      },
      "source": [
        "## Neuroimaging details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do-cZtbdkU-m"
      },
      "outputs": [],
      "source": [
        "df.columns[75:95]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX4IzYpLhP_N"
      },
      "source": [
        "### Quality control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTukrrl3hUZR"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'If neuroimaging data was utilized for clustering, did the article perform any sort of quality control measure?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-JN-o_whkTT"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Neuroimaging data was utilized for clustering purposes and a quality control was performed manually',\n",
        "            'Neuroimaging data was utilized for clustering purposes but do they do not require quality control (example: DATScan)']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Neuroimaging_QC_Performed_or_Not_Needed' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 0, 1)  # Apply 0 or 1 based on criteria\n",
        ")\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7D6DyD1kdqv"
      },
      "source": [
        "### Individual features\n",
        "\n",
        "Just adding to relevantvars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArBoETG_kh9j"
      },
      "outputs": [],
      "source": [
        "siglas = [ 'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI cortical thickness]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI subcortical volume]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI white matter lesions / hyperintensities]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [Functional MRI]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI Others]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [PET]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [SPECT DAT binding ratios]']\n",
        "\n",
        "neuroimagingvars = siglas\n",
        "\n",
        "for value in siglas:\n",
        "  if value not in relevantvars:\n",
        "    relevantvars.append(value)\n",
        "  else:\n",
        "    print('Already added')\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TaGmnJ1zkIU"
      },
      "outputs": [],
      "source": [
        "calculate_percentage_distribution(df, columns = siglas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8qmmXXelTPy"
      },
      "source": [
        "## Number of subtypes\n",
        "\n",
        "Subtyping strategies that aren't conventional are labelled NaN por .describe reasons, but needs to be reported as they are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNEE8LwslV0F"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'How many subtypes did the evaluated study identified?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01r1T2kDmpgc"
      },
      "outputs": [],
      "source": [
        "df[colpos].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqh_DOximLSn"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['two-cluster (2 subtypes);  three-cluster (3 subtypes)',\n",
        "            '8 multi-partitition clusters, each only, respectively, with: 2; 2; 3; 2; 2; 2; 2; 2',\n",
        "            'Domains Clustering - 4 clusters; Symptoms Clustering - 6 clusters ']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'ClustersNeuroimaging_QC_Performed_or_Not_Needed' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 0, 1)  # Apply 0 or 1 based on criteria\n",
        ")\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6mi8Iesf0oN"
      },
      "source": [
        "## Subtype stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK3BfHm1f194"
      },
      "outputs": [],
      "source": [
        "# Identifying those studies that claim that the found subtypes are stable\n",
        "print(df['Subtype stability'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYkWPpk6k6Lh"
      },
      "outputs": [],
      "source": [
        "# Checking studies - all need to be longitudinal to prove they are stable\n",
        "df[df['Subtype stability'] == 'Stable'][['Article Title','Longitudinal_Followup_Status','Subtype stability']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcUSBiJxpHh0"
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Stable']\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Subtype stability'\n",
        "newcolname = 'Subtype_Stability_Mestre_Definition' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeYpvu97DINS"
      },
      "source": [
        "## **Statistical methods - REMOVED**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1lCE-Jms_Ws"
      },
      "source": [
        "## Algorithm for classifying individual patients"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Did the article provide an individual level prediction algorithm?' # Analysis column and position\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "Zt7NYP1iS1uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal criteria list\n",
        "criteria = ['Yes']\n",
        "\n",
        "# Preparatory settings\n",
        "newcolname = 'Individual_Classification_Algorithm' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(df[colpos].isin(criteria), 1, 0))\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "HSEAm0Y4S_Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Year Divisions"
      ],
      "metadata": {
        "id": "gWnoavnP1OeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantile division\n",
        "\n",
        "Divided in 4 quartiles"
      ],
      "metadata": {
        "id": "BgzHTRV21tz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing by quartiles - needed for all\n",
        "df, names = quantis_single_column(df, ['Year of publication'], divisoes = 3, show_ranges=True)\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if 'Year of publication Quantile Division' not in relevantvars:\n",
        "  relevantvars.append('Year of publication Quantile Division')\n",
        "else:\n",
        "  print('Year of publication Quantile Division', 'Already added')\n",
        "\n",
        "print(df[names].value_counts())\n",
        "df[names].head()"
      ],
      "metadata": {
        "id": "kz0bKP0G1wbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cut-off: 2021 (including)"
      ],
      "metadata": {
        "id": "InyMwqmb1bSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play analysis 2021 and after x before\n",
        "criteria = [2020]\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Year of publication'\n",
        "newcolname = 'Before_2020_After' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = np.where(\n",
        "    pd.isna(df[colpos]), np.nan,  # Keep NaN as NaN\n",
        "    np.where(\n",
        "        df[colpos] <= criteria[0], 0, 1)).astype(str)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "afsSMZam1RWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sdMCbgjsDfM"
      },
      "source": [
        "# Special variable creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE3p35FjsFft"
      },
      "source": [
        "## Systematic review of subtyping article from 2021\n",
        "\n",
        "Quality score from Mestre article (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8150501/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExnKcFe0jS8t",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Formal criteria list\n",
        "qualitylist = ['Similar_PD_Staging', # 1 = homogeneous staging or duration\n",
        "               'Multicentric_X_Others', # 1 = multicentric\n",
        "               'Recruitment_Community_or_Population', # 1 = community of population based\n",
        "               'Formal_Criteria_From_Mestre', # 1 = formal criteria or neurologist | 2 = postmortem\n",
        "               'Consecutive_or_Random_Sampling', # 1 = consecutive or random\n",
        "               'Clustering_More_Than_One_Domain_Mestre_Division', # 1 = more than 1 domain\n",
        "               'Validation_More_Than_One_Domain_Mestre_Division', # 1 = one domain | 2 = more than 1 domain\n",
        "               'Longitudinal_Folloup_Years_Mestre_Definition', # 1 = Follow up for 1 to 3 years | 2 = Followup for more than 3 years\n",
        "               'Longitudinal_Folloup_Completion_Mestre_Definition', # 1 = 50 - 75% followup rate | 2 = > 75% followup rate\n",
        "               'Subtype_Stability_Mestre_Definition', # 1 = stable\n",
        "               'Individual_Classification_Algorithm', # = provides an individual classification algorithm\n",
        "               'Validation_not_Perfored_X_same_Cohort_X_Different_Cohort' # 1 = performed in the same cohort | 2 = performed in another cohort\n",
        "]\n",
        "\n",
        "# Checking if data is alright as expected\n",
        "calculate_percentage_distribution(df, columns = qualitylist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLxwA8_wqP_L"
      },
      "outputs": [],
      "source": [
        "# Preparatory settings\n",
        "colpos = 'Article Title'\n",
        "newcolname = 'Quality_Index_Mestre' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df[qualitylist].sum(axis=1)\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GFeHWU6jKF_"
      },
      "source": [
        "## Transitioning from subtype to precisision medicine article\n",
        "\n",
        "Taken from the recent precision medicine article: https://movementdisorders.onlinelibrary.wiley.com/doi/epdf/10.1002/mds.29708"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855-xJdw9gM8"
      },
      "source": [
        "### Purpose 1 = To predict disease progression for clinical care"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2S3DrZdA38L"
      },
      "outputs": [],
      "source": [
        "df[df['Validation_in_a_Different_Cohort'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlAOEEkYEwk6"
      },
      "outputs": [],
      "source": [
        "newcolname = 'Longitudinal_Followup_Years'\n",
        "newcolname2 = 'Completeness_of_Followup'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa4f3-v-DygN"
      },
      "outputs": [],
      "source": [
        "# Requirements\n",
        "criteria = {\n",
        "    'Individual_Classification_Algorithm': lambda x: x == 1, # Provides individual level prediction\n",
        "    'Longitudinal_Followup_Status': lambda x: x == 1, # Has a longitudinal follow-up\n",
        "    'Longitudinal_Followup_Years': lambda x: x >= 3, # Needs an adequate follow-up time, 3 seems to be the minimum\n",
        "    'Clustering_Number_of_Specific_Domains': lambda x: x > 1, # Needs to be multidimensional, 2 seems to be the bare minimum\n",
        "    'Validation_Number_of_Specific_Domains': lambda x: x > 1, # Needs to be multidimensional, 2 seems to be the bare minimum\n",
        "    'Number of PD patients utilized for clustering purposes': lambda x: x > 100, # Needs adequate sample size, 100 seems to be the minimum\n",
        "    'Validation_in_a_Different_Cohort': lambda x: x == 1, # Validation performed in a different cohort\n",
        "    'Minimum_Info_Age_Sex_Staging': lambda x: x == 1 # Validation performed in a different cohort\n",
        "    }\n",
        "\n",
        "calculate_percentage_distribution(df, columns = list(criteria.keys()), max_unique_values=12) # Viewing those criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giSHrXHOjMPW"
      },
      "outputs": [],
      "source": [
        "# Preparatory settings\n",
        "colpos = 'Article Title' # Analysis column and position\n",
        "newcolname = 'Follows_All_Criteria_of_Good_Subtyping_for_Disease_Prediction' # Name of the new column\n",
        "newcolname2 = 'Number_of_Criteria_Followed_for_Good_Subtyping_for_Disease_Prediction' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: all(criteria[col](row[col]) for col in criteria), axis=1) # Who fills all the criteria\n",
        "newcol2 = df.apply(lambda row: sum(criteria[col](row[col]) for col in criteria), axis=1) # Number of criterias that are filled\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "  df.insert(insert_position, newcolname2, newcol2) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "  df[newcolname2] = newcol2 # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "  relevantvars.append(newcolname2)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "  print(newcolname2, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "print(df[newcolname2].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbD6JFY5BohY"
      },
      "source": [
        "Quick note - the article also mentions that lifestyle and genetic factors should be accounted for in subtyping, aswell as the ethnicity background\n",
        "\n",
        "Refer to the section on \"Minimum descriptive data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDrxAMqPBt5r"
      },
      "outputs": [],
      "source": [
        "siglas = ['None of the mentioned data on the PD population was informed by the study',\n",
        "           'Age', 'Gender', 'Ethnicity', 'Education', 'Income', 'Family History',\n",
        "           'Age at disease onset', 'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)',\n",
        "           'Vital Signs (blood pressure, cardiac frequency)', 'Physical Attributes (weight, height etc)',\n",
        "           'Medication Doses', 'Genetic Status (monogenic PD, polygenic risk score)',\n",
        "           'Summarised data from clinical scales (such as mean MDS-UPDRS scores, number of non-motor symptoms)',\n",
        "           'Summarised data from neuroimaging studies (such as cortical thickness)',\n",
        "           \"Biomarker's profile (results from biomarker tests)\"]\n",
        "\n",
        "# Viewing the distribution of responses\n",
        "calculate_percentage_distribution(df, siglas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ml3KeRfCfmp"
      },
      "source": [
        "**On MRI:** Moreover, much is still unknown about themolecular, genetic, and protein structures that contributeto disease progression and how they interact, even in the“genetic”PD subtypes.3Magnetic resonance imaging (MRI) markers show some promise for their prognosticvalue for clinically relevant disease milestones\n",
        "\n",
        "**On Clinical Trials:** Another comment is that clinical trials should collect data for post host analyses to be maade and to identify the found subtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cn-Mcs8FYvU"
      },
      "source": [
        "### Purpose 2 = To Predict Response to Treatments in Clinical Practice\n",
        "\n",
        "Needs to perform analyses on longitudinal medication usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_tDAeBsGDaO"
      },
      "source": [
        "### Purpose 3 = To Identify Therapeutic Targets for Disease Modification\n",
        "\n",
        "Good for extracting reference on our \"biological\" to \"clinical\" approach. Good for selection of studies focused only on omics data to see their qualities and quantities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5mgnKNWGFNp"
      },
      "outputs": [],
      "source": [
        "print(df['Clustering_Any_Omics_or_Biomarkers'].value_counts(dropna=False))\n",
        "print(df['Validation_Any_Omics_or_Biomarkers'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYmXzkynn_BM"
      },
      "outputs": [],
      "source": [
        "# Creating column if the study utilized either clustering or validation biomarkers\n",
        "criteria = {\n",
        "    'Clustering_Any_Omics_or_Biomarkers': lambda x: x == 1,\n",
        "    'Validation_Any_Omics_or_Biomarkers': lambda x: x == 1,\n",
        "}\n",
        "\n",
        "# Preparatory settings\n",
        "colpos = 'Validation_Any_Omics_or_Biomarkers' # Analysis column and position\n",
        "newcolname = 'Utilized_Biomarkers_or_Omics_Either_in_Clustering_or_Validation' # Name of the new column\n",
        "insert_position = df.columns.get_loc(colpos) + 1 # Calculating position\n",
        "\n",
        "# New column to be created\n",
        "newcol = df.apply(lambda row: any(criteria[col](row[col]) for col in criteria), axis=1).astype(int) # Who fills either criteria\n",
        "\n",
        "# Inserting the column\n",
        "try:\n",
        "  df.insert(insert_position, newcolname, newcol) # Inserting the column\n",
        "except ValueError:\n",
        "  df[newcolname] = newcol # Updating columns\n",
        "\n",
        "# Adding this list of variables to the relevantvars\n",
        "if newcolname not in relevantvars:\n",
        "  relevantvars.append(newcolname)\n",
        "else:\n",
        "  print(newcolname, 'Already added')\n",
        "\n",
        "print(df[newcolname].value_counts(dropna=False))\n",
        "\n",
        "df[['Clustering_Any_Omics_or_Biomarkers','Validation_Any_Omics_or_Biomarkers','Utilized_Biomarkers_or_Omics_Either_in_Clustering_or_Validation']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R3RPVoyLWwP"
      },
      "source": [
        "### Other notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeMmlUGELX9Q"
      },
      "source": [
        " It is our recommendation that the\n",
        "definition of “why, what, and where” becomes the\n",
        "starting steps for any future research identifying or validating PD subtyping systems, complemented by the use\n",
        "of methodological quality tools we have presented in\n",
        "our earlier work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVGHIEKxLuLp"
      },
      "source": [
        "One can question whether the various purposes identified in this paper could possibly be served by a single PD\n",
        "subtyping system. The development of a single PD subtype\n",
        "system serving different purposes would require significant\n",
        "collaborative efforts and computational power with the\n",
        "use of novel machine learning approaches, to combine\n",
        "molecular data with rich clinical data in high-volume longitudinal datasets from a large variety of different sources\n",
        "and with widely differing structures and qualities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmr6vHVIL0J8"
      },
      "source": [
        "However, existing datasets have limitations, including relatively\n",
        "short follow-up biased toward initial and middle disease\n",
        "stages, outcome assessment at fixed time points not\n",
        "accounting for day-to-day variability, lack of non-clinical\n",
        "biomarkers, little information on non-PD peripheral\n",
        "or central changes (eg, cardiovascular risk,46 diabetes\n",
        "mellitus,47,48 and compensatory factors), and bias\n",
        "toward slowly progressive cases able to stay engaged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHt05oFGL4Tc"
      },
      "source": [
        "In our previous review of subtyping studies, we concluded that, with few exceptions, PD subtypes have not\n",
        "been fruitful enough mainly due to lack of replication\n",
        "and validation across cohorts/papers, the instability of\n",
        "PD subtype membership, or a lack of deep phenotyping\n",
        "with biomarker data in PD subtype development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsSZ-5r8kcVp"
      },
      "source": [
        "Machine learning plays an important role in the delivery of individual-based knowledge,49 as it can analyze and cluster data in high-volume longitudinal datasets from a large variety of different sources and with widely differing structures and qualities. Depending on the purpose of the subtyping effort, these models may need to consider the temporal dynamics of clinical symptoms, biomarkers, and response to treatment. In this scenario, when reliable and accurate predictive modeling for individual patients is a reality, group-level predictions will become less relevant"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Table"
      ],
      "metadata": {
        "id": "xIeqYc3XC6rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the result\n",
        "sheet_path = 'Tables/Data of the study.xlsx'\n",
        "sheet_name = 'Data'\n",
        "\n",
        "with pd.ExcelWriter(sheet_path, engine='xlsxwriter') as writer:\n",
        "    df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width\n",
        "    for column in df:\n",
        "        column_width = max(df[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = df.columns.get_loc(column)\n",
        "        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_width)"
      ],
      "metadata": {
        "id": "kpEjMz5-DANI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill Supplementary Table 1"
      ],
      "metadata": {
        "id": "mQvNLKDkARn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qLJgpHdaAZiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "suptable = pd.read_excel('Artigo e Drafts/Supplementary Material/Supplementary Table 3. All Data.xlsx')\n",
        "suptable.head()"
      ],
      "metadata": {
        "id": "BrxXnEqjAUO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suptable.columns"
      ],
      "metadata": {
        "id": "urUCdSdwBD8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interestcols = ['Number_of_Criteria_Followed_for_Good_Subtyping_for_Disease_Prediction',\n",
        "       'Quality_Index_Mestre',\n",
        "       'Which type of PD patients were included in the clustering analyses?',\n",
        "       'Were subtyped patients roughly at the same disease stage or duration?',\n",
        "       'From how many different centers came the patients to be subtyped?',\n",
        "       'How were patients recruited?',\n",
        "       'Which diagnostic criteria was utilized to diagnose PD?',\n",
        "       'Which was the sampling method?',\n",
        "       'Was there a longitudinal follow-up in any way?',\n",
        "       'If longitudinal follow-up was provided, how much years were patients followed for?',\n",
        "       'Completeness of follow-up', 'Completeness_of_Followup',\n",
        "       'Longitudinal_Followup_Years',\n",
        "       'Which type of descriptive data regarding the PD population was presented?',\n",
        "       'Were the included patients for clustering purposes drug-naive?',\n",
        "       'Number of PD patients utilized for clustering purposes',\n",
        "       'Number of PD patients utilized for validation purposes',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [AMP-PD]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [BioFIND]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Fox Insight]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [GP2 Dataset]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [LRRK2 Cohort Consortium]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson Progression Marker Initiative (PPMI)]',\n",
        "       \"Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson's Disease Biomarker Program (PDBP)]\",\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [UK Biobank]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Oxford Parkinson Disease Center Discovery Cohort]',\n",
        "       'Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]',\n",
        "       'If a different publicly available dataset than the ones mentioned above was utlized for any means, please specify its full name',\n",
        "       'From which countries or world region came the patients whose data was used in the clustering?',\n",
        "       'Which type data was utilized for the clustering algorithm?',\n",
        "       'Which type data was evaluated for association / interpretation purposes of the identified clusters, indepentently if they were significant or not?',\n",
        "       'If the paper contains multiple exploration of clusters (creates different subgroups multiple times) shortly describe the differences in the cluster strategy for each exploration:',\n",
        "       'If clinical data obtained from rating scales was used for clustering purposes, which data and rating scales were utilized?',\n",
        "       'If neuroimaging data was utilized for clustering, did the article perform any sort of quality control measure?',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI cortical thickness]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI subcortical volume]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI white matter lesions / hyperintensities]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [Functional MRI]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [MRI Others]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [PET]',\n",
        "       'If neuroimaging data was utilized for clustering or interpretation / association  purposes, which ones were utilized? [SPECT DAT binding ratios]',\n",
        "       'How many subtypes did the evaluated study identified?',\n",
        "       'If they were identified, what are the names (meaning) of the identified clusters?',\n",
        "       'What is the number of patients present in each of the identified clusters?',\n",
        "       'Subtype stability',\n",
        "       'Did the article provide an individual level prediction algorithm?']"
      ],
      "metadata": {
        "id": "5SBQZPouA_SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the rows of suptable\n",
        "for row in range(len(suptable)):\n",
        "    # Check if the 'Article Title' matches in both DataFrames\n",
        "    if suptable['Article Title'].iloc[row] == df['Article Title'].iloc[row]:\n",
        "        # Iterate through the columns of interest and set values using .loc\n",
        "        for col in interestcols:\n",
        "            suptable.loc[row, col] = df.loc[row, col]\n",
        "\n",
        "# Optionally save the updated suptable to a CSV file\n",
        "suptable.to_excel('Artigo e Drafts/Supplementary Material/Supplementary Table 3. All Data (Automatical).xlsx', index=False)\n",
        "\n",
        "print(\"Updated suptable with data from df\")\n"
      ],
      "metadata": {
        "id": "65E2mQY2BQ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suptable.tail()"
      ],
      "metadata": {
        "id": "QD1SCsrFAiBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qau93s8XrLGt"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uadGcOtk5LFp"
      },
      "source": [
        "## Year comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wbpaOvs5OmY"
      },
      "source": [
        "### Cut-off: quantile division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMPteByj5Ic9"
      },
      "outputs": [],
      "source": [
        "# REMOVING VALIDATION AND CLUSTERING NAMES - THEY ARE INTERESTING, BUT I WANT THE MOST RELEVANT ASPECTS\n",
        "# REMOVING ratingscales ALSO - FOCUSED\n",
        "\n",
        "dependentvar = 'Year of publication Quantile Division'\n",
        "\n",
        "# Running the code - first creating the master table\n",
        "comparison_analysis = analyze_data_corrected_v8(df[relevantvars],\n",
        "                                                qualitative_variable = dependentvar,\n",
        "                                                p_significance=4, decimal_places=1,\n",
        "                                                exclude_vars = ['Article Title', 'Author (only the last name)',\n",
        "                                                                'Reviewer'] + neuroimagingvars + validation_names + clustering_names + ratingscales, # Those are giving problems to the analysis\n",
        "                                                numerical_threshold=5, printsteps = False, hidetotal = False)\n",
        "\n",
        "# Running the adjustment function\n",
        "# First one - creates new rows for categories\n",
        "detailed_df = detailed_output_analysis_condensed(comparison_analysis)\n",
        "\n",
        "# Second one - adjusts the last created output\n",
        "condensed_df = condense_rows(detailed_df)\n",
        "print('Lenght of dataset:',len(condensed_df))\n",
        "condensed_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBJs0EAj5bKk"
      },
      "outputs": [],
      "source": [
        "# Export the result\n",
        "\n",
        "sheet_path = 'Tables/'+dependentvar+'.xlsx'\n",
        "sheet_name = dependentvar[0:31]\n",
        "\n",
        "with pd.ExcelWriter(sheet_path, engine='xlsxwriter') as writer:\n",
        "    condensed_df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width\n",
        "    for column in condensed_df:\n",
        "        column_width = max(condensed_df[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = condensed_df.columns.get_loc(column)\n",
        "        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_width)\n",
        "\n",
        "    # Ensure p-value is treated as numeric\n",
        "    if 'p-value' in condensed_df.columns:\n",
        "        condensed_df['p-value'] = pd.to_numeric(condensed_df['p-value'], errors='coerce')\n",
        "\n",
        "    # Only significant interactions\n",
        "    sigcondensed = condensed_df[condensed_df['p-value'] < 0.05]\n",
        "    sheet_name = 'Significant'\n",
        "\n",
        "    sigcondensed.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width for the significant sheet\n",
        "    for column in sigcondensed:\n",
        "        column_width = max(sigcondensed[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = sigcondensed.columns.get_loc(column)\n",
        "        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgVMJuLQ5V-Y"
      },
      "source": [
        "### Cut-off: 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3gHOzU8Ja4jg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7oTDC_NsiKJ"
      },
      "outputs": [],
      "source": [
        "# REMOVING VALIDATION AND CLUSTERING NAMES - THEY ARE INTERESTING, BUT I WANT THE MOST RELEVANT ASPECTS\n",
        "# REMOVING ratingscales ALSO - FOCUSED\n",
        "\n",
        "dependentvar = 'Before_2020_After'\n",
        "\n",
        "# Running the code - first creating the master table\n",
        "comparison_analysis = analyze_data_corrected_v8(df[relevantvars],\n",
        "                                                qualitative_variable = dependentvar,\n",
        "                                                numerical_threshold=5,\n",
        "                                                p_significance=4, decimal_places=1,\n",
        "                                                exclude_vars = ['Article Title', 'Author (only the last name)',\n",
        "                                                                'Reviewer'] + neuroimagingvars + ratingscales, # Those are giving problems to the analysis\n",
        "                                                printsteps = False, hidetotal = False)\n",
        "\n",
        "# Running the adjustment function\n",
        "# First one - creates new rows for categories\n",
        "detailed_df = detailed_output_analysis_condensed(comparison_analysis)\n",
        "\n",
        "# Second one - adjusts the last created output\n",
        "condensed_df = condense_rows(detailed_df)\n",
        "print('Lenght of dataset:',len(condensed_df))\n",
        "condensed_df.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cec3mY_H5p0M"
      },
      "outputs": [],
      "source": [
        "# Export the result\n",
        "sheet_path = 'Tables/'+dependentvar+'.xlsx'\n",
        "sheet_name = dependentvar\n",
        "\n",
        "with pd.ExcelWriter(sheet_path, engine='xlsxwriter') as writer:\n",
        "    condensed_df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width\n",
        "    for column in condensed_df:\n",
        "        column_width = max(condensed_df[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = condensed_df.columns.get_loc(column)\n",
        "        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_width)\n",
        "\n",
        "    # Ensure p-value is treated as numeric\n",
        "    if 'p-value' in condensed_df.columns:\n",
        "        condensed_df['p-value'] = pd.to_numeric(condensed_df['p-value'], errors='coerce')\n",
        "\n",
        "    # Only significant interactions\n",
        "    sigcondensed = condensed_df[condensed_df['p-value'] < 0.05]\n",
        "    sheet_name = 'Significant'\n",
        "\n",
        "    sigcondensed.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width for the significant sheet\n",
        "    for column in sigcondensed:\n",
        "        column_width = max(sigcondensed[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = sigcondensed.columns.get_loc(column)\n",
        "        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PpQcYM3rzHh"
      },
      "source": [
        "## Open access datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s5hVxQzsFUa"
      },
      "outputs": [],
      "source": [
        "# Defining column of interest\n",
        "colpos = 'Open_Datasets_Used_Yes_or_No' # Analysis column and position\n",
        "sheet_path = 'Tables/' + colpos + '.xlsx'\n",
        "\n",
        "# Checking\n",
        "df[colpos].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX4108jVsFUb"
      },
      "outputs": [],
      "source": [
        "# Removed: neuroimagingvars (error), clustering names, Validation names and ratingscales - too much detail\n",
        "\n",
        "# Replace dependent names to avoid it being the same as independent\n",
        "# Step 1: Store original names/values\n",
        "original_values = df[colpos].copy()\n",
        "\n",
        "# Step 2: Substitute names/values for analysis\n",
        "df[colpos] = df[colpos].replace(0, 'No')\n",
        "df[colpos] = df[colpos].replace(1, 'Yes')\n",
        "\n",
        "\n",
        "\n",
        "# Execute analysis or other processing steps\n",
        "comparison_analysis = analyze_data_corrected_v8(df[relevantvars],\n",
        "                                                qualitative_variable=colpos,\n",
        "                                                p_significance=4, decimal_places=1,\n",
        "                                                exclude_vars=['Article Title', 'Author (only the last name)',\n",
        "                                                              'Reviewer'] + neuroimagingvars + validation_names + clustering_names + ratingscales,\n",
        "                                                numerical_threshold=5, printsteps=False, hidetotal=False)\n",
        "\n",
        "# Running the adjustment function\n",
        "# First one - creates new rows for categories\n",
        "detailed_df = detailed_output_analysis_condensed(comparison_analysis)\n",
        "\n",
        "# Second one - adjusts the last created output\n",
        "condensed_df = condense_rows(detailed_df)\n",
        "\n",
        "# Step 3: Revert to original names/values after analysis\n",
        "df[colpos] = original_values\n",
        "\n",
        "# Exporting\n",
        "with pd.ExcelWriter(sheet_path, engine='xlsxwriter') as writer:\n",
        "    # Writing the full dataset to its own sheet\n",
        "    condensed_df.to_excel(writer, sheet_name=colpos, index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width for the full dataset\n",
        "    max_width_scale_factor = 0.95\n",
        "    for column in condensed_df:\n",
        "        column_width = max_width_scale_factor * max(condensed_df[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = condensed_df.columns.get_loc(column)\n",
        "        writer.sheets[colpos].set_column(col_idx, col_idx, column_width*max_width_scale_factor)\n",
        "\n",
        "    # Filtering and writing only significant interactions to another sheet\n",
        "    sigcondensed = condensed_df[condensed_df['p-value'] < 0.05]\n",
        "    sigcondensed.to_excel(writer, sheet_name='Significant', index=False, startrow=0)\n",
        "\n",
        "    # Adjusting column width for the significant sheet\n",
        "    for column in sigcondensed:\n",
        "        column_width = max_width_scale_factor * max(sigcondensed[column].astype(str).map(len).max(), len(column))\n",
        "        col_idx = sigcondensed.columns.get_loc(column)\n",
        "        writer.sheets['Significant'].set_column(col_idx, col_idx, column_width*max_width_scale_factor)\n",
        "\n",
        "# Showing results and output\n",
        "print('Length of dataset:', len(condensed_df))\n",
        "condensed_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7koCv7OGeKG"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df['None of the mentioned data on the PD population was informed by the study'], df['Open_Datasets_Used_Yes_or_No'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filling pre-made table\n",
        "\n",
        "Created a table that will be in the official article and I want to fill it automatically with each information in their position\n",
        "\n",
        "To use this, take the names present in the column called \"Var name\" in the Shape table and put them in \"Table 1 (To be Filled)\" in substitution of the formal name\n",
        "\n",
        "Once it is finished, reset it with the appropriate names from the (Shape) table"
      ],
      "metadata": {
        "id": "2SS8PHHzHxUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "UhyqlzGHIADn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Table to input data\n",
        "filltable = pd.read_excel('Artigo e Drafts/Obsoleto/Table 1 (To be Filled).xlsx')\n",
        "filltable.head()"
      ],
      "metadata": {
        "id": "vht-YpbWIC9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Table to take data from\n",
        "datatable = pd.read_excel('Tables/Before_2020_After.xlsx')\n",
        "datatable.head()"
      ],
      "metadata": {
        "id": "ahgeSSDjI85I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting p-value to less decimals\n",
        "# Function to modify p-value\n",
        "def modify_p_value(p_value):\n",
        "    if isinstance(p_value, float):\n",
        "        if p_value < 0.001:\n",
        "            return \"< 0.001\"\n",
        "        if p_value < 0.01:\n",
        "            return \"< 0.01\"\n",
        "        else:\n",
        "            return f\"{p_value:.2f}\"\n",
        "    return p_value\n",
        "\n",
        "# Apply the function to the p-value column\n",
        "datatable['p-value'] = datatable['p-value'].apply(modify_p_value)\n",
        "\n",
        "# Function to round percentage values to the nearest integer\n",
        "def round_percentage(value):\n",
        "    try:\n",
        "        temp = int(round(float(value.strip('%'))))\n",
        "        tempstring = str(temp) + '%'\n",
        "        return tempstring\n",
        "    except ValueError:\n",
        "        return value  # Return the value as is if it can't be converted to float\n",
        "\n",
        "# Apply the function to the % Complete Data column\n",
        "datatable['% Complete Data'] = datatable['% Complete Data'].apply(round_percentage)\n",
        "\n",
        "datatable.head()"
      ],
      "metadata": {
        "id": "iySujS-oN_Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_values(row):\n",
        "    var_name = row['Table 1. General characteristics of the included studies according to year of publication range']\n",
        "    print(var_name)\n",
        "    if pd.notnull(var_name):\n",
        "        for variable in datatable['Variables'].values:\n",
        "            if var_name in variable:\n",
        "                match = datatable[datatable['Variables'] == variable]\n",
        "                if not match.empty:\n",
        "                    row['Unnamed: 4'] = match['Total (n = 82)'].values[0]\n",
        "                    row['Unnamed: 5'] = match['0.0 (n = 43)'].values[0]\n",
        "                    row['Unnamed: 6'] = match['1.0 (n = 39)'].values[0]\n",
        "                    row['Unnamed: 7'] = match['p-value'].values[0]\n",
        "                break\n",
        "    return row\n",
        "\n",
        "# Apply the mapping function to filltable\n",
        "filltable = filltable.apply(map_values, axis=1)\n",
        "\n",
        "# Display the updated filltable\n",
        "filltable.head()"
      ],
      "metadata": {
        "id": "ql943L7_aXKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Updated mapping function\n",
        "def map_values(row):\n",
        "    var_name = row['Unnamed: 4']\n",
        "    if pd.notnull(var_name) and var_name in datatable['Variables'].values:\n",
        "        match = datatable[datatable['Variables'] == var_name]\n",
        "        if not match.empty:\n",
        "            row['Unnamed: 5'] = match['0.0 (n = 42)'].values[0]\n",
        "            row['Unnamed: 6'] = match['1.0 (n = 39)'].values[0]\n",
        "            row['Unnamed: 7'] = match['p-value'].values[0]\n",
        "            row['Unnamed: 8'] = match['% Complete Data'].values[0]\n",
        "    return row\n",
        "\n",
        "# Apply the mapping function to filltable\n",
        "filltable = filltable.apply(map_values, axis=1)\n",
        "\n",
        "filltable.head()'''"
      ],
      "metadata": {
        "id": "7JzNBa-0Wbfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining things\n",
        "colpos = 'Table 1 (Filled)' # Analysis column and position\n",
        "sheet_path = 'Artigo e Drafts/Obsoleto/' + colpos + '.xlsx'\n",
        "\n",
        "# Exporting\n",
        "with pd.ExcelWriter(sheet_path, engine='xlsxwriter') as writer:\n",
        "    # Writing the full dataset to its own sheet\n",
        "    filltable.to_excel(writer, sheet_name=colpos, index=False, startrow=0)\n",
        "\n",
        "# Showing results and output\n",
        "print('Length of dataset:', len(filltable))\n",
        "filltable.head(10)"
      ],
      "metadata": {
        "id": "GBxUqc-eL_WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filling pre-made table (Open_dataset)\n",
        "\n",
        "Created a table that will be in the official article and I want to fill it automatically with each information in their position\n",
        "\n",
        "To use this, take the names present in the column called \"Var name\" in the Shape table and put them in \"Table 1 (To be Filled)\" in substitution of the formal name\n",
        "\n",
        "Once it is finished, reset it with the appropriate names from the (Shape) table"
      ],
      "metadata": {
        "id": "cr6ihqibvl2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "gGLzzku9vl2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Table to input data\n",
        "filltable = pd.read_excel('Artigo e Drafts/Obsoleto/Table 1 (To be Filled Open Datasets).xlsx')\n",
        "filltable.head()"
      ],
      "metadata": {
        "id": "QhBQ7gilvl2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Table to take data from\n",
        "datatable = pd.read_excel('Tables/Open_Datasets_Used_Yes_or_No.xlsx')\n",
        "datatable.head()"
      ],
      "metadata": {
        "id": "AnjlRGujvl2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting p-value to less decimals\n",
        "# Function to modify p-value\n",
        "def modify_p_value(p_value):\n",
        "    if isinstance(p_value, float):\n",
        "        if p_value < 0.001:\n",
        "            return \"< 0.001\"\n",
        "        if p_value < 0.01:\n",
        "            return \"< 0.01\"\n",
        "        else:\n",
        "            return f\"{p_value:.2f}\"\n",
        "    return p_value\n",
        "\n",
        "# Apply the function to the p-value column\n",
        "datatable['p-value'] = datatable['p-value'].apply(modify_p_value)\n",
        "\n",
        "# Function to round percentage values to the nearest integer\n",
        "def round_percentage(value):\n",
        "    try:\n",
        "        temp = int(round(float(value.strip('%'))))\n",
        "        tempstring = str(temp) + '%'\n",
        "        return tempstring\n",
        "    except ValueError:\n",
        "        return value  # Return the value as is if it can't be converted to float\n",
        "\n",
        "# Apply the function to the % Complete Data column\n",
        "datatable['% Complete Data'] = datatable['% Complete Data'].apply(round_percentage)\n",
        "\n",
        "datatable.head()"
      ],
      "metadata": {
        "id": "XSWMllzqvl2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_values(row):\n",
        "    var_name = row['Table 1. General characteristics of the included studies according to year of publication range']\n",
        "    print(var_name)\n",
        "    if pd.notnull(var_name):\n",
        "        for variable in datatable['Variables'].values:\n",
        "            if var_name in variable:\n",
        "                match = datatable[datatable['Variables'] == variable]\n",
        "                if not match.empty:\n",
        "                    row['Unnamed: 4'] = match['No (n = 57)'].values[0]\n",
        "                    row['Unnamed: 5'] = match['Yes (n = 25)'].values[0]\n",
        "                    row['Unnamed: 6'] = match['p-value'].values[0]\n",
        "                break\n",
        "    return row\n",
        "\n",
        "# Apply the mapping function to filltable\n",
        "filltable = filltable.apply(map_values, axis=1)\n",
        "\n",
        "# Display the updated filltable\n",
        "filltable.head()"
      ],
      "metadata": {
        "id": "nJN1SZXWvl2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Updated mapping function\n",
        "def map_values(row):\n",
        "    var_name = row['Unnamed: 4']\n",
        "    if pd.notnull(var_name) and var_name in datatable['Variables'].values:\n",
        "        match = datatable[datatable['Variables'] == var_name]\n",
        "        if not match.empty:\n",
        "            row['Unnamed: 5'] = match['0.0 (n = 42)'].values[0]\n",
        "            row['Unnamed: 6'] = match['1.0 (n = 39)'].values[0]\n",
        "            row['Unnamed: 7'] = match['p-value'].values[0]\n",
        "            row['Unnamed: 8'] = match['% Complete Data'].values[0]\n",
        "    return row\n",
        "\n",
        "# Apply the mapping function to filltable\n",
        "filltable = filltable.apply(map_values, axis=1)\n",
        "\n",
        "filltable.head()'''"
      ],
      "metadata": {
        "id": "4qrGpUiLvl2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining things\n",
        "colpos = 'Table 1 (Filled Open Datasets)' # Analysis column and position\n",
        "sheet_path = 'Artigo e Drafts/Obsoleto/' + colpos + '.xlsx'\n",
        "\n",
        "# Exporting\n",
        "with pd.ExcelWriter(sheet_path, engine='xlsxwriter') as writer:\n",
        "    # Writing the full dataset to its own sheet\n",
        "    filltable.to_excel(writer, sheet_name=colpos, index=False, startrow=0)\n",
        "\n",
        "# Showing results and output\n",
        "print('Length of dataset:', len(filltable))\n",
        "filltable.head(10)"
      ],
      "metadata": {
        "id": "ADuLgM0jvl2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCiQUj4Je16F"
      },
      "source": [
        "# Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05QxkTeHe4U4"
      },
      "source": [
        "## Essential progression criteria\n",
        "\n",
        "**Idea:** select X articles and produce a bar chart like this one: https://drive.google.com/file/d/1r4qX5pM9eG5HR_PjCNsy9kKG4-o-r0EN/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMpggaD7w0Y"
      },
      "source": [
        "### Creating variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZqrpcy-iMRf"
      },
      "outputs": [],
      "source": [
        "# Creating new columns based on aforementioned criteria\n",
        "df['Individual classification algorithm'] = df['Individual_Classification_Algorithm'].apply(lambda x: 1 if x == 1 else 0)\n",
        "df['Longitudinal follow-up'] = df['Longitudinal_Followup_Status'].apply(lambda x: 1 if x == 1 else 0)\n",
        "df['Follow up data for at least 3 years'] = df['Longitudinal_Followup_Years'].apply(lambda x: 1 if x >= 3 else 0)\n",
        "df['Clustering with 2 or more domains'] = df['Clustering_Number_of_Specific_Domains'].apply(lambda x: 1 if x > 1 else 0)\n",
        "df['Post hoc analysis with 2 or more domains'] = df['Validation_Number_of_Specific_Domains'].apply(lambda x: 1 if x > 1 else 0)\n",
        "df['Clustering with at least 100 patients'] = df['Number of PD patients utilized for clustering purposes'].apply(lambda x: 1 if x > 100 else 0)\n",
        "df['Validation in a different cohort'] = df['Validation_in_a_Different_Cohort'].apply(lambda x: 1 if x == 1 else 0)\n",
        "df['Describes basic demographic and clinical characteristics of the patients analysed'] = df['Minimum_Info_Age_Sex_Staging'].apply(lambda x: 1 if x == 1 else 0) # Age, sex and disease staging\n",
        "df['Year Category'] = df['Before_2020_After'].apply(lambda x: '2021 - 2024' if str(x) in ['1', '1.0'] else '1999 - 2020')\n",
        "\n",
        "transition_criteria = [\n",
        "    'Year Category',\n",
        "    'Individual classification algorithm',\n",
        "    'Longitudinal follow-up',\n",
        "    'Follow up data for at least 3 years',\n",
        "    'Clustering with 2 or more domains',\n",
        "    'Post hoc analysis with 2 or more domains',\n",
        "    'Clustering with at least 100 patients',\n",
        "    'Validation in a different cohort',\n",
        "    'Describes basic demographic and clinical characteristics of the patients analysed'\n",
        "]\n",
        "\n",
        "df[['Year of publication'] + transition_criteria].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxHwAgpSiytY"
      },
      "outputs": [],
      "source": [
        "calculate_percentage_distribution(df, transition_criteria[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caE6AgA49qr6"
      },
      "source": [
        "### Option 1 - All Studies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Esg55t6djjU3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "# Define a figure name\n",
        "figurename = 'Non essential criteria general'\n",
        "\n",
        "# Calculate percentages\n",
        "percentages = (df[transition_criteria[1:]].mean() * 100).round()\n",
        "\n",
        "# Sort percentages in ascending order\n",
        "percentages = percentages.sort_values()\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 8))  # Increase figure size for more space\n",
        "\n",
        "# Get evenly spaced colors from the viridis colormap\n",
        "num_bars = len(percentages)\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, num_bars))\n",
        "\n",
        "# Plot the bars with custom colors and alpha\n",
        "bars = ax.barh(percentages.index, percentages.values, color=colors, alpha=0.8)\n",
        "\n",
        "# Add labels to the bars\n",
        "label_padding = 1  # Distance of labels from the bars\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    # Position the text to the right of the bars, with a custom padding\n",
        "    ax.text(width + label_padding, bar.get_y() + bar.get_height()/2, f'{width:.0f}%', va='center', ha='left', fontsize=12)\n",
        "\n",
        "# Adjust column names to occupy 2 lines in graph (centered in middle, limited line width)\n",
        "column_names = [textwrap.fill(col, width=38) for col in percentages.index]\n",
        "ax.set_yticklabels(column_names)\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Percentage', fontsize=12)\n",
        "\n",
        "# Adjust x-axis limits to 100% with spaces every 10%\n",
        "ax.set_xlim(0, 110)\n",
        "ax.set_xticks(range(0, 101, 10))\n",
        "\n",
        "# Increase axis tick size\n",
        "ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "# Remove vertical grid lines\n",
        "ax.grid(False)\n",
        "plt.tight_layout()  # Ensure labels are not cut off\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(f'Plots/SVG/{figurename}.svg', format='svg', dpi=300)\n",
        "plt.savefig(f'Plots/PNG/{figurename}.png', format='png', dpi=300)\n",
        "\n",
        "# Show plot\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5NK4blA9uDx"
      },
      "source": [
        "### Option 2 - Divided by Year"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing simple chi square among categories\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def chi_square_test(column1, column2):\n",
        "    # Create a contingency table\n",
        "    contingency_table = pd.crosstab(column1.astype(str), column2.astype(str))\n",
        "\n",
        "    # Perform the chi-square test\n",
        "    try:\n",
        "      chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return None\n",
        "\n",
        "    # Return the p-value\n",
        "    return round(p,3)\n",
        "\n",
        "# Apply chi-square test for each column and create a new column for p-values\n",
        "\n",
        "transition_criteria_no_year_chi_square = []\n",
        "for col in transition_criteria[1:]:\n",
        "    transition_criteria_no_year_chi_square.append(chi_square_test(df[col], df['Year Category']))"
      ],
      "metadata": {
        "id": "llncZlMb1Xq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e9xy5Ey2pN5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "import numpy as np\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Define a figurename\n",
        "figurename = 'Essential progression criteria by year'\n",
        "\n",
        "# Assuming 'df' and 'transition_criteria' are defined\n",
        "# Calculate percentages for each 'Year Category'\n",
        "percentages = df[transition_criteria].groupby('Year Category').mean().T * 100\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(14, 10))  # Adjust the size as needed\n",
        "\n",
        "bar_width = 0.35  # Set the width of each bar\n",
        "index = np.arange(len(percentages))  # The label locations\n",
        "\n",
        "# Reverse the order of columns to plot '1999 - 2021' first\n",
        "columns = percentages.columns[::-1]\n",
        "\n",
        "# Choose a color palette\n",
        "current_palette = cm.RdBu  # Options: 'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic'\n",
        "alpha = 0.75 # alpha\n",
        "\n",
        "# Plot bars for each year category, reversed to have '1999 - 2021' at top\n",
        "for i, year in enumerate(percentages.columns[::-1]):\n",
        "    # Use colors from extreme ends of the colormap\n",
        "    color_1 = current_palette(0.0)  # Start of the colormap\n",
        "    color_2 = current_palette(1.0)  # End of the colormap\n",
        "\n",
        "    # Assign colors alternately from each end of the colormap for visibility\n",
        "    if i % 2 == 0:\n",
        "        color = color_1\n",
        "    else:\n",
        "        color = color_2\n",
        "\n",
        "    bars = ax.barh(index + i * bar_width, percentages[year], bar_width, label=year, alpha=alpha, color=color)\n",
        "\n",
        "    # Add labels to the bars\n",
        "    label_padding = 1  # Slightly increase padding for clarity\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + label_padding, bar.get_y() + bar.get_height()/2, f'{width:.0f}%', va='center', ha='left', fontsize=12)\n",
        "\n",
        "# Define p-values and corresponding bold condition\n",
        "p_values = transition_criteria_no_year_chi_square\n",
        "columns_p_values = dict(zip(transition_criteria[1:], p_values))\n",
        "\n",
        "# Adjust column names to occupy 2 lines in graph and append * if p-value < 0.05\n",
        "column_names = [textwrap.fill(col + ('*' if columns_p_values[col] < 0.05 else ''), width=25) for col in percentages.index]\n",
        "\n",
        "ax.set_yticks(index + bar_width / 2 * (len(columns) - 1))\n",
        "ax.set_yticklabels(column_names)\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Percentage', fontsize=16)\n",
        "'''ax.set_title('Percentage of Criteria Met by Year Category', fontsize=16)'''\n",
        "ax.legend()\n",
        "\n",
        "# Adjust x-axis limits to 100% with spaces every 10%\n",
        "ax.set_xlim(0, 100)  # Extend x-axis limit to fit labels comfortably\n",
        "ax.set_xticks(range(0, 101, 10))\n",
        "\n",
        "# Increase x-axis tick size\n",
        "ax.tick_params(axis='x', labelsize=13)\n",
        "ax.tick_params(axis='y', labelsize=13)\n",
        "\n",
        "### Adjusting legends ###\n",
        "# Get handles and labels from the existing plot\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "# Reverse the order of handles/labels for the legend\n",
        "handles = handles[::-1]\n",
        "labels = labels[::-1]\n",
        "\n",
        "# Display the legend with modifications\n",
        "ax.legend(handles, labels, fontsize='large', title='Year Category', title_fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Very similar, shows there isn't really anything significant\n",
        "percentages"
      ],
      "metadata": {
        "id": "hu6tjxUQJFRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Option 3 - Donut Chart\n",
        "\n",
        "Showing the total of studies that fall within specific criterias counts\n",
        "\n",
        "Total possible: 8"
      ],
      "metadata": {
        "id": "KAqCHL-Fd6Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Number_of_Criteria_Followed_for_Good_Subtyping_for_Disease_Prediction'].value_counts())"
      ],
      "metadata": {
        "id": "JxoGvx-DeXVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define a figurename\n",
        "figurename = 'Essential progression criteria donut chart without text'\n",
        "\n",
        "# Setting data for use\n",
        "data = df['Number_of_Criteria_Followed_for_Good_Subtyping_for_Disease_Prediction'].value_counts()\n",
        "\n",
        "# Convert the series to a DataFrame\n",
        "tempdf = data.reset_index()\n",
        "tempdf.columns = ['Number_of_Criteria', 'Count']\n",
        "\n",
        "# Sort the data by 'Number_of_Criteria' in ascending order (not necessary here as we want words in order)\n",
        "tempdf = tempdf.sort_values(by='Number_of_Criteria', ascending=True)\n",
        "\n",
        "# Create a mapping dictionary\n",
        "criteria_mapping = {\n",
        "    0: \"Zero\",\n",
        "    1: \"One\",\n",
        "    2: \"Two\",\n",
        "    3: \"Three\",\n",
        "    4: \"Four\",\n",
        "    5: \"Five\",\n",
        "    6: \"Six\",\n",
        "    7: \"Seven\",\n",
        "    8: \"Eight\"\n",
        "}\n",
        "\n",
        "# Apply the mapping\n",
        "tempdf['Number_of_Criteria'] = tempdf['Number_of_Criteria'].replace(criteria_mapping)\n",
        "\n",
        "# Create a color palette with Matplotlib's viridis, applying a softer alpha to each color\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(tempdf)))\n",
        "colors = [(r, g, b, 0.6) for r, g, b, _ in colors]  # Adding alpha transparency\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))  # Maintain a large figure size for clarity\n",
        "\n",
        "# Suppress the first and last labels\n",
        "labels = tempdf['Number_of_Criteria'].tolist()\n",
        "labels[0] = \"\"\n",
        "labels[-1] = \"\"\n",
        "\n",
        "wedges, texts, autotexts = ax.pie(tempdf['Count'], labels=labels, autopct='%1.0f%%', startangle=90, colors=colors, pctdistance=0.85, textprops={'color':\"black\", 'fontsize': 14})\n",
        "\n",
        "# Draw a circle at the center to create a donut shape\n",
        "centre_circle = plt.Circle((0,0), 0.50, fc='white')  # Adjust the radius to 0.50 to reduce the size of the central white space\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "# Improve text readability and aesthetics for remaining labels\n",
        "for text in texts:\n",
        "    text.set_fontsize(24)   # Increase font size for labels\n",
        "\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('black')  # Ensure percentage labels are visible\n",
        "    autotext.set_fontsize(16)    # Consistent and readable font size\n",
        "\n",
        "# Reposition the first and last labels manually\n",
        "angle_first = wedges[0].theta2 - (wedges[0].theta2 - wedges[0].theta1) / 2\n",
        "angle_last = wedges[-1].theta2 - (wedges[-1].theta2 - wedges[-1].theta1) / 2\n",
        "\n",
        "x_first = 2.0 * np.cos(np.radians(angle_first))\n",
        "y_first = 1.1 * np.sin(np.radians(angle_first))\n",
        "x_last = 1.55 * np.cos(np.radians(angle_last))\n",
        "y_last = 1.1 * np.sin(np.radians(angle_last))\n",
        "\n",
        "# Add custom text for the first label\n",
        "ax.text(x_first, y_first, tempdf['Number_of_Criteria'].iloc[0], ha='center', va='center', fontsize=24, color='black')\n",
        "\n",
        "# Add custom text for the last label\n",
        "ax.text(x_last, y_last, tempdf['Number_of_Criteria'].iloc[-1], ha='center', va='center', fontsize=24, color='black')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to ensure there is no content overlap\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(f'Plots/SVG/{figurename}.svg', format='svg', dpi=300)\n",
        "plt.savefig(f'Plots/PNG/{figurename}.png', format='png', dpi=300)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "crlUz80ueOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define a figurename\n",
        "figurename = 'Essential progression criteria donut chart with text'\n",
        "\n",
        "# Setting data for use\n",
        "data = df['Number_of_Criteria_Followed_for_Good_Subtyping_for_Disease_Prediction'].value_counts()\n",
        "\n",
        "# Convert the series to a DataFrame\n",
        "tempdf = data.reset_index()\n",
        "tempdf.columns = ['Number_of_Criteria', 'Count']\n",
        "\n",
        "# Sort the data by 'Number_of_Criteria' in ascending order (not necessary here as we want words in order)\n",
        "tempdf = tempdf.sort_values(by='Number_of_Criteria', ascending=True)\n",
        "\n",
        "# Create a mapping dictionary\n",
        "criteria_mapping = {\n",
        "    0: \"Zero\",\n",
        "    1: \"One\",\n",
        "    2: \"Two\",\n",
        "    3: \"Three\",\n",
        "    4: \"Four\",\n",
        "    5: \"Five\",\n",
        "    6: \"Six\",\n",
        "    7: \"Seven\",\n",
        "    8: \"Eight\"\n",
        "}\n",
        "\n",
        "# Apply the mapping\n",
        "tempdf['Number_of_Criteria'] = tempdf['Number_of_Criteria'].replace(criteria_mapping)\n",
        "\n",
        "# Create a color palette with Matplotlib's viridis, applying a softer alpha to each color\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(tempdf)))\n",
        "colors = [(r, g, b, 0.6) for r, g, b, _ in colors]  # Adding alpha transparency\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))  # Maintain a large figure size for clarity\n",
        "\n",
        "# Suppress the first and last labels\n",
        "labels = tempdf['Number_of_Criteria'].tolist()\n",
        "labels[0] = \"\"\n",
        "labels[-1] = \"\"\n",
        "\n",
        "wedges, texts, autotexts = ax.pie(tempdf['Count'], labels=labels, autopct='%1.0f%%', startangle=90, colors=colors, pctdistance=0.85, textprops={'color':\"black\", 'fontsize': 14})\n",
        "\n",
        "# Draw a circle at the center to create a donut shape\n",
        "centre_circle = plt.Circle((0,0), 0.50, fc='white')  # Adjust the radius to 0.50 to reduce the size of the central white space\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "# Improve text readability and aesthetics for remaining labels\n",
        "for text in texts:\n",
        "    text.set_fontsize(24)   # Increase font size for labels\n",
        "\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('black')  # Ensure percentage labels are visible\n",
        "    autotext.set_fontsize(16)    # Consistent and readable font size\n",
        "\n",
        "# Reposition the first and last labels manually\n",
        "angle_first = wedges[0].theta2 - (wedges[0].theta2 - wedges[0].theta1) / 2\n",
        "angle_last = wedges[-1].theta2 - (wedges[-1].theta2 - wedges[-1].theta1) / 2\n",
        "\n",
        "x_first = 2.0 * np.cos(np.radians(angle_first))\n",
        "y_first = 1.1 * np.sin(np.radians(angle_first))\n",
        "x_last = 1.55 * np.cos(np.radians(angle_last))\n",
        "y_last = 1.1 * np.sin(np.radians(angle_last))\n",
        "\n",
        "# Add custom text for the first label\n",
        "ax.text(x_first, y_first, tempdf['Number_of_Criteria'].iloc[0], ha='center', va='center', fontsize=24, color='black')\n",
        "\n",
        "# Add custom text for the last label\n",
        "ax.text(x_last, y_last, tempdf['Number_of_Criteria'].iloc[-1], ha='center', va='center', fontsize=24, color='black')\n",
        "\n",
        "# Add phrase in the middle of the donut chart\n",
        "plt.text(0, 0, 'No study achieved\\nall the 8 desired\\nessential criteria',\n",
        "         horizontalalignment='center', verticalalignment='center', fontsize=18, color='black')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to ensure there is no content overlap\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(f'Plots/SVG/{figurename}.svg', format='svg', dpi=300)\n",
        "plt.savefig(f'Plots/PNG/{figurename}.png', format='png', dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RBOqXecNNCF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-nF4tNP8HEG"
      },
      "source": [
        "## Non essential criteria\n",
        "\n",
        "**Idea:** present visually other criteria that are relevant but weren't cited by the article on the essential criteria\n",
        "\n",
        "It is mostly a display of measures collected by the quality index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGTe1BvE8HEN"
      },
      "source": [
        "### Creating variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyaKLDYZ8HEO"
      },
      "outputs": [],
      "source": [
        "# Defining criteria for the graph\n",
        "criteria = [\n",
        "    'Year Category',\n",
        "    'Formal_Criteria_Utilized_or_Informed',\n",
        "    'Recruitment_Community_or_Population',\n",
        "    'Specific_Cluster_Validation_Differences_Yes_or_No',\n",
        "    'Similar_PD_Staging',\n",
        "    'Drug_Naive_Status',\n",
        "    'Multicentric_X_Others',\n",
        "    'Consecutive_or_Random_Sampling',\n",
        "    'Performed_Validation']\n",
        "\n",
        "data = df[criteria]\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Year Category': 'Year Category',\n",
        "    'Formal_Criteria_Utilized_or_Informed': 'Formal diagnostic criteria used',\n",
        "    'Recruitment_Community_or_Population': 'Populational or community based recruitment',\n",
        "    'Specific_Cluster_Validation_Differences_Yes_or_No': 'Post hoc analysis with different variables than those used in clustering',\n",
        "    'Similar_PD_Staging': 'Patients in a similar PD staging or duration',\n",
        "    'Drug_Naive_Status': 'Patients being drug naive at clustering',\n",
        "    'Multicentric_X_Others': 'Multicentric study',\n",
        "    'Consecutive_or_Random_Sampling': 'Consecutive or random sampling',\n",
        "    'Performed_Validation': 'Validation performed either within or in another dataset'}\n",
        "\n",
        "# Data situation\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Review\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOwIVB-s8HEO"
      },
      "source": [
        "### Option 1 - All Studies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAwStoT98HEO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "# Define a figure name\n",
        "figurename = 'Non essential criteria general'\n",
        "\n",
        "# Calculate percentages\n",
        "percentages = (data[list(column_renames.values())[1:]].mean() * 100).round()\n",
        "\n",
        "# Sort percentages in ascending order\n",
        "percentages = percentages.sort_values()\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 8))  # Increase figure size for more space\n",
        "\n",
        "# Get evenly spaced colors from the viridis colormap\n",
        "num_bars = len(percentages)\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, num_bars))\n",
        "\n",
        "# Plot the bars with custom colors and alpha\n",
        "bars = ax.barh(percentages.index, percentages.values, color=colors, alpha=0.8)\n",
        "\n",
        "# Add labels to the bars\n",
        "label_padding = 1  # Distance of labels from the bars\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    # Position the text to the right of the bars, with a custom padding\n",
        "    ax.text(width + label_padding, bar.get_y() + bar.get_height()/2, f'{width:.0f}%', va='center', ha='left', fontsize=12)\n",
        "\n",
        "# Adjust column names to occupy 2 lines in graph (centered in middle, limited line width)\n",
        "column_names = [textwrap.fill(col, width=38) for col in percentages.index]\n",
        "ax.set_yticklabels(column_names)\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Percentage', fontsize=12)\n",
        "\n",
        "# Adjust x-axis limits to 100% with spaces every 10%\n",
        "ax.set_xlim(0, 110)\n",
        "ax.set_xticks(range(0, 101, 10))\n",
        "\n",
        "# Increase axis tick size\n",
        "ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "# Remove vertical grid lines\n",
        "ax.grid(False)\n",
        "plt.tight_layout()  # Ensure labels are not cut off\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(f'Plots/SVG/{figurename}.svg', format='svg', dpi=300)\n",
        "plt.savefig(f'Plots/PNG/{figurename}.png', format='png', dpi=300)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iGDainF8HEO"
      },
      "source": [
        "### Option 2 - Divided by Year"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing simple chi square among categories\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def chi_square_test(column1, column2):\n",
        "    # Create a contingency table\n",
        "    contingency_table = pd.crosstab(column1.astype(str), column2.astype(str))\n",
        "\n",
        "    # Perform the chi-square test\n",
        "    try:\n",
        "      chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return None\n",
        "\n",
        "    # Return the p-value\n",
        "    return round(p,3)\n",
        "\n",
        "# Apply chi-square test for each column and create a new column for p-values\n",
        "\n",
        "# Apply chi-square test for each column against 'Year Category' and store p-values\n",
        "chi_square = []\n",
        "for col in list(column_renames.values())[1:]:\n",
        "    p_value = chi_square_test(data[col], data['Year Category'])  # Assuming 'data' has 'Year Category'\n",
        "    chi_square.append(p_value)\n",
        "\n",
        "print(chi_square)"
      ],
      "metadata": {
        "id": "acQxkAJS8HEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWDJt5OA8HEO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "import numpy as np\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Define a figurename\n",
        "figurename = 'Non essential criteria by year'\n",
        "\n",
        "# Assuming 'df' and 'transition_criteria' are defined\n",
        "# Calculate percentages for each 'Year Category'\n",
        "percentages = data[list(column_renames.values())].groupby('Year Category').mean().T * 100\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(14, 10))  # Adjust the size as needed\n",
        "\n",
        "bar_width = 0.35  # Set the width of each bar\n",
        "index = np.arange(len(percentages))  # The label locations\n",
        "\n",
        "# Reverse the order of columns to plot '1999 - 2021' first\n",
        "columns = percentages.columns[::-1]\n",
        "\n",
        "# Choose a color palette\n",
        "current_palette = cm.coolwarm  # Options: 'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic'\n",
        "alpha = 0.7 # alpha\n",
        "\n",
        "# Plot bars for each year category, reversed to have '1999 - 2021' at top\n",
        "for i, year in enumerate(percentages.columns[::-1]):\n",
        "    # Use colors from extreme ends of the colormap\n",
        "    color_1 = current_palette(0.0)  # Start of the colormap\n",
        "    color_2 = current_palette(1.0)  # End of the colormap\n",
        "\n",
        "    # Assign colors alternately from each end of the colormap for visibility\n",
        "    if i % 2 == 0:\n",
        "        color = color_1\n",
        "    else:\n",
        "        color = color_2\n",
        "\n",
        "    bars = ax.barh(index + i * bar_width, percentages[year], bar_width, label=year, alpha=alpha, color=color)\n",
        "\n",
        "    # Add labels to the bars\n",
        "    label_padding = 1  # Slightly increase padding for clarity\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + label_padding, bar.get_y() + bar.get_height()/2, f'{width:.0f}%', va='center', ha='left', fontsize=12)\n",
        "\n",
        "# Define p-values and corresponding bold condition\n",
        "columns_p_values = dict(zip(list(column_renames.values())[1:], chi_square))\n",
        "\n",
        "# Adjust column names to occupy 2 lines in graph and append * if p-value < 0.05\n",
        "column_names = [textwrap.fill(col + ('*' if columns_p_values[col] < 0.05 else ''), width=25) for col in percentages.index]\n",
        "\n",
        "ax.set_yticks(index + bar_width / 2 * (len(columns) - 1))\n",
        "ax.set_yticklabels(column_names)\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Percentage', fontsize=16)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust x-axis limits to 100% with spaces every 10%\n",
        "ax.set_xlim(0, 100)  # Extend x-axis limit to fit labels comfortably\n",
        "ax.set_xticks(range(0, 101, 10))\n",
        "\n",
        "# Increase x-axis tick size\n",
        "ax.tick_params(axis='x', labelsize=13)\n",
        "ax.tick_params(axis='y', labelsize=13)\n",
        "\n",
        "### Adjusting legends ###\n",
        "# Get handles and labels from the existing plot\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "# Reverse the order of handles/labels for the legend\n",
        "handles = handles[::-1]\n",
        "labels = labels[::-1]\n",
        "\n",
        "# Display the legend with modifications\n",
        "ax.legend(handles, labels, fontsize='large', title='Year Category', title_fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive data availability"
      ],
      "metadata": {
        "id": "oktWWqM6Lnvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "RwUb2zaO1HuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed (no interest) - 'Income', 'Vital Signs (blood pressure, cardiac frequency)', 'Physical Attributes (weight, height etc)'\n",
        "\n",
        "descriptivedata = ['Before_2020_After','None of the mentioned data on the PD population was informed by the study',\n",
        "           'Age', 'Gender', 'Ethnicity', 'Education', 'Family History',\n",
        "           'Age at disease onset', 'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)',\n",
        "           'Medication Doses', 'Genetic_Status_Informed',\n",
        "           'Summarised data from clinical scales (such as mean MDS-UPDRS scores, number of non-motor symptoms)',\n",
        "           'Summarised data from neuroimaging studies (such as cortical thickness)',\n",
        "           \"Biomarker's profile (results from biomarker tests)\"]\n",
        "\n",
        "# Doing simple chi square among categories\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def chi_square_test(column1, column2):\n",
        "    # Create a contingency table\n",
        "    contingency_table = pd.crosstab(column1.astype(str), column2.astype(str))\n",
        "\n",
        "    # Perform the chi-square test\n",
        "    try:\n",
        "      chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return None\n",
        "\n",
        "    # Return the p-value\n",
        "    return round(p,3)\n",
        "\n",
        "# Apply chi-square test for each column and create a new column for p-values\n",
        "\n",
        "pvalueslist = []\n",
        "for col in descriptivedata[1:]:\n",
        "    pvalueslist.append(chi_square_test(df[col], df['Before_2020_After']))\n",
        "\n",
        "pvalueslist"
      ],
      "metadata": {
        "id": "Z6YtAZKe1CcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[descriptivedata]\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Before_2020_After':'Year Category',\n",
        "    'None of the mentioned data on the PD population was informed by the study': 'No data provided',\n",
        "    'Age': 'Age',\n",
        "    'Gender': 'Sex',\n",
        "    'Ethnicity': 'Ethnicity',\n",
        "    'Education': 'Education',\n",
        "    'Family History': 'Family history',\n",
        "    'Age at disease onset': 'Disease onset age',\n",
        "    'Disease duration (years) or staging (such as Hoehn & Yahr and Schawb and England scales)': 'Disease duration/staging',\n",
        "    'Medication Doses': 'Medication doses',\n",
        "    'Genetic_Status_Informed': 'Genetic status',\n",
        "    'Summarised data from clinical scales (such as mean MDS-UPDRS scores, number of non-motor symptoms)': 'Clinical scales data',\n",
        "    'Summarised data from neuroimaging studies (such as cortical thickness)': 'Neuroimaging data',\n",
        "    \"Biomarker's profile (results from biomarker tests)\": 'Biomarker profile'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ],
      "metadata": {
        "id": "PyrH6SQG1GKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1 - All Studies\n"
      ],
      "metadata": {
        "id": "4Bs4644303YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "# Define a figure name\n",
        "figurename = 'Descriptive data availability general'\n",
        "\n",
        "# Calculate percentages\n",
        "percentages = (data[list(column_renames.values())[1:]].mean() * 100).round()\n",
        "\n",
        "# Sort percentages in ascending order\n",
        "percentages = percentages.sort_values()\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 8))  # Increase figure size for more space\n",
        "\n",
        "# Get evenly spaced colors from the viridis colormap\n",
        "num_bars = len(percentages)\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, num_bars))\n",
        "\n",
        "# Plot the bars with custom colors and alpha\n",
        "bars = ax.barh(percentages.index, percentages.values, color=colors, alpha=0.8)\n",
        "\n",
        "# Add labels to the bars\n",
        "label_padding = 1  # Distance of labels from the bars\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    # Position the text to the right of the bars, with a custom padding\n",
        "    ax.text(width + label_padding, bar.get_y() + bar.get_height()/2, f'{width:.0f}%', va='center', ha='left', fontsize=12)\n",
        "\n",
        "# Adjust column names to occupy 2 lines in graph (centered in middle, limited line width)\n",
        "column_names = [textwrap.fill(col, width=38) for col in percentages.index]\n",
        "ax.set_yticklabels(column_names)\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Percentage', fontsize=12)\n",
        "\n",
        "# Adjust x-axis limits to 100% with spaces every 10%\n",
        "ax.set_xlim(0, 110)\n",
        "ax.set_xticks(range(0, 101, 10))\n",
        "\n",
        "# Increase axis tick size\n",
        "ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "# Remove vertical grid lines\n",
        "ax.grid(False)\n",
        "plt.tight_layout()  # Ensure labels are not cut off\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(f'Plots/SVG/{figurename}.svg', format='svg', dpi=300)\n",
        "plt.savefig(f'Plots/PNG/{figurename}.png', format='png', dpi=300)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LqVAsqDX0_IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 2 - Divided by Year\n"
      ],
      "metadata": {
        "id": "Now2LVeW06we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Year Category'] = data['Year Category'].replace('1.0', '2021 - 2024')\n",
        "data['Year Category'] = data['Year Category'].replace('0.0', '1999 - 2020')"
      ],
      "metadata": {
        "id": "KBMWPBGHVtC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[list(column_renames.values())].groupby('Year Category').mean()"
      ],
      "metadata": {
        "id": "fhljHc9kPouS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting a figure name\n",
        "figurename = 'Descriptive data informed by year'\n",
        "\n",
        "# Assuming 'df' and 'transition_criteria' are defined\n",
        "# Calculate percentages for each 'Year Category'\n",
        "percentages = data[list(column_renames.values())].groupby('Year Category').mean().T * 100\n",
        "\n",
        "# Reverse the DataFrame along the index to feed the results backwards\n",
        "percentages = percentages.iloc[::-1]\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(14, 10))  # Adjust the size as needed\n",
        "\n",
        "bar_width = 0.35  # Set the width of each bar\n",
        "index = np.arange(len(percentages))  # The label locations\n",
        "\n",
        "# Reverse the order of columns to plot '1999 - 2021' first\n",
        "columns = percentages.columns[::-1]\n",
        "\n",
        "### OPTION 1 FOR COLOR - PROVIDING HEX VALUES TO A LIST\n",
        "color_palette = ['#1f77b4', '#ff7f0e']  # Two hex values\n",
        "alpha = 0.8  # Transparency\n",
        "\n",
        "# Plot bars for each year category, reversed to have '1999 - 2021' at top\n",
        "for i, year in enumerate(columns):\n",
        "    # Assign colors alternately from the custom palette for visibility\n",
        "    color = color_palette[i % 2]  # Alternating colors\n",
        "    bars = ax.barh(index + i * bar_width, percentages[year], bar_width, label=year, alpha=alpha, color=color)\n",
        "\n",
        "    # Add labels to the bars\n",
        "    label_padding = 1  # Slightly increase padding for clarity\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + label_padding, bar.get_y() + bar.get_height()/2, f'{width:.0f}%', va='center', ha='left', fontsize=12)\n",
        "\n",
        "# Define p-values and corresponding bold condition\n",
        "columns_p_values = dict(zip(list(column_renames.values())[1:], pvalueslist))\n",
        "\n",
        "# Adjust column names to occupy 2 lines in graph and append * if p-value < 0.05\n",
        "column_names = [textwrap.fill(col + ('*' if columns_p_values[col] < 0.05 else ''), width=20) for col in percentages.index]\n",
        "\n",
        "ax.set_yticks(index + bar_width / 2 * (len(columns) - 1))\n",
        "ax.set_yticklabels(column_names)\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Percentage', fontsize=16)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust x-axis limits to 100% with spaces every 10%\n",
        "ax.set_xlim(0, 100)  # Extend x-axis limit to fit labels comfortably\n",
        "ax.set_xticks(range(0, 101, 10))\n",
        "\n",
        "# Increase x-axis tick size\n",
        "ax.tick_params(axis='x', labelsize=13)\n",
        "ax.tick_params(axis='y', labelsize=13)\n",
        "\n",
        "### Adjusting legends ###\n",
        "# Get handles and labels from the existing plot\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "handles.reverse()\n",
        "labels.reverse()\n",
        "\n",
        "# Adjusting legends: Positioned outside the graph to the right, centered vertically, no frame\n",
        "ax.legend(handles, labels, fontsize='large', title='Year Category', title_fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G5MvZFqrXGuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### OPTION 2 FOR PROVIDING COLOR\n",
        "# Choose a color palette\n",
        "current_palette = cm.BrBG  # Options: 'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic'\n",
        "alpha = 0.75 # alpha\n",
        "\n",
        "# Plot bars for each year category, reversed to have '1999 - 2021' at top\n",
        "for i, year in enumerate(percentages.columns[::-1]):\n",
        "    # Use colors from extreme ends of the colormap\n",
        "    color_1 = current_palette(0.0)  # Start of the colormap\n",
        "    color_2 = current_palette(1.0)  # End of the colormap\n",
        "\n",
        "    # Assign colors alternately from each end of the colormap for visibility\n",
        "    if i % 2 == 0:\n",
        "        color = color_1\n",
        "    else:\n",
        "        color = color_2\n",
        "\n",
        "    bars = ax.barh(index + i * bar_width, percentages[year], bar_width, label=year, alpha=alpha, color=color)"
      ],
      "metadata": {
        "id": "VGkO8bz1T0Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with radar chart - bad"
      ],
      "metadata": {
        "id": "BdDR-npoWx7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "# Assuming 'data' and 'column_renames' are defined\n",
        "# Calculate percentages for each 'Year Category'\n",
        "percentages = data[list(column_renames.values())].groupby('Year Category').mean().T * 100\n",
        "\n",
        "# Prepare Radar Chart Data\n",
        "labels = percentages.index\n",
        "num_vars = len(labels)\n",
        "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()  # Close the loop\n",
        "angles += angles[:1]  # Ensure the graph starts and ends at the same point\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10), subplot_kw=dict(polar=True))\n",
        "\n",
        "# Draw one axe per variable and add labels\n",
        "ax.set_theta_offset(np.pi / 2)\n",
        "ax.set_theta_direction(-1)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "# Draw ylabels\n",
        "ax.set_rlabel_position(0)\n",
        "ax.set_yticks([20, 40, 60, 80, 100])  # Define the y-ticks to match your scale\n",
        "ax.set_yticklabels([\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"])\n",
        "ax.set_ylim(0, 100)\n",
        "\n",
        "# Plot each 'Year Category'\n",
        "for col in percentages.columns:\n",
        "    values = percentages[col].tolist()\n",
        "    values += values[:1]  # Close the loop\n",
        "    ax.plot(angles, values, linewidth=1, linestyle='solid', label=col)\n",
        "    ax.fill(angles, values, alpha=0.25)  # Adjust alpha to improve visibility if needed\n",
        "\n",
        "# Add a legend\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "\n",
        "plt.title('Descriptive Data Informed by Year - Radar Chart', size=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Fiyg3gNYWYfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Countries represented\n",
        "\n",
        "Adjusted - PPMI isn't counting as Middle East and Africa (only 1 center)"
      ],
      "metadata": {
        "id": "ynVABynha1ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the different options for answers\n",
        "descriptivedata = [\n",
        "    'Sub-Saharan Africa_Adjusted_PPMI',\n",
        "    'Middle East & North Africa_Adjusted_PPMI',\n",
        "    'East Asia & Pacific (China, Japan, Australia)_Adjusted_PPMI',\n",
        "    'South Asia (India, Pakistan, Bangladesh)_Adjusted_PPMI',\n",
        "    'Europe & Central Asia_Adjusted_PPMI',\n",
        "    'Latin America & the Caribbean_Adjusted_PPMI',\n",
        "    'North America_Adjusted_PPMI'\n",
        "]\n",
        "\n",
        "data = df[descriptivedata]\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Sub-Saharan Africa_Adjusted_PPMI':'Sub-Saharan Africa',\n",
        "    'Middle East & North Africa_Adjusted_PPMI':'Middle East & North Africa',\n",
        "    'East Asia & Pacific (China, Japan, Australia)_Adjusted_PPMI':'East Asia & Pacific',\n",
        "    'South Asia (India, Pakistan, Bangladesh)_Adjusted_PPMI':'South Asia',\n",
        "    'Europe & Central Asia_Adjusted_PPMI':'Europe & Central Asia',\n",
        "    'Latin America & the Caribbean_Adjusted_PPMI':'Latin America & the Caribbean',\n",
        "    'North America_Adjusted_PPMI':'North America'}\n",
        "\n",
        "# Data situation\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Sum up all 1's in each column to get the count of studies per region\n",
        "region_counts = data.sum()\n",
        "\n",
        "# Calculate percentages\n",
        "total_patients = region_counts.sum()\n",
        "percentages = region_counts / total_patients * 100\n",
        "percentages = round(percentages,0)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, column_renames.values())"
      ],
      "metadata": {
        "id": "InEqRbk2a4Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentages"
      ],
      "metadata": {
        "id": "qH8CvPGMwitS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "!pip install squarify\n",
        "import squarify\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "figurename = 'Countries represented adjusted PPMI'\n",
        "\n",
        "# Assuming 'percentages' contains the percentage distribution data\n",
        "# Prepare data and filter out zero values\n",
        "filtered_percentages = percentages[percentages > 0]\n",
        "\n",
        "# Adjust very small values slightly for better visibility in the plot\n",
        "min_area = 4  # Minimum percentage area for visibility\n",
        "adjusted_sizes = [max(value, min_area) for value in filtered_percentages]\n",
        "\n",
        "# Generate labels with percentages\n",
        "labels = [f\"{idx}\\n({val:.0f}%)\" for idx, val in zip(filtered_percentages.index, filtered_percentages)]\n",
        "\n",
        "# Plotting a Treemap only with non-zero regions\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ax.set_facecolor('black')  # Set the background color to black\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(filtered_percentages)))  # Color map adjustment\n",
        "squarify.plot(sizes=adjusted_sizes,\n",
        "              label=labels,\n",
        "              alpha=0.55,\n",
        "              color=colors,\n",
        "              text_kwargs={'fontsize':17.5, 'color':'black'},  # Ensure labels are visible against a black background\n",
        "              ec='black',  # Set the edge color to white for visibility\n",
        "              linewidth=2.5)  # Increase line width for thicker borders\n",
        "\n",
        "# Add an outer border\n",
        "rect = patches.Rectangle((0, 0), 1, 1, fill=False, color=\"black\", linewidth=3, transform=ax.transAxes, clip_on=False)\n",
        "ax.add_patch(rect)\n",
        "plt.axis('off')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7xROAxY6dHr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Install rpy2\n",
        "!pip install rpy2\n",
        "\n",
        "# Load the R extension\n",
        "%load_ext rpy2.ipython'''"
      ],
      "metadata": {
        "id": "3lKakHTZwUfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%R\n",
        "# Install necessary packages if not already installed\n",
        "if (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n",
        "if (!require(\"treemapify\")) install.packages(\"treemapify\")\n",
        "if (!require(\"viridis\")) install.packages(\"viridis\")\n",
        "\n",
        "# Load the packages\n",
        "library(ggplot2)\n",
        "library(treemapify)\n",
        "library(viridis)'''"
      ],
      "metadata": {
        "id": "pUTFxZPCzZ7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%R\n",
        "# Define the data\n",
        "data <- data.frame(\n",
        "  Region = c(\"Sub-Saharan Africa\", \"Middle East & North Africa\", \"East Asia & Pacific\",\n",
        "             \"South Asia\", \"Europe & Central Asia\", \"Latin America & the Caribbean\",\n",
        "             \"North America\"),\n",
        "  Counts = c(0, 0, 19, 0, 47, 1, 32)\n",
        ")\n",
        "\n",
        "# Apply viridis color map with alpha set to 0.55\n",
        "colors <- viridis_pal(alpha = 1)(length(unique(data$Region)))\n",
        "\n",
        "# Create the treemap\n",
        "treemap <- ggplot(data, aes(area = Counts, fill = Region, label = paste(Region, \"\\n(\", Counts, \"%)\", sep=\"\"))) +\n",
        "  geom_treemap() +\n",
        "  geom_treemap_text(fontface = \"bold\", colour = \"black\", place = \"centre\", grow = TRUE, reflow = TRUE) +\n",
        "  scale_fill_manual(values = colors) +\n",
        "  theme_minimal() +\n",
        "  theme(legend.position = \"none\")\n",
        "\n",
        "# Print the treemap\n",
        "print(treemap)\n",
        "'''"
      ],
      "metadata": {
        "id": "Jvj-tMHOxBEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is a DataFrame where rows are entries and columns are regions\n",
        "region_totals = data.sum()\n",
        "\n",
        "# Prepare data for the treemap\n",
        "regions = region_totals.index\n",
        "counts = region_totals.values\n",
        "\n",
        "# Apply square root transformation to 'counts' for a more balanced visual representation\n",
        "transformed_counts = np.sqrt(counts)\n",
        "\n",
        "# Create the treemap\n",
        "fig = px.treemap(\n",
        "    data_frame=pd.DataFrame({'Region': regions, 'Counts': transformed_counts}),  # Creating a DataFrame directly\n",
        "    path=[px.Constant(\"all\"), 'Region'],  # Defines the hierarchy of the treemap\n",
        "    values='Counts',  # The area of each sector\n",
        "    title=\"Treemap of Patient Distribution by Region\"\n",
        ")\n",
        "\n",
        "# Apply color palette similar to viridis\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(regions)))\n",
        "colors_hex = [f'rgb({int(c[0]*255)}, {int(c[1]*255)}, {int(c[2]*255)})' for c in colors]\n",
        "fig.update_traces(marker_colors=colors_hex)\n",
        "\n",
        "# Update layout aesthetics\n",
        "fig.update_layout(\n",
        "    margin=dict(t=50, l=25, r=25, b=25)\n",
        ")\n",
        "\n",
        "# Display the treemap\n",
        "fig.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "oCvKHvSuvnBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN_4E-jxSE1r"
      },
      "source": [
        "## Feature types according to year (clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS3DjvS6p2Yd"
      },
      "source": [
        "### All features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmpu4nJhTx_I"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Clustering_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                'Clustering_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                'Clustering_Clinical_Including_Cognition',\n",
        "                'Clustering_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other',\n",
        "                'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                'Clustering_Any_Neurophysiology',\n",
        "                'Clustering_Any_Neuroimaging',\n",
        "                'Clustering_Any_Omics']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "try:\n",
        "    data = df[featuretypes]\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}\")\n",
        "    print(\"Check the following column names:\", featuretypes)\n",
        "    print(\"Existing columns in df:\", df.columns)\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Clustering_Clinical_Any_Motor_Scales_No_Complications': 'Motor scales',\n",
        "    'Clustering_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)': 'Motor complications',\n",
        "    'Clustering_Clinical_Including_Cognition': 'Cognition',\n",
        "    'Clustering_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other': 'Other non-motor',\n",
        "    'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)': 'Biomarkers',\n",
        "    'Clustering_Any_Neurophysiology': 'Neurophysiology',\n",
        "    'Clustering_Any_Neuroimaging': 'Neuroimaging',\n",
        "    'Clustering_Any_Omics': 'Omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWFbxhR0nEyJ"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering feature types according to year - all'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gr2-Hmgisvb"
      },
      "source": [
        "### All features 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyyDZFTAisvh"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Clustering_Any_Clinical',\n",
        "                'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                'Clustering_Any_Neurophysiology',\n",
        "                'Clustering_Any_Neuroimaging',\n",
        "                'Clustering_Any_Omics']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "try:\n",
        "    data = df[featuretypes]\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}\")\n",
        "    print(\"Check the following column names:\", featuretypes)\n",
        "    print(\"Existing columns in df:\", df.columns)\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Clustering_Any_Clinical':'Clinical',\n",
        "    'Clustering_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)': 'Biomarkers',\n",
        "    'Clustering_Any_Neurophysiology': 'Neurophysiology',\n",
        "    'Clustering_Any_Neuroimaging': 'Neuroimaging',\n",
        "    'Clustering_Any_Omics': 'Omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgqBGKybisvi"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering feature types according to year - all 2.0'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ozdWKsq1Fh"
      },
      "source": [
        "### Clinical non-motor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxPLLvmQq1Fq"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Clustering_Clinical_Including_Cognition',\n",
        "                'Clustering_Clinical_Any_Neuropsychiatric',\n",
        " 'Clustering_Clinical_Any_Sleep',\n",
        " 'Clustering_Clinical_Any_Olfaction',\n",
        " 'Clustering_Clinical_Any_Autonomic',\n",
        " 'Clustering_Clinical_QoL',\n",
        " 'Clustering_Clinical_Non_Motor_Other']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Clustering_Clinical_Including_Cognition' : 'Cognition',\n",
        "    'Clustering_Clinical_Any_Neuropsychiatric': 'Neuropsychiatric symptoms',\n",
        "    'Clustering_Clinical_Any_Sleep': 'Sleep symptoms',\n",
        "    'Clustering_Clinical_Any_Olfaction': 'Anosmia or hyposmia',\n",
        "    'Clustering_Clinical_Any_Autonomic': 'Autonomic symptoms',\n",
        "    'Clustering_Clinical_QoL': 'Quality of life',\n",
        "    'Clustering_Clinical_Non_Motor_Other': 'Other symptoms'}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, column_renames.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrNbsYzCq1Fq"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering feature types according to year - clinical non-motor detailed'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfCwAAvjlVxE"
      },
      "source": [
        "### Clinical motor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me2Vaj-YlVxE"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Clustering_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                'Clustering_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                 'Clustering_Clinical_Phonation',\n",
        " 'Clustering_Clinical_Motor_Tests',\n",
        " 'Clustering_Any_Demographic',\n",
        " 'Clustering_Any_Time_or_Staging_Measurements',\n",
        " 'Clustering_Clinical_Vital_or_Neurologic_Examination'\n",
        "]\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Clustering_Clinical_Any_Motor_Scales_No_Complications': 'Motor scales',\n",
        "    'Clustering_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)': 'Motor complications',\n",
        "        'Clustering_Clinical_Phonation': 'Phonation',\n",
        "    'Clustering_Clinical_Motor_Tests': 'Motor tests',\n",
        "    'Clustering_Any_Demographic': 'Demographic data',\n",
        "    'Clustering_Any_Time_or_Staging_Measurements':'Disease staging',\n",
        "    'Clustering_Clinical_Vital_or_Neurologic_Examination': 'Neurologic examination findings'\n",
        "\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA5Jxr5PlVxF"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering feature types according to year - clinical motor detailed'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5iAYHeb0IUn"
      },
      "source": [
        "### Neuroimaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFQaeXDC0IUu"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Clustering_Neuroimaging - Structural or Volumetric MRI',\n",
        " 'Clustering_Neuroimaging - Diffusion Imaging MRI',\n",
        " 'Clustering_Neuroimaging - DaTSCAN',\n",
        " 'Clustering_Neuroimaging - Radiomics',\n",
        " 'Clustering_Neuroimaging - Functional MRI',\n",
        " 'Clustering_Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Clustering_Neuroimaging - Structural or Volumetric MRI': 'Structural or Volumetric MRI',\n",
        " 'Clustering_Neuroimaging - Diffusion Imaging MRI': 'Diffusion Imaging MRI',\n",
        " 'Clustering_Neuroimaging - DaTSCAN': 'DaTSCAN',\n",
        " 'Clustering_Neuroimaging - Radiomics': 'Radiomics',\n",
        " 'Clustering_Neuroimaging - Functional MRI': 'Functional MRI',\n",
        " 'Clustering_Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)': 'Other Methods'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNwuTOJJ0IUu"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering feature types according to year - neuroimaging'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 0.55, 0.05))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 0.55, 0.05)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 0.55)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5URbP5iB1NpU"
      },
      "source": [
        "### Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gFJSvlN1RZH"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                 'Clustering_Omics - Proteomics',\n",
        " 'Clustering_Omics - Transcriptomics',\n",
        " 'Clustering_Omics - Genomics',\n",
        " 'Clustering_Omics - Lipidomics',\n",
        " 'Clustering_Omics - Other (e.g. metabolomics, metagenomics)']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Clustering_Omics - Proteomics': 'Proteomics',\n",
        "    'Clustering_Omics - Transcriptomics': 'Transcriptomics',\n",
        "    'Clustering_Omics - Genomics': 'Genomics',\n",
        "    'Clustering_Omics - Lipidomics': 'Lipidomics',\n",
        "    'Clustering_Omics - Other (e.g. metabolomics, metagenomics)': 'Other omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYrzUG531SV1"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering feature types according to year - omics'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 0.55, 0.05))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 0.55, 0.05)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 0.55)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thQ5RzK7weh5"
      },
      "source": [
        "## Feature types according to year (validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C6xnhMrweh_"
      },
      "source": [
        "### All features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VutTtYDnweh_"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Validation_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                'Validation_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                'Validation_Clinical_Including_Cognition',\n",
        "                'Validation_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other',\n",
        "                'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                'Validation_Any_Neurophysiology',\n",
        "                'Validation_Any_Neuroimaging',\n",
        "                'Validation_Any_Omics']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "try:\n",
        "    data = df[featuretypes]\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}\")\n",
        "    print(\"Check the following column names:\", featuretypes)\n",
        "    print(\"Existing columns in df:\", df.columns)\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Validation_Clinical_Any_Motor_Scales_No_Complications': 'Motor scales',\n",
        "    'Validation_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)': 'Motor complications',\n",
        "    'Validation_Clinical_Including_Cognition': 'Cognition',\n",
        "    'Validation_Clinical_Any_Non_Motor_Excluding_Cognition_QoL_Other': 'Other non-motor',\n",
        "    'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)': 'Biomarkers',\n",
        "    'Validation_Any_Neurophysiology': 'Neurophysiology',\n",
        "    'Validation_Any_Neuroimaging': 'Neuroimaging',\n",
        "    'Validation_Any_Omics': 'Omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLzkkgAHweh_"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Validation feature types according to year - all'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KfMLkkVkICu"
      },
      "source": [
        "### All features 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDQIvAJekICv"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Validation_Any_Clinical',\n",
        "                'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)',\n",
        "                'Validation_Any_Neurophysiology',\n",
        "                'Validation_Any_Neuroimaging',\n",
        "                'Validation_Any_Omics']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "try:\n",
        "    data = df[featuretypes]\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}\")\n",
        "    print(\"Check the following column names:\", featuretypes)\n",
        "    print(\"Existing columns in df:\", df.columns)\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Validation_Any_Clinical':'Clinical',\n",
        "    'Validation_Biomarkers (uric acid, LDL, tryglicerides, neurofilament light [Nfl], tau, SAA etc)': 'Biomarkers',\n",
        "    'Validation_Any_Neurophysiology': 'Neurophysiology',\n",
        "    'Validation_Any_Neuroimaging': 'Neuroimaging',\n",
        "    'Validation_Any_Omics': 'Omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exK6rsKhkICv"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Validation feature types according to year - all 2.0'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g-hYnZkmhnY"
      },
      "source": [
        "### Clinical non-motor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IunZPrK-mhne"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Validation_Clinical_Including_Cognition',\n",
        "                'Validation_Clinical_Any_Neuropsychiatric',\n",
        " 'Validation_Clinical_Any_Sleep',\n",
        " 'Validation_Clinical_Any_Olfaction',\n",
        " 'Validation_Clinical_Any_Autonomic',\n",
        " 'Validation_Clinical_QoL',\n",
        " 'Validation_Clinical_Non_Motor_Other']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Validation_Clinical_Including_Cognition' : 'Cognition',\n",
        "    'Validation_Clinical_Any_Neuropsychiatric': 'Neuropsychiatric symptoms',\n",
        "    'Validation_Clinical_Any_Sleep': 'Sleep symptoms',\n",
        "    'Validation_Clinical_Any_Olfaction': 'Anosmia or hyposmia',\n",
        "    'Validation_Clinical_Any_Autonomic': 'Autonomic symptoms',\n",
        "    'Validation_Clinical_QoL': 'Quality of life',\n",
        "    'Validation_Clinical_Non_Motor_Other': 'Other symptoms'}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, column_renames.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48NV1HMOmhne"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Validation feature types according to year - clinical non-motor detailed'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26gZN55emIwR"
      },
      "source": [
        "### Clinical motor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUDUk2r-mIwS"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Validation_Clinical_Any_Motor_Scales_No_Complications',\n",
        "                'Validation_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)',\n",
        "                 'Validation_Clinical_Phonation',\n",
        " 'Validation_Clinical_Motor_Tests',\n",
        " 'Validation_Any_Demographic',\n",
        " 'Validation_Any_Time_or_Staging_Measurements',\n",
        " 'Validation_Clinical_Vital_or_Neurologic_Examination'\n",
        "]\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Validation_Clinical_Any_Motor_Scales_No_Complications': 'Motor scales',\n",
        "    'Validation_Clinical - Motor Complications (dyskinesia, OFF periods, dystonia, motor fluctuations)': 'Motor complications',\n",
        "        'Validation_Clinical_Phonation': 'Phonation',\n",
        "    'Validation_Clinical_Motor_Tests': 'Motor tests',\n",
        "    'Validation_Any_Demographic': 'Demographic data',\n",
        "    'Validation_Any_Time_or_Staging_Measurements':'Disease staging',\n",
        "    'Validation_Clinical_Vital_or_Neurologic_Examination': 'Neurologic examination findings'\n",
        "\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, column_renames.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AONAHMUrmIwS"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Validation feature types according to year - clinical motor detailed'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpOI9WVtweh_"
      },
      "source": [
        "### Neuroimaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL11bEXMweiA"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Validation_Neuroimaging - Structural or Volumetric MRI',\n",
        " 'Validation_Neuroimaging - Diffusion Imaging MRI',\n",
        " 'Validation_Neuroimaging - DaTSCAN',\n",
        " 'Validation_Neuroimaging - Radiomics',\n",
        " 'Validation_Neuroimaging - Functional MRI',\n",
        " 'Validation_Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Validation_Neuroimaging - Structural or Volumetric MRI': 'Structural or Volumetric MRI',\n",
        " 'Validation_Neuroimaging - Diffusion Imaging MRI': 'Diffusion Imaging MRI',\n",
        " 'Validation_Neuroimaging - DaTSCAN': 'DaTSCAN',\n",
        " 'Validation_Neuroimaging - Radiomics': 'Radiomics',\n",
        " 'Validation_Neuroimaging - Functional MRI': 'Functional MRI',\n",
        " 'Validation_Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)': 'Other Methods'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, column_renames.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtOTBs41weiA"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Validation feature types according to year - neuroimaging'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 0.55, 0.05))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 0.55, 0.05)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 0.55)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNQcZE8yweiA"
      },
      "source": [
        "### Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PDK676qweiA"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                 'Validation_Omics - Proteomics',\n",
        " 'Validation_Omics - Transcriptomics',\n",
        " 'Validation_Omics - Genomics',\n",
        " 'Validation_Omics - Lipidomics',\n",
        " 'Validation_Omics - Other (e.g. metabolomics, metagenomics)']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Validation_Omics - Proteomics': 'Proteomics',\n",
        "    'Validation_Omics - Transcriptomics': 'Transcriptomics',\n",
        "    'Validation_Omics - Genomics': 'Genomics',\n",
        "    'Validation_Omics - Lipidomics': 'Lipidomics',\n",
        "    'Validation_Omics - Other (e.g. metabolomics, metagenomics)': 'Other omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBlS-IdkweiA"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Validation feature types according to year - omics'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 0.55, 0.05))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 0.55, 0.05)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 0.55)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx4B7CjzpLwE"
      },
      "source": [
        "## Feature types according to year (clustering or validation)\n",
        "\n",
        "Not used in the manuscript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFfvpZNApLwL"
      },
      "source": [
        "### All features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkn5vkU9pLwL"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Utilized_Motor_Scales_no_Complications_Either_in_Clustering_or_Validation',\n",
        "                'Utilized_Motor_Complications_Either_in_Clustering_or_Validation',\n",
        "                'Utilized_Cognition_Either_in_Clustering_or_Validation',\n",
        "                'Utilized_Non_Motor_Excluding_Cognition_QoL_Other_Either_in_Clustering_or_Validation',\n",
        "                'Utilized_Any_Biomarkers_Either_in_Clustering_or_Validation',\n",
        "                'Utilized_Any_Neurophisiology_Either_in_Clustering_or_Validation',\n",
        "                'Utilized_Any_Neuroimaging_Either_in_Clustering_or_Validation',\n",
        "                'Utilized_Any_Omics_Either_in_Clustering_or_Validation']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "try:\n",
        "    data = df[featuretypes]\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}\")\n",
        "    print(\"Check the following column names:\", featuretypes)\n",
        "    print(\"Existing columns in df:\", df.columns)\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Utilized_Motor_Scales_no_Complications_Either_in_Clustering_or_Validation': 'Motor scales',\n",
        "    'Utilized_Motor_Complications_Either_in_Clustering_or_Validation': 'Motor complications',\n",
        "    'Utilized_Cognition_Either_in_Clustering_or_Validation': 'Cognition',\n",
        "    'Utilized_Non_Motor_Excluding_Cognition_QoL_Other_Either_in_Clustering_or_Validation': 'Other non-motor',\n",
        "    'Utilized_Any_Biomarkers_Either_in_Clustering_or_Validation': 'Biomarkers',\n",
        "    'Utilized_Any_Neurophisiology_Either_in_Clustering_or_Validation': 'Neurophysiology',\n",
        "    'Utilized_Any_Neuroimaging_Either_in_Clustering_or_Validation': 'Neuroimaging',\n",
        "    'Utilized_Any_Omics_Either_in_Clustering_or_Validation': 'Omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1hGkz5HpLwM"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering or validation feature types according to year - all'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1DJd5N7pLwM"
      },
      "source": [
        "### Clinical special"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZgX3mjLpLwM"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Quality of Life_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Other (pain, fatigue etc)_Either_in_Clustering_or_Validation',\n",
        " 'Utilized_Demographic_Either_in_Clustering_or_Validation',\n",
        " 'Utilized_Time_or_Staging_Measurements_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Vital Signs or Neurologic Examination Findings_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Phonation_Either_in_Clustering_or_Validation',\n",
        " 'Clinical - Motor Tests (example: Timed-Up & Go)_Either_in_Clustering_or_Validation']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Clinical - Neuropsychiatric (anxiety, depression, impulsivity, delusions, hallucinations etc)_Either_in_Clustering_or_Validation': 'Neuropsychiatric symptoms',\n",
        "    'Clinical - Sleep (REM sleep behavior disorder, insomnia, daytime sleepiness)_Either_in_Clustering_or_Validation': 'Sleep symptoms',\n",
        "    'Clinical - Anosmia or Hyposmia (absence or loss of the capability to smell)_Either_in_Clustering_or_Validation': 'Anosmia or hyposmia',\n",
        "    'Clinical - Autonomic (gastroparesis, constipation, orthostatic hypotension, urinary incontinence, sexual difficulties)_Either_in_Clustering_or_Validation': 'Autonomic symptoms',\n",
        "    'Clinical - Quality of Life_Either_in_Clustering_or_Validation': 'Quality of life',\n",
        "    'Clinical - Other (pain, fatigue etc)_Either_in_Clustering_or_Validation': 'Other symptoms',\n",
        "    'Utilized_Demographic_Either_in_Clustering_or_Validation': 'Demographic data',\n",
        "    'Utilized_Time_or_Staging_Measurements_Either_in_Clustering_or_Validation':'Disease staging',\n",
        "    'Clinical - Vital Signs or Neurologic Examination Findings_Either_in_Clustering_or_Validation': 'Neurologic examination findings',\n",
        "    'Clinical - Phonation_Either_in_Clustering_or_Validation': 'Phonation',\n",
        "    'Clinical - Motor Tests (example: Timed-Up & Go)_Either_in_Clustering_or_Validation': 'Motor tests'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, column_renames.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJrNy4qppLwM"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering or validation feature types according to year - clinical detailed'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 1.1, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_wQwycDpLwM"
      },
      "source": [
        "### Neuroimaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmT7mDQXpLwM"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Neuroimaging - Structural or Volumetric MRI_Either_in_Clustering_or_Validation',\n",
        " 'Neuroimaging - Diffusion Imaging MRI_Either_in_Clustering_or_Validation',\n",
        " 'Neuroimaging - DaTSCAN_Either_in_Clustering_or_Validation',\n",
        " 'Neuroimaging - Radiomics_Either_in_Clustering_or_Validation',\n",
        " 'Neuroimaging - Functional MRI_Either_in_Clustering_or_Validation',\n",
        " 'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)_Either_in_Clustering_or_Validation']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "# Renaming columns\n",
        "column_renames = {\n",
        "    'Neuroimaging - Structural or Volumetric MRI_Either_in_Clustering_or_Validation': 'Structural or Volumetric MRI',\n",
        " 'Neuroimaging - Diffusion Imaging MRI_Either_in_Clustering_or_Validation': 'Diffusion Imaging MRI',\n",
        " 'Neuroimaging - DaTSCAN_Either_in_Clustering_or_Validation': 'DaTSCAN',\n",
        " 'Neuroimaging - Radiomics_Either_in_Clustering_or_Validation': 'Radiomics',\n",
        " 'Neuroimaging - Functional MRI_Either_in_Clustering_or_Validation': 'Functional MRI',\n",
        " 'Neuroimaging - Other Methods (e.g. neuromelanin-sensitive MRI)_Either_in_Clustering_or_Validation': 'Other Methods'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, column_renames.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uAMvksjpLwM"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering or validation feature types according to year - neuroimaging'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 0.55, 0.05))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 0.55, 0.05)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 0.55)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze3d4FMRpLwM"
      },
      "source": [
        "### Omics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0cUsCEYpLwM"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                 'Omics - Proteomics_Either_in_Clustering_or_Validation',\n",
        " 'Omics - Transcriptomics_Either_in_Clustering_or_Validation',\n",
        " 'Omics - Genomics_Either_in_Clustering_or_Validation',\n",
        " 'Omics - Lipidomics_Either_in_Clustering_or_Validation',\n",
        " 'Omics - Other (e.g. metabolomics, metagenomics)_Either_in_Clustering_or_Validation']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = {\n",
        "    'Year of publication Quantile Division': 'Year of publication Quantile Division',\n",
        "    'Omics - Proteomics_Either_in_Clustering_or_Validation': 'Proteomics',\n",
        "    'Omics - Transcriptomics_Either_in_Clustering_or_Validation': 'Transcriptomics',\n",
        "    'Omics - Genomics_Either_in_Clustering_or_Validation': 'Genomics',\n",
        "    'Omics - Lipidomics_Either_in_Clustering_or_Validation': 'Lipidomics',\n",
        "    'Omics - Other (e.g. metabolomics, metagenomics)_Either_in_Clustering_or_Validation': 'Other omics'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDmPnCPppLwN"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Clustering or validation feature types according to year - omics'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in column_renames.values():\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 0.55, 0.05))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 0.55, 0.05)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Set the y-axis limits to ensure consistent spacing\n",
        "ax.set_ylim(-0.05, 0.55)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MYE-D6C2r4G"
      },
      "source": [
        "## Cohorts used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84prCopC2r4G"
      },
      "outputs": [],
      "source": [
        "featuretypes = ['Year of publication Quantile Division',\n",
        "                'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [AMP-PD]',\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [BioFIND]',\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Fox Insight]',\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [GP2 Dataset]',\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [LRRK2 Cohort Consortium]',\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson Progression Marker Initiative (PPMI)]',\n",
        " \"Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson's Disease Biomarker Program (PDBP)]\",\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [UK Biobank]',\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Oxford Parkinson Disease Center Discovery Cohort]',\n",
        " 'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]']\n",
        "\n",
        "# Creating temporary dataset to work with\n",
        "data = df[featuretypes]\n",
        "\n",
        "column_renames = column_renames = {'Year of publication Quantile Division':'Year of publication Quantile Division',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [AMP-PD]': 'AMP-PD',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [BioFIND]': 'BioFIND',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Fox Insight]': 'Fox Insight',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [GP2 Dataset]': 'GP2',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [LRRK2 Cohort Consortium]': 'LRRK2 Cohort Consortium',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson Progression Marker Initiative (PPMI)]': 'PPMI',\n",
        "    \"Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Parkinson's Disease Biomarker Program (PDBP)]\": \"PDBP\",\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [UK Biobank]': 'UK Biobank',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Oxford Parkinson Disease Center Discovery Cohort]' : 'OPDC Discovery',\n",
        "    'Dataset_Used Which datasets were utilized in the creation and/or validation of the subtyping algorithm? [Other specific or local datasets]': 'Other specific or local datasets'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping columns with 0 values but also the specific datasets\n",
        "# List of columns to remove\n",
        "cols_to_remove = ['AMP-PD', 'Fox Insight', 'GP2',  'LRRK2 Cohort Consortium', 'UK Biobank', 'Other specific or local datasets']\n",
        "\n",
        "# Drop the columns\n",
        "try:\n",
        "  data = data.drop(cols_to_remove, axis=1)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "uK0PtD0k-1iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HljEuz9G2r4H"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "figurename = 'Cohorts used during the years'\n",
        "\n",
        "# Grouping by 'Year of publication Quantile Division' and calculating mean for plotting\n",
        "df_grouped = data.groupby('Year of publication Quantile Division').mean()\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "for feature in data.columns:\n",
        "  if feature != 'Year of publication Quantile Division':\n",
        "    ax.plot(df_grouped.index, df_grouped[feature], marker='o', label=feature)\n",
        "\n",
        "# Adjusting labels, ticks, and legend\n",
        "ax.set_ylabel('Proportion of utilization (%)', fontsize=14)\n",
        "ax.set_xlabel('Year range', fontsize=14)\n",
        "ax.set_xticks(np.arange(len(df_grouped)))\n",
        "ax.set_xticklabels(df_grouped.index, rotation=0)\n",
        "ax.set_yticks(np.arange(0, 0.6, 0.1))  # Adjusting y-ticks to every 0.1\n",
        "ax.set_yticklabels(['{:.0f}%'.format(x * 100) for x in np.arange(0, 0.6, 0.1)])  # Converting to percentage\n",
        "ax.tick_params(axis='x', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "ax.tick_params(axis='y', which='major', labelsize=12)  # Increase font size for x-axis ticks\n",
        "\n",
        "# Customizing the legend\n",
        "legend = ax.legend(title='', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, fontsize=11)\n",
        "\n",
        "# Aesthetic adjustments\n",
        "ax.set_facecolor('white')  # Set background to white\n",
        "ax.grid(False)  # Turn off the grid\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "# Showing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scales utilized\n",
        "\n",
        "Radar chart"
      ],
      "metadata": {
        "id": "fBszA4AALHZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed (no interest) - 'Income', 'Vital Signs (blood pressure, cardiac frequency)', 'Physical Attributes (weight, height etc)'\n",
        "\n",
        "descriptivedata = ['Before_2020_After',\n",
        "                   'UPDRS motor scores','UPDRS non-motor scores',\n",
        "                   'MDS-UPDRS Part I', 'MDS-UPDRS Part II', 'MDS-UPDRS Part III', 'MDS-UPDRS Part IV',\n",
        "                   'The Schwab and England ADL (Activities of Daily Living) scale',\n",
        "                   'Hoehn and Yahr Scale or Modified Hoehn and Yahr Scale',\n",
        "                   'University of Pennsylvania Smell Identification Test (UPSIT)',\n",
        "                   'Non-Motor Symptoms Scale (NNMS)',\n",
        "                   'Non-Motor Symptoms Questionnaire (NMSQ)',\n",
        "                   'Montreal Cognitive Assessment (MoCA)',\n",
        "                   'Mini-Mental State Examination (MMSE)',\n",
        "                   'SCOPA-AUT',\n",
        "                   'Geriatric Depression Scale (GDS)', 'Beck Depression Inventory (BDI)',\n",
        "                   'Hospital Anxiety and Depression Scale (HADS)', 'State-Trait Anxiety Inventory for Adults (STAI)',\n",
        "                   'Questionnaire for Impulsive-Compulsive Disorders in Parkinson’s Disease (QUIP)',\n",
        "                   'REM-Sleep Behavior Disorder Screening Questionnaire (RBDSQ)', 'Epworth Sleepiness Scale (ESS)']"
      ],
      "metadata": {
        "id": "_OxIA06GLrYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[descriptivedata]\n",
        "\n",
        "# Renaming columns\n",
        "column_renames ={\n",
        "    'Before_2020_After':'Year Category',\n",
        "    'UPDRS motor scores': 'Motor UPDRS',\n",
        "    'UPDRS non-motor scores': 'Non-motor UPDRS',\n",
        "    'MDS-UPDRS Part I': 'MDS-UPDRS I',\n",
        "    'MDS-UPDRS Part II': 'MDS-UPDRS II',\n",
        "    'MDS-UPDRS Part III': 'MDS-UPDRS III',\n",
        "    'MDS-UPDRS Part IV': 'MDS-UPDRS IV',\n",
        "    'The Schwab and England ADL (Activities of Daily Living) scale': 'Schwab & England ADL',\n",
        "    'Hoehn and Yahr Scale or Modified Hoehn and Yahr Scale': 'Hoehn & Yahr Scale',\n",
        "    'University of Pennsylvania Smell Identification Test (UPSIT)': 'UPSIT',\n",
        "    'Non-Motor Symptoms Scale (NNMS)': 'NNMS',\n",
        "    'Non-Motor Symptoms Questionnaire (NMSQ)': 'NMSQ',\n",
        "    'Montreal Cognitive Assessment (MoCA)': 'MoCA',\n",
        "    'Mini-Mental State Examination (MMSE)': 'MMSE',\n",
        "    'SCOPA-AUT': 'SCOPA-AUT',\n",
        "    'Geriatric Depression Scale (GDS)': 'GDS',\n",
        "    'Beck Depression Inventory (BDI)': 'BDI',\n",
        "    'Hospital Anxiety and Depression Scale (HADS)': 'HADS',\n",
        "    'State-Trait Anxiety Inventory for Adults (STAI)': 'STAI',\n",
        "    'Questionnaire for Impulsive-Compulsive Disorders in Parkinson’s Disease (QUIP)': 'QUIP',\n",
        "    'REM-Sleep Behavior Disorder Screening Questionnaire (RBDSQ)': 'RBDSQ',\n",
        "    'Epworth Sleepiness Scale (ESS)': 'ESS'\n",
        "}\n",
        "\n",
        "data.rename(columns=column_renames, inplace=True)\n",
        "\n",
        "# Calculate percentages\n",
        "percentages = (data[list(column_renames.values())[1:]].mean() * 100).round()\n",
        "top_percentages = percentages.nlargest(15)  # Select top 15 values\n",
        "\n",
        "# Showing\n",
        "calculate_percentage_distribution(data, list(column_renames.values())[1:])"
      ],
      "metadata": {
        "id": "37_zz-CxMERe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido\n",
        "import kaleido"
      ],
      "metadata": {
        "id": "sFlZWHAnUbNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "figurename = 'Clinical scales radar chart'\n",
        "\n",
        "# Calculate the sum of each column, then compute the percentage relative to the total observations\n",
        "total_observations = len(data)\n",
        "column_percentages = (data[list(data.columns[1:])].sum() / total_observations * 100).round(0)\n",
        "\n",
        "# Sort the percentages and select the top 15\n",
        "top_15 = column_percentages.sort_values(ascending=False).head(15)\n",
        "\n",
        "# Categories and values for the radar chart, closing the loop by appending the first element at the end\n",
        "categories = list(top_15.index) + [top_15.index[0]]\n",
        "values = list(top_15.values) + [top_15.values[0]]\n",
        "\n",
        "# Create the radar chart\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=values,\n",
        "    theta=categories,\n",
        "    fill='toself',\n",
        "    name='Top 15 Feature Percentages'\n",
        "))\n",
        "\n",
        "# Update layout to make it more visually appealing and simpler\n",
        "fig.update_layout(\n",
        "    polar=dict(\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, max(values) + 10],  # Adjust the range to maximum observed value + 10\n",
        "            gridcolor='#F0F8FF',  # Change grid line color to light blue\n",
        "            gridwidth=2,  # Increase grid line width to 1\n",
        "            color='black',  # Standard color for text\n",
        "            tickvals=list(range(0, int(max(values) + 10), 10)),  # Setting ticks every 10%\n",
        "            tickfont=dict(size=15, color='black'),  # Font size and color for tick labels\n",
        "            linecolor='black'  # Color of the radial axis lines\n",
        "        ),\n",
        "        angularaxis=dict(  # Customize angular axis\n",
        "            tickfont=dict(size=17, color='black'),  # Font size and color for category labels\n",
        "            rotation=90,  # Rotate labels for better layout\n",
        "            direction='clockwise'  # Direction of categories\n",
        "        )\n",
        "    ),\n",
        "    title_font=dict(size=14, color='black'),  # Subdued title font size and color\n",
        "    showlegend=False,\n",
        "    margin=dict(\n",
        "        b=40,\n",
        "        r=45,\n",
        "        l=45,\n",
        "        t=40\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n",
        "\n",
        "# If you need to save the plot to a file, uncomment and adapt the line below\n",
        "import plotly.io as pio\n",
        "pio.write_image(fig, 'Plots/SVG/' + figurename + '.svg', format='svg')\n",
        "pio.write_image(fig, 'Plots/PNG/' + figurename + '.png', format='png')"
      ],
      "metadata": {
        "id": "2A1cp085Wi0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aqb9KTb50nP"
      },
      "source": [
        "## Bubble Plot\n",
        "\n",
        "**X** = Years | **Y** = Number of clustering features | **Bubble size** = patients in cluster | **Bubble color** = use of non-clinical feature for clustering | **Bubble opacity** = quality scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeGsHqRr7N0_"
      },
      "outputs": [],
      "source": [
        "data = df[['Year of publication', 'Clustering_Number_of_Specific_Domains', 'Number of PD patients utilized for clustering purposes for bubble', 'Clustering_Any_Non_Clinical','Quality_Index_Mestre']].astype(int)\n",
        "\n",
        "for column in data:\n",
        "  data[column] = pd.to_numeric(data[column], errors='coerce')\n",
        "\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Number of PD patients utilized for clustering purposes for bubble'].astype(int).describe()"
      ],
      "metadata": {
        "id": "8-B1Q8p8wmUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Number of PD patients utilized for clustering purposes for bubble'].astype(int).value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "xdyPWGVhwhB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(df['Clustering_Number_of_Specific_Domains'])"
      ],
      "metadata": {
        "id": "8qhtIrymGNjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q88j75e38WSf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "figurename = 'Bubble plot of number of features per year according to usage of non-clinical data and quality index'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Adjust transparency to range from 40% to 100%\n",
        "transparency = 0.2 + 0.8 * (data['Quality_Index_Mestre'] / 15)\n",
        "\n",
        "# Using the original color palette but ensuring it's for binary data\n",
        "cmap = plt.get_cmap('coolwarm', 2)  # 2 distinct colors for binary data\n",
        "\n",
        "# Further reduced bubble size\n",
        "size_scale = data['Number of PD patients utilized for clustering purposes for bubble'] * 0.8  # Reduced size scale\n",
        "\n",
        "bubble = plt.scatter(\n",
        "    data['Year of publication'],\n",
        "    data['Clustering_Number_of_Specific_Domains'],\n",
        "    s=size_scale,  # Bubble size scaling\n",
        "    c=data['Clustering_Any_Non_Clinical'],  # Using the non-clinical feature for colors\n",
        "    cmap=cmap,  # Updated color map for binary data\n",
        "    alpha=transparency,  # Transparency determined by 'Quality_Index_Mestre'\n",
        ")\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.xlabel('Year of publication', fontsize=14)\n",
        "plt.ylabel('Number of input cluster features', fontsize=14)\n",
        "\n",
        "# Adjusting y-axis limits to avoid touching x-axis\n",
        "plt.ylim(bottom=np.min(data['Clustering_Number_of_Specific_Domains']) - 1)\n",
        "\n",
        "# Set y-ticks to cover from 1 to the max value of 'Clustering_Number_of_Specific_Domains', inclusive\n",
        "yticks = np.arange(1, data['Clustering_Number_of_Specific_Domains'].max() + 1)\n",
        "plt.yticks(yticks, fontsize=14)\n",
        "\n",
        "# Modify x-axis to display ticks every four years\n",
        "years = np.arange(data['Year of publication'].min(), data['Year of publication'].max() + 1, 3)\n",
        "plt.xticks(years, fontsize=14)\n",
        "\n",
        "# Manually set and enlarge the legend with accurate colors\n",
        "colors = [cmap(0), cmap(1)]  # Extract colors from the colormap\n",
        "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=14, label=\"Only utilizes clinical features\"),\n",
        "           plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=14, label='Also utilizes a non-clinical features')]\n",
        "\n",
        "legend = plt.legend(handles=handles, title=\"\", frameon=False, loc='upper left', fontsize=12, title_fontsize=14)\n",
        "\n",
        "plt.grid(False)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Saving\n",
        "plt.savefig('Plots/SVG/'+figurename+'.svg', format='svg', dpi=300)  # Export as SVG\n",
        "plt.savefig('Plots/PNG/'+figurename+'.png', format='png', dpi=300)  # Export as PNG\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pTWlk50o6oSI",
        "8sdMCbgjsDfM",
        "xIeqYc3XC6rI",
        "qau93s8XrLGt",
        "2SS8PHHzHxUk"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}